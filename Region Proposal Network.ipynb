{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b86109",
   "metadata": {},
   "source": [
    "https://dongjk.github.io/code/object+detection/keras/2018/05/21/Faster_R-CNN_step_by_step,_Part_I.html\n",
    "https://www.kaggle.com/kishor1210/train-faster-rcnn-using-keras\n",
    "https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/\n",
    "https://www.programmersought.com/article/47744573673/\n",
    "https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf\n",
    "https://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4695631b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 16200."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85309f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "# Ignore FutureWarning from numpy\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";\n",
    "\n",
    "# Allow growth of GPU memory, otherwise it will always look like all the memory is being used\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45759d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyqt5 in c:\\users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (5.15.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (4.6.3)\n",
      "Requirement already satisfied: PyQt5-Qt5>=5.15 in c:\\users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pyqt5) (5.15.2)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.8 in c:\\users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pyqt5) (12.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyqt5 lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c7481",
   "metadata": {},
   "source": [
    "# XML parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd46fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in c:\\users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (0.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63cb135",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = \"anotations\"\n",
    "\n",
    "from utils import read_data, plot_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0330506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45216037",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox, images = read_data(FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45230ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc30ed4588>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# not work because in reading step bounding boxes are already multiplied \n",
    "plot_image(images[0], bbox[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff203a0",
   "metadata": {},
   "source": [
    "# Network training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394371c",
   "metadata": {},
   "source": [
    "# backbone \n",
    "\n",
    "Vgg 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22a3e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "vvg16 = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b940c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vvg16.predict(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f15862",
   "metadata": {},
   "source": [
    "## helper functions \n",
    "\n",
    "all helper functions moved to utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f7dbb",
   "metadata": {},
   "source": [
    "## loss function \n",
    "\n",
    "loss function moved to utils\n",
    "\n",
    "there should be a mistake how ious (problem was with dimensions of pois and thta no IOU was bigger than threshold) and deltas are calculated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416e41f",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "because of compicated loss function, model training can be implemented in few ways:\n",
    "\n",
    "    * create a loss layer, train the model and move trained weights to model without loss layer \n",
    "    * create a custom model with its own custom train_step "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f61425",
   "metadata": {},
   "source": [
    "### option 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2993c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import utils \n",
    "\n",
    "class LossLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(LossLayer, self).__init__()\n",
    "        self.bbox = tf.constant(utils.generate_bbox_coords(dims = np.array([0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5])), dtype = tf.float32)\n",
    "        self.loss_fn = utils.my_loss_fn\n",
    "        self.loss_res = tf.constant([0])\n",
    "\n",
    "    def call(self, output_scores, output_deltas, bounding_box, training=None):\n",
    "        if training:\n",
    "            self.loss_res , class_loss, regression_loss_l1  = self.loss_fn(\n",
    "                                                                        tf.cast(bounding_box, dtype = tf.float32),\n",
    "                                                                        output_scores,\n",
    "                                                                        tf.cast(output_deltas, dtype = tf.float32),\n",
    "                                                                        self.bbox,\n",
    "                                                                        tf.constant(10, dtype = tf.float32),\n",
    "                                                                        range_positive=0.5)\n",
    "            self.add_loss(self.loss_res)\n",
    "            self.add_metric(class_loss, name = \"class_loss\")\n",
    "            self.add_metric(regression_loss_l1, name = \"regression_loss_l1\")\n",
    "        return self.loss_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ae6a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = LossLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4f35def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8400, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.bbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8174aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_tile = Input(shape=(20,20,512))\n",
    "k = 21 # because of 9 different shapes\n",
    "\n",
    "gt_bbox = Input(shape=(None, 4))\n",
    "convolution_3x3 = Conv2D(\n",
    "    filters=512,\n",
    "    padding = \"same\",\n",
    "    kernel_size=(3, 3),\n",
    "    name=\"3x3\"\n",
    ")(feature_map_tile)\n",
    "\n",
    "output_deltas = Conv2D(\n",
    "    filters= 4 * k,\n",
    "    kernel_size=(1, 1),\n",
    "    activation=\"linear\",\n",
    "    kernel_initializer=\"uniform\",\n",
    "    name=\"deltas1\"\n",
    ")(convolution_3x3)\n",
    "\n",
    "output_scores = Conv2D(\n",
    "    filters=k,\n",
    "    kernel_size=(1, 1),\n",
    "    activation=\"sigmoid\",\n",
    "    kernel_initializer=\"uniform\",\n",
    "    name=\"scores1\"\n",
    ")(convolution_3x3)\n",
    "\n",
    "\n",
    "loss_layer = LossLayer()(output_scores, output_deltas, gt_bbox)\n",
    "\n",
    "model = Model(inputs=[feature_map_tile, gt_bbox], outputs=[loss_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "997e2c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 20, 20, 512) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "3x3 (Conv2D)                    (None, 20, 20, 512)  2359808     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "scores1 (Conv2D)                (None, 20, 20, 21)   10773       3x3[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "deltas1 (Conv2D)                (None, 20, 20, 84)   43092       3x3[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, None, 4)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "loss_layer_3 (LossLayer)        (1,)                 0           scores1[0][0]                    \n",
      "                                                                 deltas1[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,413,673\n",
      "Trainable params: 2,413,673\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc840d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71ea1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_one = tf.expand_dims(res[1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11559000",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_one = tf.expand_dims(bbox[1], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7195f7",
   "metadata": {},
   "source": [
    "create dummy targets as they wont be used in training. real targets are gives as inputs to model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4794598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dummy_target = np.tile(np.array([[1]]), (res_one.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef50f87b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "WARNING:tensorflow:Using a while_loop for converting Where\n",
      "WARNING:tensorflow:Using a while_loop for converting Where\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 17.8876 - class_loss: 2.9304 - regression_loss_l1: 1.4957\n",
      "Epoch 2/5000\n",
      "WARNING:tensorflow:From C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 118.6483 - class_loss: 1.4670 - regression_loss_l1: 11.7181\n",
      "Epoch 3/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 87.5500 - class_loss: 0.6116 - regression_loss_l1: 8.6938\n",
      "Epoch 4/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 71.5773 - class_loss: 0.4400 - regression_loss_l1: 7.1137\n",
      "Epoch 5/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 19.8269 - class_loss: 0.4338 - regression_loss_l1: 1.9393\n",
      "Epoch 6/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 54.3449 - class_loss: 0.3801 - regression_loss_l1: 5.3965\n",
      "Epoch 7/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 74.7372 - class_loss: 0.3782 - regression_loss_l1: 7.4359\n",
      "Epoch 8/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 67.3708 - class_loss: 0.3409 - regression_loss_l1: 6.7030\n",
      "Epoch 9/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 59.2876 - class_loss: 0.3290 - regression_loss_l1: 5.8959\n",
      "Epoch 10/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 51.1523 - class_loss: 0.2838 - regression_loss_l1: 5.0868\n",
      "Epoch 11/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 28.1247 - class_loss: 0.2372 - regression_loss_l1: 2.7887\n",
      "Epoch 12/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 18.0718 - class_loss: 0.2440 - regression_loss_l1: 1.7828\n",
      "Epoch 13/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 28.2309 - class_loss: 0.1797 - regression_loss_l1: 2.8051\n",
      "Epoch 14/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 37.0104 - class_loss: 0.1860 - regression_loss_l1: 3.6824\n",
      "Epoch 15/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 39.6614 - class_loss: 0.1647 - regression_loss_l1: 3.9497\n",
      "Epoch 16/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 37.9347 - class_loss: 0.1719 - regression_loss_l1: 3.7763\n",
      "Epoch 17/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 28.8007 - class_loss: 0.1650 - regression_loss_l1: 2.8636\n",
      "Epoch 18/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 25.6264 - class_loss: 0.1647 - regression_loss_l1: 2.5462\n",
      "Epoch 19/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 17.9028 - class_loss: 0.1911 - regression_loss_l1: 1.7712\n",
      "Epoch 20/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 11.2195 - class_loss: 0.1412 - regression_loss_l1: 1.1078\n",
      "Epoch 21/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 16.7196 - class_loss: 0.1911 - regression_loss_l1: 1.6528\n",
      "Epoch 22/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 20.2591 - class_loss: 0.1709 - regression_loss_l1: 2.0088\n",
      "Epoch 23/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 20.2539 - class_loss: 0.1985 - regression_loss_l1: 2.0055\n",
      "Epoch 24/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 16.7699 - class_loss: 0.1933 - regression_loss_l1: 1.6577\n",
      "Epoch 25/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 15.7294 - class_loss: 0.1813 - regression_loss_l1: 1.5548\n",
      "Epoch 26/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 12.8097 - class_loss: 0.1868 - regression_loss_l1: 1.2623\n",
      "Epoch 27/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.5391 - class_loss: 0.1808 - regression_loss_l1: 0.9358\n",
      "Epoch 28/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.7448 - class_loss: 0.1911 - regression_loss_l1: 0.8554\n",
      "Epoch 29/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 10.4270 - class_loss: 0.2049 - regression_loss_l1: 1.0222\n",
      "Epoch 30/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.7509 - class_loss: 0.1928 - regression_loss_l1: 1.1558\n",
      "Epoch 31/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.1017 - class_loss: 0.1895 - regression_loss_l1: 1.0912\n",
      "Epoch 32/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.4408 - class_loss: 0.1837 - regression_loss_l1: 1.0257\n",
      "Epoch 33/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.7157 - class_loss: 0.2008 - regression_loss_l1: 0.8515\n",
      "Epoch 34/5000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.6733 - class_loss: 0.1919 - regression_loss_l1: 0.6481\n",
      "Epoch 35/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.1778 - class_loss: 0.2035 - regression_loss_l1: 0.6974\n",
      "Epoch 36/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.8039 - class_loss: 0.2034 - regression_loss_l1: 0.6601\n",
      "Epoch 37/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.0682 - class_loss: 0.1910 - regression_loss_l1: 0.7877\n",
      "Epoch 38/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.4391 - class_loss: 0.1729 - regression_loss_l1: 0.6266\n",
      "Epoch 39/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.4878 - class_loss: 0.1883 - regression_loss_l1: 0.7299\n",
      "Epoch 40/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.2762 - class_loss: 0.1857 - regression_loss_l1: 0.6091\n",
      "Epoch 41/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.5734 - class_loss: 0.1690 - regression_loss_l1: 0.5404\n",
      "Epoch 42/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.1648 - class_loss: 0.1520 - regression_loss_l1: 0.7013\n",
      "Epoch 43/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.2920 - class_loss: 0.1589 - regression_loss_l1: 0.7133\n",
      "Epoch 44/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.9203 - class_loss: 0.1501 - regression_loss_l1: 0.7770\n",
      "Epoch 45/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.0460 - class_loss: 0.1362 - regression_loss_l1: 0.6910\n",
      "Epoch 46/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.1939 - class_loss: 0.1377 - regression_loss_l1: 0.6056\n",
      "Epoch 47/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.3510 - class_loss: 0.1587 - regression_loss_l1: 0.6192\n",
      "Epoch 48/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.6936 - class_loss: 0.1160 - regression_loss_l1: 0.8578\n",
      "Epoch 49/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.8772 - class_loss: 0.1407 - regression_loss_l1: 0.5737\n",
      "Epoch 50/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.6221 - class_loss: 0.1273 - regression_loss_l1: 0.6495\n",
      "Epoch 51/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.0696 - class_loss: 0.1192 - regression_loss_l1: 0.6950\n",
      "Epoch 52/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.6169 - class_loss: 0.1124 - regression_loss_l1: 0.6505\n",
      "Epoch 53/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.9208 - class_loss: 0.1011 - regression_loss_l1: 0.5820\n",
      "Epoch 54/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.8551 - class_loss: 0.1130 - regression_loss_l1: 0.6742\n",
      "Epoch 55/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.8678 - class_loss: 0.1014 - regression_loss_l1: 0.6766\n",
      "Epoch 56/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.4153 - class_loss: 0.1095 - regression_loss_l1: 0.8306\n",
      "Epoch 57/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.6720 - class_loss: 0.0972 - regression_loss_l1: 0.8575\n",
      "Epoch 58/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.0308 - class_loss: 0.0856 - regression_loss_l1: 0.8945\n",
      "Epoch 59/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.2744 - class_loss: 0.0870 - regression_loss_l1: 0.6187\n",
      "Epoch 60/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 7.8332 - class_loss: 0.1153 - regression_loss_l1: 0.7718\n",
      "Epoch 61/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.5060 - class_loss: 0.0983 - regression_loss_l1: 0.8408\n",
      "Epoch 62/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.7919 - class_loss: 0.0818 - regression_loss_l1: 0.7710\n",
      "Epoch 63/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.5950 - class_loss: 0.0868 - regression_loss_l1: 0.7508\n",
      "Epoch 64/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.4460 - class_loss: 0.0750 - regression_loss_l1: 1.0371\n",
      "Epoch 65/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.5491 - class_loss: 0.0782 - regression_loss_l1: 0.6471\n",
      "Epoch 66/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.3683 - class_loss: 0.0949 - regression_loss_l1: 0.8273\n",
      "Epoch 67/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.5222 - class_loss: 0.0722 - regression_loss_l1: 0.7450\n",
      "Epoch 68/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.7321 - class_loss: 0.0815 - regression_loss_l1: 0.7651\n",
      "Epoch 69/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.0760 - class_loss: 0.0854 - regression_loss_l1: 0.7991\n",
      "Epoch 70/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.4206 - class_loss: 0.1038 - regression_loss_l1: 0.7317\n",
      "Epoch 71/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.3056 - class_loss: 0.0674 - regression_loss_l1: 0.7238\n",
      "Epoch 72/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.2261 - class_loss: 0.0873 - regression_loss_l1: 0.8139\n",
      "Epoch 73/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.0478 - class_loss: 0.0839 - regression_loss_l1: 0.8964\n",
      "Epoch 74/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.5281 - class_loss: 0.0767 - regression_loss_l1: 0.6451\n",
      "Epoch 75/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.0675 - class_loss: 0.0720 - regression_loss_l1: 0.8996\n",
      "Epoch 76/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.5119 - class_loss: 0.0792 - regression_loss_l1: 0.7433\n",
      "Epoch 77/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.2241 - class_loss: 0.0656 - regression_loss_l1: 0.8158\n",
      "Epoch 78/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.0304 - class_loss: 0.0694 - regression_loss_l1: 0.5961\n",
      "Epoch 79/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.1616 - class_loss: 0.0738 - regression_loss_l1: 0.9088\n",
      "Epoch 80/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.9147 - class_loss: 0.0656 - regression_loss_l1: 0.6849\n",
      "Epoch 81/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.2181 - class_loss: 0.0590 - regression_loss_l1: 0.8159\n",
      "Epoch 82/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.1456 - class_loss: 0.0693 - regression_loss_l1: 0.9076\n",
      "Epoch 83/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.0036 - class_loss: 0.0610 - regression_loss_l1: 0.6943\n",
      "Epoch 84/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.8336 - class_loss: 0.0712 - regression_loss_l1: 0.8762\n",
      "Epoch 85/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.1733 - class_loss: 0.0597 - regression_loss_l1: 0.8114\n",
      "Epoch 86/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.4458 - class_loss: 0.0731 - regression_loss_l1: 0.6373\n",
      "Epoch 87/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.3917 - class_loss: 0.0732 - regression_loss_l1: 0.6319\n",
      "Epoch 88/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.3571 - class_loss: 0.0612 - regression_loss_l1: 0.8296\n",
      "Epoch 89/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.3352 - class_loss: 0.0600 - regression_loss_l1: 0.8275\n",
      "Epoch 90/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.8778 - class_loss: 0.0605 - regression_loss_l1: 0.5817\n",
      "Epoch 91/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.1208 - class_loss: 0.0659 - regression_loss_l1: 0.7055\n",
      "Epoch 92/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.8173 - class_loss: 0.0645 - regression_loss_l1: 0.6753\n",
      "Epoch 93/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.7006 - class_loss: 0.0680 - regression_loss_l1: 0.5633\n",
      "Epoch 94/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.9821 - class_loss: 0.0694 - regression_loss_l1: 0.5913\n",
      "Epoch 95/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.5447 - class_loss: 0.0544 - regression_loss_l1: 0.5490\n",
      "Epoch 96/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.1505 - class_loss: 0.0625 - regression_loss_l1: 0.7088\n",
      "Epoch 97/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.7057 - class_loss: 0.0620 - regression_loss_l1: 0.5644\n",
      "Epoch 98/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.3367 - class_loss: 0.0537 - regression_loss_l1: 0.6283\n",
      "Epoch 99/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.8669 - class_loss: 0.0550 - regression_loss_l1: 0.8812\n",
      "Epoch 100/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.2655 - class_loss: 0.0547 - regression_loss_l1: 0.6211\n",
      "Epoch 101/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.8430 - class_loss: 0.0587 - regression_loss_l1: 0.8784\n",
      "Epoch 102/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.5098 - class_loss: 0.0557 - regression_loss_l1: 0.9454\n",
      "Epoch 103/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.5421 - class_loss: 0.0486 - regression_loss_l1: 0.5493\n",
      "Epoch 104/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.4971 - class_loss: 0.0455 - regression_loss_l1: 0.9452\n",
      "Epoch 105/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.6276 - class_loss: 0.0558 - regression_loss_l1: 1.0572\n",
      "Epoch 106/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.8060 - class_loss: 0.0644 - regression_loss_l1: 0.7742\n",
      "Epoch 107/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.0897 - class_loss: 0.0534 - regression_loss_l1: 0.6036\n",
      "Epoch 108/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.5383 - class_loss: 0.0499 - regression_loss_l1: 1.0488\n",
      "Epoch 109/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.1974 - class_loss: 0.0636 - regression_loss_l1: 0.6134\n",
      "Epoch 110/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.1951 - class_loss: 0.0489 - regression_loss_l1: 1.0146\n",
      "Epoch 111/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 11.9896 - class_loss: 0.0551 - regression_loss_l1: 1.1934\n",
      "Epoch 112/5000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.4623 - class_loss: 0.0561 - regression_loss_l1: 0.5406\n",
      "Epoch 113/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.0967 - class_loss: 0.0501 - regression_loss_l1: 1.0047\n",
      "Epoch 114/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.0721 - class_loss: 0.0482 - regression_loss_l1: 1.1024\n",
      "Epoch 115/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.7593 - class_loss: 0.0538 - regression_loss_l1: 0.7706\n",
      "Epoch 116/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.0643 - class_loss: 0.0551 - regression_loss_l1: 0.9009\n",
      "Epoch 117/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.9869 - class_loss: 0.0604 - regression_loss_l1: 0.8927\n",
      "Epoch 118/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.1570 - class_loss: 0.0501 - regression_loss_l1: 0.7107\n",
      "Epoch 119/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.8914 - class_loss: 0.0551 - regression_loss_l1: 0.9836\n",
      "Epoch 120/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.1057 - class_loss: 0.0481 - regression_loss_l1: 0.9058\n",
      "Epoch 121/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.4435 - class_loss: 0.0560 - regression_loss_l1: 0.6388\n",
      "Epoch 122/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 10.2521 - class_loss: 0.0536 - regression_loss_l1: 1.0199\n",
      "Epoch 123/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.9917 - class_loss: 0.0586 - regression_loss_l1: 0.6933\n",
      "Epoch 124/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.7218 - class_loss: 0.0560 - regression_loss_l1: 0.8666\n",
      "Epoch 125/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.1824 - class_loss: 0.0570 - regression_loss_l1: 0.8125\n",
      "Epoch 126/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.9542 - class_loss: 0.0520 - regression_loss_l1: 0.8902\n",
      "Epoch 127/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.5777 - class_loss: 0.0522 - regression_loss_l1: 0.8526\n",
      "Epoch 128/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 10.1933 - class_loss: 0.0604 - regression_loss_l1: 1.0133\n",
      "Epoch 129/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.4068 - class_loss: 0.0632 - regression_loss_l1: 0.7344\n",
      "Epoch 130/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 11.8229 - class_loss: 0.0592 - regression_loss_l1: 1.1764\n",
      "Epoch 131/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 12.6418 - class_loss: 0.0522 - regression_loss_l1: 1.2590\n",
      "Epoch 132/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.1371 - class_loss: 0.0564 - regression_loss_l1: 0.9081\n",
      "Epoch 133/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.9355 - class_loss: 0.0526 - regression_loss_l1: 0.6883\n",
      "Epoch 134/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 10.5749 - class_loss: 0.0619 - regression_loss_l1: 1.0513\n",
      "Epoch 135/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.0857 - class_loss: 0.0596 - regression_loss_l1: 0.8026\n",
      "Epoch 136/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.8382 - class_loss: 0.0573 - regression_loss_l1: 0.8781\n",
      "Epoch 137/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.2756 - class_loss: 0.0587 - regression_loss_l1: 0.7217\n",
      "Epoch 138/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.4779 - class_loss: 0.0559 - regression_loss_l1: 0.7422\n",
      "Epoch 139/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.7641 - class_loss: 0.0506 - regression_loss_l1: 0.8713\n",
      "Epoch 140/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.4903 - class_loss: 0.0567 - regression_loss_l1: 0.5434\n",
      "Epoch 141/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.3256 - class_loss: 0.0533 - regression_loss_l1: 0.6272\n",
      "Epoch 142/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.5600 - class_loss: 0.0562 - regression_loss_l1: 0.7504\n",
      "Epoch 143/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.4655 - class_loss: 0.0505 - regression_loss_l1: 0.7415\n",
      "Epoch 144/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.8031 - class_loss: 0.0591 - regression_loss_l1: 0.4744\n",
      "Epoch 145/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.6922 - class_loss: 0.0535 - regression_loss_l1: 0.8639\n",
      "Epoch 146/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.3957 - class_loss: 0.0573 - regression_loss_l1: 0.8338\n",
      "Epoch 147/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.1653 - class_loss: 0.0506 - regression_loss_l1: 0.6115\n",
      "Epoch 148/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.4711 - class_loss: 0.0514 - regression_loss_l1: 0.7420\n",
      "Epoch 149/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.3317 - class_loss: 0.0535 - regression_loss_l1: 0.5278\n",
      "Epoch 150/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.1180 - class_loss: 0.0503 - regression_loss_l1: 0.8068\n",
      "Epoch 151/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.8011 - class_loss: 0.0508 - regression_loss_l1: 0.6750\n",
      "Epoch 152/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.7505 - class_loss: 0.0503 - regression_loss_l1: 0.5700\n",
      "Epoch 153/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.3763 - class_loss: 0.0528 - regression_loss_l1: 0.6324\n",
      "Epoch 154/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.3197 - class_loss: 0.0478 - regression_loss_l1: 0.5272\n",
      "Epoch 155/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.5636 - class_loss: 0.0531 - regression_loss_l1: 0.6510\n",
      "Epoch 156/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.1703 - class_loss: 0.0452 - regression_loss_l1: 0.4125\n",
      "Epoch 157/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.7483 - class_loss: 0.0446 - regression_loss_l1: 0.4704\n",
      "Epoch 158/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.0549 - class_loss: 0.0462 - regression_loss_l1: 0.6009\n",
      "Epoch 159/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.2613 - class_loss: 0.0489 - regression_loss_l1: 0.4212\n",
      "Epoch 160/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.2810 - class_loss: 0.0501 - regression_loss_l1: 0.6231\n",
      "Epoch 161/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.1352 - class_loss: 0.0450 - regression_loss_l1: 0.5090\n",
      "Epoch 162/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.9640 - class_loss: 0.0458 - regression_loss_l1: 0.4918\n",
      "Epoch 163/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.6054 - class_loss: 0.0422 - regression_loss_l1: 0.4563\n",
      "Epoch 164/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.0682 - class_loss: 0.0426 - regression_loss_l1: 0.4026\n",
      "Epoch 165/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.6115 - class_loss: 0.0429 - regression_loss_l1: 0.4569\n",
      "Epoch 166/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.7721 - class_loss: 0.0433 - regression_loss_l1: 0.3729\n",
      "Epoch 167/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.0697 - class_loss: 0.0449 - regression_loss_l1: 0.4025\n",
      "Epoch 168/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.2133 - class_loss: 0.0383 - regression_loss_l1: 0.4175\n",
      "Epoch 169/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.2290 - class_loss: 0.0391 - regression_loss_l1: 0.4190\n",
      "Epoch 170/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.3359 - class_loss: 0.0378 - regression_loss_l1: 0.3298\n",
      "Epoch 171/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.3195 - class_loss: 0.0370 - regression_loss_l1: 0.3283\n",
      "Epoch 172/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.2258 - class_loss: 0.0357 - regression_loss_l1: 0.3190\n",
      "Epoch 173/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0597 - class_loss: 0.0388 - regression_loss_l1: 0.3021\n",
      "Epoch 174/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.3596 - class_loss: 0.0349 - regression_loss_l1: 0.3325\n",
      "Epoch 175/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.1155 - class_loss: 0.0358 - regression_loss_l1: 0.3080\n",
      "Epoch 176/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.2519 - class_loss: 0.0346 - regression_loss_l1: 0.3217\n",
      "Epoch 177/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.9264 - class_loss: 0.0358 - regression_loss_l1: 0.2891\n",
      "Epoch 178/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.3980 - class_loss: 0.0325 - regression_loss_l1: 0.3366\n",
      "Epoch 179/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8358 - class_loss: 0.0279 - regression_loss_l1: 0.2808\n",
      "Epoch 180/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.7197 - class_loss: 0.0317 - regression_loss_l1: 0.2688\n",
      "Epoch 181/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.7955 - class_loss: 0.0296 - regression_loss_l1: 0.2766\n",
      "Epoch 182/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.0750 - class_loss: 0.0376 - regression_loss_l1: 0.3037\n",
      "Epoch 183/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.0126 - class_loss: 0.0280 - regression_loss_l1: 0.2985\n",
      "Epoch 184/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step - loss: 3.1715 - class_loss: 0.0284 - regression_loss_l1: 0.3143\n",
      "Epoch 185/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.4613 - class_loss: 0.0274 - regression_loss_l1: 0.2434\n",
      "Epoch 186/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.4548 - class_loss: 0.0288 - regression_loss_l1: 0.3426\n",
      "Epoch 187/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.1832 - class_loss: 0.0277 - regression_loss_l1: 0.3155\n",
      "Epoch 188/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.7033 - class_loss: 0.0270 - regression_loss_l1: 0.2676\n",
      "Epoch 189/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.1318 - class_loss: 0.0304 - regression_loss_l1: 0.3101\n",
      "Epoch 190/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.0993 - class_loss: 0.0306 - regression_loss_l1: 0.3069\n",
      "Epoch 191/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3308 - class_loss: 0.0280 - regression_loss_l1: 0.2303\n",
      "Epoch 192/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.3152 - class_loss: 0.0238 - regression_loss_l1: 0.3291\n",
      "Epoch 193/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.6362 - class_loss: 0.0261 - regression_loss_l1: 0.2610\n",
      "Epoch 194/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.4277 - class_loss: 0.0276 - regression_loss_l1: 0.3400\n",
      "Epoch 195/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.1327 - class_loss: 0.0234 - regression_loss_l1: 0.3109\n",
      "Epoch 196/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.5489 - class_loss: 0.0249 - regression_loss_l1: 0.3524\n",
      "Epoch 197/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.0771 - class_loss: 0.0266 - regression_loss_l1: 0.3050\n",
      "Epoch 198/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.2149 - class_loss: 0.0261 - regression_loss_l1: 0.3189\n",
      "Epoch 199/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.0649 - class_loss: 0.0238 - regression_loss_l1: 0.3041\n",
      "Epoch 200/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.0135 - class_loss: 0.0226 - regression_loss_l1: 0.2991\n",
      "Epoch 201/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8656 - class_loss: 0.0229 - regression_loss_l1: 0.2843\n",
      "Epoch 202/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.6332 - class_loss: 0.0251 - regression_loss_l1: 0.3608\n",
      "Epoch 203/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.9247 - class_loss: 0.0249 - regression_loss_l1: 0.2900\n",
      "Epoch 204/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0065 - class_loss: 0.0263 - regression_loss_l1: 0.2980\n",
      "Epoch 205/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.7411 - class_loss: 0.0261 - regression_loss_l1: 0.3715\n",
      "Epoch 206/5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-5bfe3b53ef9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtensorboard_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mres_one\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbbox_one\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \"\"\"\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    340\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_supports_tf_logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \"\"\"\n\u001b[0;32m   1062\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "history = model.fit([res_one, bbox_one], dummy_target, batch_size=1, epochs = 5000, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd638dd0",
   "metadata": {},
   "source": [
    "save model weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "50a1bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbbef01",
   "metadata": {},
   "source": [
    "create model without loss layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "7026335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_tile = Input(shape=(20,20,512))\n",
    "\n",
    "convolution_3x3 = Conv2D(\n",
    "    filters=512,\n",
    "    padding = \"same\",\n",
    "    kernel_size=(3, 3),\n",
    "    name=\"3x3\"\n",
    ")(feature_map_tile)\n",
    "\n",
    "output_deltas = Conv2D(\n",
    "    filters= 4 * k,\n",
    "    kernel_size=(1, 1),\n",
    "    activation=\"linear\",\n",
    "    kernel_initializer=\"uniform\",\n",
    "    name=\"deltas1\"\n",
    ")(convolution_3x3)\n",
    "\n",
    "output_scores = Conv2D(\n",
    "    filters=k,\n",
    "    kernel_size=(1, 1),\n",
    "    activation=\"sigmoid\",\n",
    "    kernel_initializer=\"uniform\",\n",
    "    name=\"scores1\"\n",
    ")(convolution_3x3)\n",
    "\n",
    "model = Model(inputs=[feature_map_tile], outputs=[output_scores, output_deltas])\n",
    "\n",
    "optimizer =tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc101b6",
   "metadata": {},
   "source": [
    "update weights by trained ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "cf4d11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "37563418",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_one = tf.expand_dims(res[3], axis = 0)\n",
    "img = images[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "3f0996dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_output_scores, predicted_output_deltas = model(res_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "bd65c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    bboxes_true = tf.constant(utils.generate_bbox_coords(dims = np.array([0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5])), dtype = tf.float32)\n",
    "    deltas = tf.reshape(predicted_output_deltas, (-1,4))\n",
    "    scores = tf.reshape(predicted_output_scores, (-1,1))\n",
    "    \n",
    "    ## deltas + bbox_cords \n",
    "    adjusted_bbox_cords = bboxes_true + deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc0f56",
   "metadata": {},
   "source": [
    "max supression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "20715510",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tf.image.non_max_suppression(\n",
    "    utils.to_box(adjusted_bbox_cords), scores[:,0],2000, iou_threshold=0.9, score_threshold = 0.7,\n",
    ")\n",
    "bboxes = tf.math.ceil(utils.to_box(tf.gather(adjusted_bbox_cords, ids)) * (640., 640., 640., 640.)) # multiplied by image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "08d6f005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e923698208>"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "image_with_bounding_box = img.numpy()\n",
    "\n",
    "for i in range(bboxes.shape[0]):\n",
    "#for i in range(2):\n",
    "    image_with_bounding_box = cv2.rectangle(image_with_bounding_box, (int(bboxes[i,0]), int(bboxes[i,1])), (int(bboxes[i,2]), int(bboxes[i,3])), (0, 255, 0), 5)\n",
    "plt.imshow(image_with_bounding_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93af0ca",
   "metadata": {},
   "source": [
    "### option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "957332f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193217eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "WARNING:tensorflow:Using a while_loop for converting Where\n",
      "WARNING:tensorflow:Using a while_loop for converting Where\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 17.7557 - class_loss: 3.3831 - l1_loss: 1.4373WARNING:tensorflow:From C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2/8 [======>.......................] - ETA: 0s - loss: 46.9902 - class_loss: 2.3276 - l1_loss: 4.4663WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0460s vs `on_train_batch_end` time: 0.1350s). Check your callbacks.\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 73.6126 - class_loss: 5.1052 - l1_loss: 6.8507\n",
      "Epoch 2/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 99.5895 - class_loss: 0.4353 - l1_loss: 9.9154\n",
      "Epoch 3/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 64.4072 - class_loss: 0.3378 - l1_loss: 6.4069\n",
      "Epoch 4/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 77.2240 - class_loss: 0.3067 - l1_loss: 7.6917\n",
      "Epoch 5/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 45.2204 - class_loss: 0.3013 - l1_loss: 4.4919\n",
      "Epoch 6/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 59.2575 - class_loss: 0.3791 - l1_loss: 5.8878\n",
      "Epoch 7/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 58.3607 - class_loss: 0.4058 - l1_loss: 5.7955\n",
      "Epoch 8/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 57.6367 - class_loss: 0.4738 - l1_loss: 5.7163\n",
      "Epoch 9/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 46.7740 - class_loss: 0.5137 - l1_loss: 4.6260\n",
      "Epoch 10/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 53.5187 - class_loss: 0.4313 - l1_loss: 5.3087\n",
      "Epoch 11/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 39.4025 - class_loss: 0.4618 - l1_loss: 3.8941\n",
      "Epoch 12/5000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 41.8571 - class_loss: 0.3775 - l1_loss: 4.1480\n",
      "Epoch 13/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 43.2555 - class_loss: 0.3064 - l1_loss: 4.2949\n",
      "Epoch 14/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 48.6023 - class_loss: 0.3257 - l1_loss: 4.8277\n",
      "Epoch 15/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 41.6643 - class_loss: 0.3728 - l1_loss: 4.1292\n",
      "Epoch 16/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 57.4155 - class_loss: 0.3872 - l1_loss: 5.7028\n",
      "Epoch 17/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 34.0430 - class_loss: 0.3674 - l1_loss: 3.3676\n",
      "Epoch 18/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 42.4611 - class_loss: 0.3869 - l1_loss: 4.2074\n",
      "Epoch 19/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 36.3691 - class_loss: 0.2710 - l1_loss: 3.6098\n",
      "Epoch 20/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 28.9856 - class_loss: 0.2891 - l1_loss: 2.8697\n",
      "Epoch 21/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 23.0055 - class_loss: 0.2878 - l1_loss: 2.2718\n",
      "Epoch 22/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 40.8130 - class_loss: 0.3025 - l1_loss: 4.0510\n",
      "Epoch 23/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 19.7198 - class_loss: 0.3112 - l1_loss: 1.9409\n",
      "Epoch 24/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 22.1475 - class_loss: 0.3254 - l1_loss: 2.1822\n",
      "Epoch 25/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 36.0587 - class_loss: 0.2300 - l1_loss: 3.5829\n",
      "Epoch 26/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 25.1772 - class_loss: 0.2756 - l1_loss: 2.4902\n",
      "Epoch 27/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 21.2332 - class_loss: 0.3700 - l1_loss: 2.0863\n",
      "Epoch 28/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 20.1731 - class_loss: 0.3558 - l1_loss: 1.9817\n",
      "Epoch 29/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.5598 - class_loss: 0.2053 - l1_loss: 1.4354\n",
      "Epoch 30/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 17.8341 - class_loss: 0.1757 - l1_loss: 1.7658\n",
      "Epoch 31/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 17.1707 - class_loss: 0.1890 - l1_loss: 1.6982\n",
      "Epoch 32/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 18.1505 - class_loss: 0.2252 - l1_loss: 1.7925\n",
      "Epoch 33/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 20.0359 - class_loss: 0.2476 - l1_loss: 1.9788\n",
      "Epoch 34/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.7222 - class_loss: 0.2045 - l1_loss: 1.4518\n",
      "Epoch 35/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.0217 - class_loss: 0.1766 - l1_loss: 1.3845\n",
      "Epoch 36/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 18.4796 - class_loss: 0.1827 - l1_loss: 1.8297\n",
      "Epoch 37/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 11.7191 - class_loss: 0.2126 - l1_loss: 1.1507\n",
      "Epoch 38/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 16.2414 - class_loss: 0.2297 - l1_loss: 1.6012\n",
      "Epoch 39/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.9101 - class_loss: 0.1808 - l1_loss: 1.3729\n",
      "Epoch 40/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 10.1475 - class_loss: 0.1682 - l1_loss: 0.9979\n",
      "Epoch 41/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 12.2653 - class_loss: 0.1837 - l1_loss: 1.2082\n",
      "Epoch 42/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 9.0518 - class_loss: 0.1738 - l1_loss: 0.8878\n",
      "Epoch 43/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.9529 - class_loss: 0.1843 - l1_loss: 0.8769\n",
      "Epoch 44/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.0996 - class_loss: 0.1734 - l1_loss: 0.7926\n",
      "Epoch 45/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.3677 - class_loss: 0.1855 - l1_loss: 0.8182\n",
      "Epoch 46/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 8.0762 - class_loss: 0.1746 - l1_loss: 0.7902\n",
      "Epoch 47/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 6.9293 - class_loss: 0.1760 - l1_loss: 0.6753\n",
      "Epoch 48/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 7.6514 - class_loss: 0.1508 - l1_loss: 0.7501\n",
      "Epoch 49/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 8.0501 - class_loss: 0.1527 - l1_loss: 0.7897\n",
      "Epoch 50/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 7.8287 - class_loss: 0.1790 - l1_loss: 0.7650\n",
      "Epoch 51/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 5.8488 - class_loss: 0.1584 - l1_loss: 0.5690\n",
      "Epoch 52/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.6747 - class_loss: 0.1368 - l1_loss: 0.5538\n",
      "Epoch 53/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 7.0439 - class_loss: 0.1948 - l1_loss: 0.6849\n",
      "Epoch 54/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 7.7415 - class_loss: 0.2213 - l1_loss: 0.7520\n",
      "Epoch 55/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.7108 - class_loss: 0.1264 - l1_loss: 0.4584\n",
      "Epoch 56/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.4612 - class_loss: 0.1167 - l1_loss: 0.3345\n",
      "Epoch 57/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 4.6418 - class_loss: 0.1053 - l1_loss: 0.4536\n",
      "Epoch 58/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 5.7874 - class_loss: 0.1466 - l1_loss: 0.5641\n",
      "Epoch 59/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.3832 - class_loss: 0.2045 - l1_loss: 0.4179\n",
      "Epoch 60/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 4.3898 - class_loss: 0.1472 - l1_loss: 0.4243\n",
      "Epoch 61/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.4433 - class_loss: 0.1098 - l1_loss: 0.5333\n",
      "Epoch 62/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 4.1413 - class_loss: 0.0914 - l1_loss: 0.4050\n",
      "Epoch 63/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.4237 - class_loss: 0.1624 - l1_loss: 0.3261\n",
      "Epoch 64/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.2464 - class_loss: 0.1377 - l1_loss: 0.3109\n",
      "Epoch 65/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.4280 - class_loss: 0.0982 - l1_loss: 0.3330\n",
      "Epoch 66/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.5880 - class_loss: 0.1013 - l1_loss: 0.3487\n",
      "Epoch 67/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.1535 - class_loss: 0.1032 - l1_loss: 0.4050\n",
      "Epoch 68/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.7088 - class_loss: 0.1749 - l1_loss: 0.3534\n",
      "Epoch 69/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0924 - class_loss: 0.1697 - l1_loss: 0.2923\n",
      "Epoch 70/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.9270 - class_loss: 0.0840 - l1_loss: 0.3843\n",
      "Epoch 71/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9273 - class_loss: 0.0811 - l1_loss: 0.2846\n",
      "Epoch 72/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.3979 - class_loss: 0.1454 - l1_loss: 0.3252\n",
      "Epoch 73/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9520 - class_loss: 0.1008 - l1_loss: 0.2851\n",
      "Epoch 74/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.5083 - class_loss: 0.1206 - l1_loss: 0.2388\n",
      "Epoch 75/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2652 - class_loss: 0.0880 - l1_loss: 0.3177\n",
      "Epoch 76/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.3563 - class_loss: 0.0848 - l1_loss: 0.2271\n",
      "Epoch 77/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.8666 - class_loss: 0.1022 - l1_loss: 0.1764\n",
      "Epoch 78/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.0562 - class_loss: 0.1105 - l1_loss: 0.1946\n",
      "Epoch 79/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0838 - class_loss: 0.0910 - l1_loss: 0.1993\n",
      "Epoch 80/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.1469 - class_loss: 0.0755 - l1_loss: 0.2071\n",
      "Epoch 81/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.9817 - class_loss: 0.0977 - l1_loss: 0.1884\n",
      "Epoch 82/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.3634 - class_loss: 0.0690 - l1_loss: 0.2294\n",
      "Epoch 83/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5196 - class_loss: 0.0933 - l1_loss: 0.2426\n",
      "Epoch 84/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7279 - class_loss: 0.1212 - l1_loss: 0.1607\n",
      "Epoch 85/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8487 - class_loss: 0.0725 - l1_loss: 0.1776\n",
      "Epoch 86/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0115 - class_loss: 0.0694 - l1_loss: 0.1942\n",
      "Epoch 87/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7167 - class_loss: 0.1003 - l1_loss: 0.1616\n",
      "Epoch 88/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4229 - class_loss: 0.0922 - l1_loss: 0.1331\n",
      "Epoch 89/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6467 - class_loss: 0.0563 - l1_loss: 0.1590\n",
      "Epoch 90/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5456 - class_loss: 0.1009 - l1_loss: 0.1445\n",
      "Epoch 91/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7552 - class_loss: 0.0741 - l1_loss: 0.1681\n",
      "Epoch 92/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6579 - class_loss: 0.0900 - l1_loss: 0.1568\n",
      "Epoch 93/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5132 - class_loss: 0.0696 - l1_loss: 0.1444\n",
      "Epoch 94/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2889 - class_loss: 0.0657 - l1_loss: 0.2223\n",
      "Epoch 95/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9288 - class_loss: 0.0646 - l1_loss: 0.1864\n",
      "Epoch 96/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8334 - class_loss: 0.1084 - l1_loss: 0.1725\n",
      "Epoch 97/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7284 - class_loss: 0.0878 - l1_loss: 0.1641\n",
      "Epoch 98/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4381 - class_loss: 0.0632 - l1_loss: 0.1375\n",
      "Epoch 99/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3563 - class_loss: 0.0557 - l1_loss: 0.1301\n",
      "Epoch 100/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5993 - class_loss: 0.0532 - l1_loss: 0.1546\n",
      "Epoch 101/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4824 - class_loss: 0.1424 - l1_loss: 0.1340\n",
      "Epoch 102/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6553 - class_loss: 0.0458 - l1_loss: 0.1609\n",
      "Epoch 103/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3959 - class_loss: 0.0466 - l1_loss: 0.1349\n",
      "Epoch 104/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6226 - class_loss: 0.0859 - l1_loss: 0.1537\n",
      "Epoch 105/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3458 - class_loss: 0.0619 - l1_loss: 0.1284\n",
      "Epoch 106/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1549 - class_loss: 0.0480 - l1_loss: 0.1107 0s - loss: 1.2168 - class_loss: 0.0613 - l1_loss: \n",
      "Epoch 107/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3255 - class_loss: 0.0418 - l1_loss: 0.1284\n",
      "Epoch 108/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3175 - class_loss: 0.0567 - l1_loss: 0.1261\n",
      "Epoch 109/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5322 - class_loss: 0.0656 - l1_loss: 0.1467\n",
      "Epoch 110/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7565 - class_loss: 0.0484 - l1_loss: 0.1708\n",
      "Epoch 111/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2966 - class_loss: 0.1298 - l1_loss: 0.2167\n",
      "Epoch 112/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0552 - class_loss: 0.0453 - l1_loss: 0.2010\n",
      "Epoch 113/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8653 - class_loss: 0.0574 - l1_loss: 0.1808\n",
      "Epoch 114/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5898 - class_loss: 0.0558 - l1_loss: 0.1534\n",
      "Epoch 115/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5137 - class_loss: 0.0672 - l1_loss: 0.1447\n",
      "Epoch 116/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5091 - class_loss: 0.0688 - l1_loss: 0.1440\n",
      "Epoch 117/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2753 - class_loss: 0.0485 - l1_loss: 0.1227\n",
      "Epoch 118/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6313 - class_loss: 0.0524 - l1_loss: 0.1579\n",
      "Epoch 119/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7842 - class_loss: 0.0921 - l1_loss: 0.1692\n",
      "Epoch 120/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7092 - class_loss: 0.0797 - l1_loss: 0.1630\n",
      "Epoch 121/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.8265 - class_loss: 0.0473 - l1_loss: 0.1779\n",
      "Epoch 122/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5514 - class_loss: 0.0497 - l1_loss: 0.1502\n",
      "Epoch 123/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0902 - class_loss: 0.0619 - l1_loss: 0.2028\n",
      "Epoch 124/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3771 - class_loss: 0.0547 - l1_loss: 0.1322\n",
      "Epoch 125/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4963 - class_loss: 0.0562 - l1_loss: 0.1440\n",
      "Epoch 126/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9303 - class_loss: 0.0345 - l1_loss: 0.1896\n",
      "Epoch 127/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.2358 - class_loss: 0.0455 - l1_loss: 0.2190\n",
      "Epoch 128/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1088 - class_loss: 0.1622 - l1_loss: 0.1947\n",
      "Epoch 129/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8278 - class_loss: 0.0302 - l1_loss: 0.1798\n",
      "Epoch 130/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.8169 - class_loss: 0.0590 - l1_loss: 0.2758\n",
      "Epoch 131/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7575 - class_loss: 0.1191 - l1_loss: 0.1638\n",
      "Epoch 132/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.5978 - class_loss: 0.0285 - l1_loss: 0.2569\n",
      "Epoch 133/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.7550 - class_loss: 0.1029 - l1_loss: 0.2652\n",
      "Epoch 134/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.4371 - class_loss: 0.0492 - l1_loss: 0.3388\n",
      "Epoch 135/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.7091 - class_loss: 0.1020 - l1_loss: 0.2607\n",
      "Epoch 136/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9347 - class_loss: 0.0991 - l1_loss: 0.2836\n",
      "Epoch 137/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.2365 - class_loss: 0.0319 - l1_loss: 0.2205\n",
      "Epoch 138/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1975 - class_loss: 0.0543 - l1_loss: 0.3143\n",
      "Epoch 139/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9711 - class_loss: 0.0449 - l1_loss: 0.1926\n",
      "Epoch 140/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0650 - class_loss: 0.0502 - l1_loss: 0.2015\n",
      "Epoch 141/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5308 - class_loss: 0.0355 - l1_loss: 0.1495\n",
      "Epoch 142/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4158 - class_loss: 0.0317 - l1_loss: 0.1384\n",
      "Epoch 143/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4910 - class_loss: 0.0463 - l1_loss: 0.1445\n",
      "Epoch 144/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4397 - class_loss: 0.0302 - l1_loss: 0.1410\n",
      "Epoch 145/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.6212 - class_loss: 0.0261 - l1_loss: 0.1595\n",
      "Epoch 146/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4317 - class_loss: 0.0676 - l1_loss: 0.1364\n",
      "Epoch 147/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6942 - class_loss: 0.0433 - l1_loss: 0.1651\n",
      "Epoch 148/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9185 - class_loss: 0.0197 - l1_loss: 0.1899\n",
      "Epoch 149/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4333 - class_loss: 0.0474 - l1_loss: 0.2386\n",
      "Epoch 150/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1261 - class_loss: 0.0721 - l1_loss: 0.2054\n",
      "Epoch 151/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6361 - class_loss: 0.0302 - l1_loss: 0.2606\n",
      "Epoch 152/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4722 - class_loss: 0.1263 - l1_loss: 0.2346\n",
      "Epoch 153/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9809 - class_loss: 0.0405 - l1_loss: 0.1940\n",
      "Epoch 154/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6699 - class_loss: 0.0217 - l1_loss: 0.2648\n",
      "Epoch 155/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7019 - class_loss: 0.0960 - l1_loss: 0.2606\n",
      "Epoch 156/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9412 - class_loss: 0.0291 - l1_loss: 0.1912\n",
      "Epoch 157/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7833 - class_loss: 0.0450 - l1_loss: 0.2738\n",
      "Epoch 158/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5588 - class_loss: 0.0632 - l1_loss: 0.2496\n",
      "Epoch 159/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.3247 - class_loss: 0.0207 - l1_loss: 0.2304\n",
      "Epoch 160/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4247 - class_loss: 0.0577 - l1_loss: 0.2367\n",
      "Epoch 161/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.9985 - class_loss: 0.0490 - l1_loss: 0.1950\n",
      "Epoch 162/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4436 - class_loss: 0.0881 - l1_loss: 0.2356\n",
      "Epoch 163/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5873 - class_loss: 0.0183 - l1_loss: 0.2569\n",
      "Epoch 164/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.4547 - class_loss: 0.0390 - l1_loss: 0.2416\n",
      "Epoch 165/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.3932 - class_loss: 0.0489 - l1_loss: 0.2344\n",
      "Epoch 166/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1647 - class_loss: 0.0703 - l1_loss: 0.2094\n",
      "Epoch 167/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4859 - class_loss: 0.0310 - l1_loss: 0.2455\n",
      "Epoch 168/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3629 - class_loss: 0.0613 - l1_loss: 0.2302\n",
      "Epoch 169/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3746 - class_loss: 0.0239 - l1_loss: 0.2351\n",
      "Epoch 170/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1310 - class_loss: 0.0265 - l1_loss: 0.2105\n",
      "Epoch 171/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0095 - class_loss: 0.0331 - l1_loss: 0.1976\n",
      "Epoch 172/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2238 - class_loss: 0.0463 - l1_loss: 0.2177\n",
      "Epoch 173/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.1202 - class_loss: 0.0575 - l1_loss: 0.2063\n",
      "Epoch 174/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6070 - class_loss: 0.0544 - l1_loss: 0.2553\n",
      "Epoch 175/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9345 - class_loss: 0.0267 - l1_loss: 0.1908\n",
      "Epoch 176/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5776 - class_loss: 0.0379 - l1_loss: 0.2540\n",
      "Epoch 177/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8111 - class_loss: 0.0410 - l1_loss: 0.2770\n",
      "Epoch 178/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5315 - class_loss: 0.0317 - l1_loss: 0.2500\n",
      "Epoch 179/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.2608 - class_loss: 0.0508 - l1_loss: 0.2210\n",
      "Epoch 180/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1647 - class_loss: 0.1019 - l1_loss: 0.3063\n",
      "Epoch 181/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0992 - class_loss: 0.0280 - l1_loss: 0.3071\n",
      "Epoch 182/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.1828 - class_loss: 0.0323 - l1_loss: 0.3150\n",
      "Epoch 183/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4767 - class_loss: 0.0456 - l1_loss: 0.2431\n",
      "Epoch 184/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.4752 - class_loss: 0.0340 - l1_loss: 0.3441\n",
      "Epoch 185/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2884 - class_loss: 0.0708 - l1_loss: 0.3218\n",
      "Epoch 186/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9389 - class_loss: 0.0244 - l1_loss: 0.2915\n",
      "Epoch 187/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2713 - class_loss: 0.0539 - l1_loss: 0.3217\n",
      "Epoch 188/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.8670 - class_loss: 0.0251 - l1_loss: 0.3842\n",
      "Epoch 189/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.5699 - class_loss: 0.0220 - l1_loss: 0.3548\n",
      "Epoch 190/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.0985 - class_loss: 0.2520 - l1_loss: 0.4846\n",
      "Epoch 191/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9021 - class_loss: 0.0278 - l1_loss: 0.4874\n",
      "Epoch 192/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.0484 - class_loss: 0.1222 - l1_loss: 0.6926\n",
      "Epoch 193/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.4394 - class_loss: 0.3742 - l1_loss: 0.7065\n",
      "Epoch 194/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.0934 - class_loss: 0.1560 - l1_loss: 0.9937\n",
      "Epoch 195/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.8549 - class_loss: 0.0940 - l1_loss: 0.9761\n",
      "Epoch 196/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 21.1028 - class_loss: 2.7378 - l1_loss: 1.8365\n",
      "Epoch 197/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 17.5933 - class_loss: 1.2695 - l1_loss: 1.6324\n",
      "Epoch 198/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 19.4775 - class_loss: 0.7536 - l1_loss: 1.8724\n",
      "Epoch 199/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.4965 - class_loss: 1.2231 - l1_loss: 1.4273\n",
      "Epoch 200/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.8516 - class_loss: 0.0702 - l1_loss: 1.3781\n",
      "Epoch 201/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.3419 - class_loss: 0.1069 - l1_loss: 1.6235\n",
      "Epoch 202/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.9347 - class_loss: 0.0498 - l1_loss: 1.6885\n",
      "Epoch 203/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.0764 - class_loss: 0.0242 - l1_loss: 1.5052\n",
      "Epoch 204/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 13.1037 - class_loss: 0.0293 - l1_loss: 1.3074\n",
      "Epoch 205/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 22.0373 - class_loss: 1.1871 - l1_loss: 2.0850\n",
      "Epoch 206/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 26.1396 - class_loss: 0.2267 - l1_loss: 2.5913\n",
      "Epoch 207/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 18.8143 - class_loss: 0.1480 - l1_loss: 1.8666\n",
      "Epoch 208/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 28.4171 - class_loss: 1.2723 - l1_loss: 2.7145\n",
      "Epoch 209/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 34.1326 - class_loss: 0.8063 - l1_loss: 3.3326\n",
      "Epoch 210/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 37.0236 - class_loss: 0.3782 - l1_loss: 3.6645\n",
      "Epoch 211/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 30.2598 - class_loss: 0.0755 - l1_loss: 3.0184\n",
      "Epoch 212/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 28.5186 - class_loss: 0.1181 - l1_loss: 2.8401\n",
      "Epoch 213/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 26.5513 - class_loss: 0.3112 - l1_loss: 2.6240\n",
      "Epoch 214/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 28.2985 - class_loss: 0.1147 - l1_loss: 2.8184\n",
      "Epoch 215/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 27.2465 - class_loss: 0.1251 - l1_loss: 2.7121\n",
      "Epoch 216/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 22.8797 - class_loss: 0.0452 - l1_loss: 2.2835\n",
      "Epoch 217/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.1932 - class_loss: 0.0331 - l1_loss: 1.8160\n",
      "Epoch 218/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.6475 - class_loss: 0.0841 - l1_loss: 1.6563\n",
      "Epoch 219/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.1747 - class_loss: 0.0155 - l1_loss: 1.2159\n",
      "Epoch 220/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.2638 - class_loss: 0.1438 - l1_loss: 1.2120\n",
      "Epoch 221/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.0474 - class_loss: 0.0299 - l1_loss: 1.3017\n",
      "Epoch 222/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.8466 - class_loss: 0.0523 - l1_loss: 1.3794\n",
      "Epoch 223/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 10.5304 - class_loss: 0.0728 - l1_loss: 1.0458\n",
      "Epoch 224/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.4017 - class_loss: 0.0648 - l1_loss: 0.9337\n",
      "Epoch 225/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 9.3154 - class_loss: 0.0313 - l1_loss: 0.9284\n",
      "Epoch 226/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.5695 - class_loss: 0.0110 - l1_loss: 0.9559\n",
      "Epoch 227/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.3818 - class_loss: 0.0177 - l1_loss: 0.7364\n",
      "Epoch 228/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.2877 - class_loss: 0.0850 - l1_loss: 0.6203\n",
      "Epoch 229/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.9208 - class_loss: 0.0290 - l1_loss: 0.5892\n",
      "Epoch 230/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.9045 - class_loss: 0.0071 - l1_loss: 0.5897\n",
      "Epoch 231/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.6894 - class_loss: 0.0091 - l1_loss: 0.6680\n",
      "Epoch 232/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.8725 - class_loss: 0.0398 - l1_loss: 0.6833\n",
      "Epoch 233/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.3792 - class_loss: 0.0096 - l1_loss: 0.5370\n",
      "Epoch 234/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.6285 - class_loss: 0.0219 - l1_loss: 0.6607\n",
      "Epoch 235/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2695 - class_loss: 0.0354 - l1_loss: 0.4234\n",
      "Epoch 236/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 4.8974 - class_loss: 0.0139 - l1_loss: 0.4884\n",
      "Epoch 237/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7564 - class_loss: 0.2296 - l1_loss: 0.4527\n",
      "Epoch 238/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.3979 - class_loss: 0.0193 - l1_loss: 0.4379 0s - loss: 4.4768 - class_loss: 0.0094 - l1_loss: \n",
      "Epoch 239/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.5906 - class_loss: 0.0364 - l1_loss: 0.6554\n",
      "Epoch 240/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.3145 - class_loss: 0.1344 - l1_loss: 0.4180\n",
      "Epoch 241/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.7026 - class_loss: 0.0124 - l1_loss: 0.4690\n",
      "Epoch 242/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.7904 - class_loss: 0.0476 - l1_loss: 0.3743\n",
      "Epoch 243/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6798 - class_loss: 0.0360 - l1_loss: 0.3644\n",
      "Epoch 244/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.4440 - class_loss: 0.0153 - l1_loss: 0.4429\n",
      "Epoch 245/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1757 - class_loss: 0.0079 - l1_loss: 0.4168\n",
      "Epoch 246/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.0401 - class_loss: 0.0392 - l1_loss: 0.5001\n",
      "Epoch 247/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.1918 - class_loss: 0.1400 - l1_loss: 0.4052\n",
      "Epoch 248/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.9262 - class_loss: 0.0204 - l1_loss: 0.4906\n",
      "Epoch 249/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.4462 - class_loss: 0.0226 - l1_loss: 0.5424\n",
      "Epoch 250/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.6361 - class_loss: 0.0174 - l1_loss: 0.4619\n",
      "Epoch 251/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9287 - class_loss: 0.0285 - l1_loss: 0.2900 0s - loss: 3.0071 - class_loss: 0.0236 - l1_loss: 0.\n",
      "Epoch 252/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.7090 - class_loss: 0.0294 - l1_loss: 0.3680\n",
      "Epoch 253/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6406 - class_loss: 0.0093 - l1_loss: 0.2631\n",
      "Epoch 254/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6357 - class_loss: 0.0131 - l1_loss: 0.2623\n",
      "Epoch 255/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3904 - class_loss: 0.0237 - l1_loss: 0.2367\n",
      "Epoch 256/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7418 - class_loss: 0.0067 - l1_loss: 0.1735\n",
      "Epoch 257/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.7471 - class_loss: 0.0117 - l1_loss: 0.1735\n",
      "Epoch 258/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5037 - class_loss: 0.0151 - l1_loss: 0.1489\n",
      "Epoch 259/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5493 - class_loss: 0.0060 - l1_loss: 0.1543\n",
      "Epoch 260/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7125 - class_loss: 0.0116 - l1_loss: 0.1701\n",
      "Epoch 261/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0634 - class_loss: 0.0318 - l1_loss: 0.2032\n",
      "Epoch 262/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5510 - class_loss: 0.0078 - l1_loss: 0.1543\n",
      "Epoch 263/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6790 - class_loss: 0.0068 - l1_loss: 0.1672\n",
      "Epoch 264/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0373 - class_loss: 0.0128 - l1_loss: 0.2025\n",
      "Epoch 265/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.2338 - class_loss: 0.0195 - l1_loss: 0.2214\n",
      "Epoch 266/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.4863 - class_loss: 0.0212 - l1_loss: 0.2465\n",
      "Epoch 267/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1060 - class_loss: 0.0141 - l1_loss: 0.4092\n",
      "Epoch 268/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8020 - class_loss: 0.0196 - l1_loss: 0.4782\n",
      "Epoch 269/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7511 - class_loss: 0.0108 - l1_loss: 0.2740\n",
      "Epoch 270/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5891 - class_loss: 0.0515 - l1_loss: 0.4538\n",
      "Epoch 271/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4355 - class_loss: 0.0068 - l1_loss: 0.3429\n",
      "Epoch 272/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6253 - class_loss: 0.0126 - l1_loss: 0.3613\n",
      "Epoch 273/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8859 - class_loss: 0.0402 - l1_loss: 0.2846\n",
      "Epoch 274/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.4231 - class_loss: 0.0062 - l1_loss: 0.2417\n",
      "Epoch 275/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1823 - class_loss: 0.0122 - l1_loss: 0.3170\n",
      "Epoch 276/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3886 - class_loss: 0.0110 - l1_loss: 0.2378 0s - loss: 2.3678 - class_loss: 0.0120 - l1_loss: 0. - ETA: 0s - loss: 2.5045 - class_loss: 0.0102 - l1_loss: 0.24\n",
      "Epoch 277/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0593 - class_loss: 0.0161 - l1_loss: 0.2043\n",
      "Epoch 278/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5524 - class_loss: 0.0177 - l1_loss: 0.1535\n",
      "Epoch 279/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4221 - class_loss: 0.0097 - l1_loss: 0.1412\n",
      "Epoch 280/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9091 - class_loss: 0.0116 - l1_loss: 0.1897\n",
      "Epoch 281/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5712 - class_loss: 0.0175 - l1_loss: 0.1554\n",
      "Epoch 282/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6448 - class_loss: 0.0143 - l1_loss: 0.1630\n",
      "Epoch 283/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.4695 - class_loss: 0.0080 - l1_loss: 0.1462\n",
      "Epoch 284/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.5736 - class_loss: 0.0092 - l1_loss: 0.1564\n",
      "Epoch 285/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.3895 - class_loss: 0.0112 - l1_loss: 0.1378\n",
      "Epoch 286/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5148 - class_loss: 0.0165 - l1_loss: 0.1498\n",
      "Epoch 287/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3627 - class_loss: 0.0198 - l1_loss: 0.1343\n",
      "Epoch 288/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4135 - class_loss: 0.0123 - l1_loss: 0.1401\n",
      "Epoch 289/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0465 - class_loss: 0.0079 - l1_loss: 0.1039\n",
      "Epoch 290/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2383 - class_loss: 0.0081 - l1_loss: 0.1230\n",
      "Epoch 291/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2360 - class_loss: 0.0190 - l1_loss: 0.1217\n",
      "Epoch 292/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4222 - class_loss: 0.0269 - l1_loss: 0.1395\n",
      "Epoch 293/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1953 - class_loss: 0.0055 - l1_loss: 0.1190\n",
      "Epoch 294/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3752 - class_loss: 0.0060 - l1_loss: 0.1369 0s - loss: 1.4584 - class_loss: 0.0043 - l1_loss: 0.\n",
      "Epoch 295/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1593 - class_loss: 0.0144 - l1_loss: 0.1145\n",
      "Epoch 296/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.4130 - class_loss: 0.0209 - l1_loss: 0.1392\n",
      "Epoch 297/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2999 - class_loss: 0.0056 - l1_loss: 0.1294\n",
      "Epoch 298/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3831 - class_loss: 0.0056 - l1_loss: 0.1378\n",
      "Epoch 299/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3780 - class_loss: 0.0437 - l1_loss: 0.1334\n",
      "Epoch 300/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3490 - class_loss: 0.0174 - l1_loss: 0.1332\n",
      "Epoch 301/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.7256 - class_loss: 0.0045 - l1_loss: 0.1721\n",
      "Epoch 302/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.3592 - class_loss: 0.0195 - l1_loss: 0.1340\n",
      "Epoch 303/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6342 - class_loss: 0.0200 - l1_loss: 0.1614\n",
      "Epoch 304/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8634 - class_loss: 0.0089 - l1_loss: 0.1854\n",
      "Epoch 305/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4872 - class_loss: 0.0091 - l1_loss: 0.1478 0s - loss: 1.5295 - class_loss: 0.0096 - l1_loss: 0.\n",
      "Epoch 306/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7691 - class_loss: 0.0311 - l1_loss: 0.1738\n",
      "Epoch 307/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3030 - class_loss: 0.0078 - l1_loss: 0.1295\n",
      "Epoch 308/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3938 - class_loss: 0.0080 - l1_loss: 0.1386\n",
      "Epoch 309/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1911 - class_loss: 0.0109 - l1_loss: 0.1180\n",
      "Epoch 310/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0722 - class_loss: 0.0193 - l1_loss: 0.1053\n",
      "Epoch 311/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5715 - class_loss: 0.0136 - l1_loss: 0.1558 0s - loss: 1.6685 - class_loss: 0.0161 - l1_loss: 0.16 - ETA: 0s - loss: 1.5632 - class_loss: 0.0150 - l1_loss: 0.15\n",
      "Epoch 312/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8043 - class_loss: 0.0065 - l1_loss: 0.1798\n",
      "Epoch 313/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8473 - class_loss: 0.0198 - l1_loss: 0.1828\n",
      "Epoch 314/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6887 - class_loss: 0.0117 - l1_loss: 0.1677 0s - loss: 1.5171 - class_loss: 0.0062 - l1_loss: \n",
      "Epoch 315/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8874 - class_loss: 0.0275 - l1_loss: 0.1860\n",
      "Epoch 316/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8542 - class_loss: 0.0192 - l1_loss: 0.1835\n",
      "Epoch 317/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.2495 - class_loss: 0.0268 - l1_loss: 0.2223\n",
      "Epoch 318/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5528 - class_loss: 0.0073 - l1_loss: 0.2545 0s - loss: 2.6935 - class_loss: 0.0067 - l1_loss: 0.\n",
      "Epoch 319/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7577 - class_loss: 0.0248 - l1_loss: 0.2733\n",
      "Epoch 320/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6604 - class_loss: 0.0092 - l1_loss: 0.3651\n",
      "Epoch 321/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.8766 - class_loss: 0.0130 - l1_loss: 0.3864\n",
      "Epoch 322/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3604 - class_loss: 0.0308 - l1_loss: 0.4330 0s - loss: 4.4882 - class_loss: 0.0105 - l1_loss: 0.44 - ETA: 0s - loss: 4.2410 - class_loss: 0.0309 - l1_loss: 0.42\n",
      "Epoch 323/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.0628 - class_loss: 0.0322 - l1_loss: 0.5031\n",
      "Epoch 324/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.4153 - class_loss: 0.0103 - l1_loss: 0.5405\n",
      "Epoch 325/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.8792 - class_loss: 0.0078 - l1_loss: 0.3871\n",
      "Epoch 326/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.3554 - class_loss: 0.0081 - l1_loss: 0.4347\n",
      "Epoch 327/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0761 - class_loss: 0.0541 - l1_loss: 0.4022\n",
      "Epoch 328/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1481 - class_loss: 0.0061 - l1_loss: 0.4142\n",
      "Epoch 329/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.9318 - class_loss: 0.0109 - l1_loss: 0.4921\n",
      "Epoch 330/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.3552 - class_loss: 0.2343 - l1_loss: 0.5121\n",
      "Epoch 331/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.9019 - class_loss: 0.0147 - l1_loss: 0.5887\n",
      "Epoch 332/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.2399 - class_loss: 0.0252 - l1_loss: 0.6215\n",
      "Epoch 333/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.9136 - class_loss: 0.0065 - l1_loss: 0.6907\n",
      "Epoch 334/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.4240 - class_loss: 0.0933 - l1_loss: 0.6331\n",
      "Epoch 335/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.9324 - class_loss: 0.1791 - l1_loss: 0.7753\n",
      "Epoch 336/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.4906 - class_loss: 0.0464 - l1_loss: 0.8444\n",
      "Epoch 337/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.6048 - class_loss: 0.0083 - l1_loss: 0.7596\n",
      "Epoch 338/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.2415 - class_loss: 0.0091 - l1_loss: 1.5232\n",
      "Epoch 339/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.3191 - class_loss: 0.1620 - l1_loss: 0.9157\n",
      "Epoch 340/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.5438 - class_loss: 0.0126 - l1_loss: 0.9531\n",
      "Epoch 341/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.3703 - class_loss: 0.0833 - l1_loss: 1.3287\n",
      "Epoch 342/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.7619 - class_loss: 0.0332 - l1_loss: 0.9729\n",
      "Epoch 343/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 11.9813 - class_loss: 0.0364 - l1_loss: 1.1945\n",
      "Epoch 344/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.0169 - class_loss: 0.0071 - l1_loss: 0.9010\n",
      "Epoch 345/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.2870 - class_loss: 0.0075 - l1_loss: 1.4280\n",
      "Epoch 346/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.3058 - class_loss: 0.0127 - l1_loss: 1.2293\n",
      "Epoch 347/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 10.2787 - class_loss: 0.1220 - l1_loss: 1.0157\n",
      "Epoch 348/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 15.9490 - class_loss: 0.0862 - l1_loss: 1.5863\n",
      "Epoch 349/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.0396 - class_loss: 0.0462 - l1_loss: 1.4993\n",
      "Epoch 350/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 19.9141 - class_loss: 0.0185 - l1_loss: 1.9896\n",
      "Epoch 351/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 18.0147 - class_loss: 0.0386 - l1_loss: 1.7976\n",
      "Epoch 352/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 20.3880 - class_loss: 0.1550 - l1_loss: 2.0233\n",
      "Epoch 353/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.5181 - class_loss: 0.0396 - l1_loss: 1.4478\n",
      "Epoch 354/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.5272 - class_loss: 0.0623 - l1_loss: 1.3465\n",
      "Epoch 355/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.2160 - class_loss: 0.1495 - l1_loss: 1.0067\n",
      "Epoch 356/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.4075 - class_loss: 0.0119 - l1_loss: 1.2396\n",
      "Epoch 357/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.7443 - class_loss: 0.0150 - l1_loss: 0.9729\n",
      "Epoch 358/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.9670 - class_loss: 0.0543 - l1_loss: 0.9913\n",
      "Epoch 359/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.0909 - class_loss: 0.0157 - l1_loss: 0.8075\n",
      "Epoch 360/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.6698 - class_loss: 0.0261 - l1_loss: 0.6644\n",
      "Epoch 361/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.4462 - class_loss: 0.0049 - l1_loss: 0.5441\n",
      "Epoch 362/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.1070 - class_loss: 0.0060 - l1_loss: 0.4101\n",
      "Epoch 363/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1990 - class_loss: 0.0104 - l1_loss: 0.3189\n",
      "Epoch 364/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5215 - class_loss: 0.0325 - l1_loss: 0.3489\n",
      "Epoch 365/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.4403 - class_loss: 0.0119 - l1_loss: 0.3428\n",
      "Epoch 366/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1010 - class_loss: 0.0111 - l1_loss: 0.3090\n",
      "Epoch 367/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0262 - class_loss: 0.0183 - l1_loss: 0.3008\n",
      "Epoch 368/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7314 - class_loss: 0.0108 - l1_loss: 0.2721\n",
      "Epoch 369/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1879 - class_loss: 0.0139 - l1_loss: 0.3174\n",
      "Epoch 370/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0108 - class_loss: 0.0080 - l1_loss: 0.3003\n",
      "Epoch 371/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7889 - class_loss: 0.0064 - l1_loss: 0.2783 0s - loss: 3.2792 - class_loss: 0.0024 - l1_loss: \n",
      "Epoch 372/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0637 - class_loss: 0.0156 - l1_loss: 0.2048\n",
      "Epoch 373/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5138 - class_loss: 0.0172 - l1_loss: 0.2497\n",
      "Epoch 374/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1209 - class_loss: 0.0082 - l1_loss: 0.2113\n",
      "Epoch 375/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8717 - class_loss: 0.0059 - l1_loss: 0.1866\n",
      "Epoch 376/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4963 - class_loss: 0.0214 - l1_loss: 0.1475\n",
      "Epoch 377/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8311 - class_loss: 0.0290 - l1_loss: 0.1802\n",
      "Epoch 378/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9272 - class_loss: 0.0047 - l1_loss: 0.1922\n",
      "Epoch 379/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7640 - class_loss: 0.0046 - l1_loss: 0.1759\n",
      "Epoch 380/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2804 - class_loss: 0.0225 - l1_loss: 0.2258\n",
      "Epoch 381/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7962 - class_loss: 0.0089 - l1_loss: 0.1787\n",
      "Epoch 382/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.2918 - class_loss: 0.0048 - l1_loss: 0.2287\n",
      "Epoch 383/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.2954 - class_loss: 0.0108 - l1_loss: 0.2285\n",
      "Epoch 384/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6256 - class_loss: 0.0238 - l1_loss: 0.2602\n",
      "Epoch 385/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.4934 - class_loss: 0.0103 - l1_loss: 0.2483\n",
      "Epoch 386/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0530 - class_loss: 0.0155 - l1_loss: 0.2037\n",
      "Epoch 387/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7088 - class_loss: 0.0057 - l1_loss: 0.2703\n",
      "Epoch 388/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0479 - class_loss: 0.0093 - l1_loss: 0.4039\n",
      "Epoch 389/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.3641 - class_loss: 0.0091 - l1_loss: 0.3355\n",
      "Epoch 390/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6047 - class_loss: 0.0189 - l1_loss: 0.2586\n",
      "Epoch 391/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.1412 - class_loss: 0.0157 - l1_loss: 0.3125\n",
      "Epoch 392/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1189 - class_loss: 0.0100 - l1_loss: 0.2109\n",
      "Epoch 393/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2575 - class_loss: 0.0141 - l1_loss: 0.3243 0s - loss: 3.4199 - class_loss: 0.0048 - l1_loss: 0.\n",
      "Epoch 394/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2968 - class_loss: 0.0180 - l1_loss: 0.2279\n",
      "Epoch 395/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5939 - class_loss: 0.0042 - l1_loss: 0.2590\n",
      "Epoch 396/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.5788 - class_loss: 0.0147 - l1_loss: 0.2564\n",
      "Epoch 397/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.9321 - class_loss: 0.0333 - l1_loss: 0.3899\n",
      "Epoch 398/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8511 - class_loss: 0.0109 - l1_loss: 0.3840\n",
      "Epoch 399/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.6329 - class_loss: 0.0128 - l1_loss: 0.4620\n",
      "Epoch 400/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0236 - class_loss: 0.0194 - l1_loss: 0.3004\n",
      "Epoch 401/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3868 - class_loss: 0.0088 - l1_loss: 0.4378\n",
      "Epoch 402/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.4577 - class_loss: 0.0151 - l1_loss: 0.4443\n",
      "Epoch 403/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.2697 - class_loss: 0.0071 - l1_loss: 0.5263\n",
      "Epoch 404/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.1598 - class_loss: 0.0045 - l1_loss: 0.9155\n",
      "Epoch 405/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.7016 - class_loss: 0.0327 - l1_loss: 0.5669\n",
      "Epoch 406/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.3547 - class_loss: 0.0270 - l1_loss: 0.6328\n",
      "Epoch 407/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3669 - class_loss: 0.0066 - l1_loss: 0.3360\n",
      "Epoch 408/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.5998 - class_loss: 0.0355 - l1_loss: 0.6564\n",
      "Epoch 409/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.1767 - class_loss: 0.0063 - l1_loss: 0.5170\n",
      "Epoch 410/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.1090 - class_loss: 0.0345 - l1_loss: 0.6074\n",
      "Epoch 411/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.4498 - class_loss: 0.0048 - l1_loss: 0.4445\n",
      "Epoch 412/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.1896 - class_loss: 0.0098 - l1_loss: 0.7180\n",
      "Epoch 413/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.1377 - class_loss: 0.2048 - l1_loss: 0.5933\n",
      "Epoch 414/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.5522 - class_loss: 0.0156 - l1_loss: 0.5537\n",
      "Epoch 415/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.9892 - class_loss: 0.0664 - l1_loss: 0.7923\n",
      "Epoch 416/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.7068 - class_loss: 0.1516 - l1_loss: 1.1555\n",
      "Epoch 417/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.7408 - class_loss: 0.0639 - l1_loss: 1.6677: 0s - loss: 16.2809 - class_loss: 0.0575 - l1_loss: 1.622\n",
      "Epoch 418/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.0582 - class_loss: 0.6617 - l1_loss: 1.5396\n",
      "Epoch 419/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 23.4009 - class_loss: 0.1519 - l1_loss: 2.3249\n",
      "Epoch 420/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 23.6286 - class_loss: 1.1193 - l1_loss: 2.2509\n",
      "Epoch 421/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 19.8302 - class_loss: 0.0656 - l1_loss: 1.9765\n",
      "Epoch 422/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 30.5381 - class_loss: 0.0897 - l1_loss: 3.0448\n",
      "Epoch 423/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 20.5801 - class_loss: 0.0762 - l1_loss: 2.0504\n",
      "Epoch 424/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 19.7706 - class_loss: 0.3287 - l1_loss: 1.9442: 0s - loss: 19.1401 - class_loss: 0.2101 - l1_loss: 1.89\n",
      "Epoch 425/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 19.5928 - class_loss: 0.0583 - l1_loss: 1.9535\n",
      "Epoch 426/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.6261 - class_loss: 0.0555 - l1_loss: 1.8571\n",
      "Epoch 427/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.9698 - class_loss: 0.0225 - l1_loss: 1.5947\n",
      "Epoch 428/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 11.7791 - class_loss: 0.0099 - l1_loss: 1.1769: 0s - loss: 11.9878 - class_loss: 0.0109 - l1_loss: 1.197\n",
      "Epoch 429/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.1314 - class_loss: 0.0312 - l1_loss: 1.3100\n",
      "Epoch 430/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.6659 - class_loss: 0.2534 - l1_loss: 0.9413\n",
      "Epoch 431/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 11.6007 - class_loss: 0.0701 - l1_loss: 1.1531\n",
      "Epoch 432/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 10.1911 - class_loss: 0.0173 - l1_loss: 1.0174\n",
      "Epoch 433/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.4576 - class_loss: 0.0137 - l1_loss: 0.9444\n",
      "Epoch 434/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.4375 - class_loss: 0.1674 - l1_loss: 1.0270\n",
      "Epoch 435/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.0682 - class_loss: 0.0091 - l1_loss: 0.7059\n",
      "Epoch 436/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.0561 - class_loss: 0.0194 - l1_loss: 0.6037\n",
      "Epoch 437/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.6333 - class_loss: 0.0063 - l1_loss: 0.6627\n",
      "Epoch 438/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.4168 - class_loss: 0.0280 - l1_loss: 0.7389\n",
      "Epoch 439/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.2635 - class_loss: 0.0148 - l1_loss: 0.6249\n",
      "Epoch 440/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.7420 - class_loss: 0.0121 - l1_loss: 0.4730\n",
      "Epoch 441/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.1675 - class_loss: 0.0108 - l1_loss: 0.5157\n",
      "Epoch 442/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 7.1424 - class_loss: 0.0343 - l1_loss: 0.7108\n",
      "Epoch 443/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.0293 - class_loss: 0.0035 - l1_loss: 0.4026\n",
      "Epoch 444/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.0634 - class_loss: 0.0049 - l1_loss: 0.4058\n",
      "Epoch 445/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0682 - class_loss: 0.0112 - l1_loss: 0.3057\n",
      "Epoch 446/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.8902 - class_loss: 0.0133 - l1_loss: 0.3877\n",
      "Epoch 447/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.2961 - class_loss: 0.0047 - l1_loss: 0.3291\n",
      "Epoch 448/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9371 - class_loss: 0.0116 - l1_loss: 0.2925\n",
      "Epoch 449/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.1521 - class_loss: 0.0064 - l1_loss: 0.3146\n",
      "Epoch 450/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.3552 - class_loss: 0.0089 - l1_loss: 0.3346\n",
      "Epoch 451/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.4363 - class_loss: 0.0140 - l1_loss: 0.3422\n",
      "Epoch 452/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.0990 - class_loss: 0.0211 - l1_loss: 0.4078\n",
      "Epoch 453/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.4341 - class_loss: 0.0058 - l1_loss: 0.6428\n",
      "Epoch 454/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.2232 - class_loss: 0.0084 - l1_loss: 0.4215\n",
      "Epoch 455/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.1796 - class_loss: 0.0237 - l1_loss: 0.4156\n",
      "Epoch 456/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.4816 - class_loss: 0.0101 - l1_loss: 0.3472\n",
      "Epoch 457/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.0399 - class_loss: 0.0049 - l1_loss: 0.4035\n",
      "Epoch 458/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.8578 - class_loss: 0.0240 - l1_loss: 0.3834\n",
      "Epoch 459/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.3068 - class_loss: 0.0316 - l1_loss: 0.4275\n",
      "Epoch 460/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.4791 - class_loss: 0.0042 - l1_loss: 0.4475\n",
      "Epoch 461/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.4036 - class_loss: 0.0048 - l1_loss: 0.4399\n",
      "Epoch 462/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.8719 - class_loss: 0.8347 - l1_loss: 0.4037\n",
      "Epoch 463/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 11.9394 - class_loss: 0.2843 - l1_loss: 1.1655\n",
      "Epoch 464/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 21.0686 - class_loss: 0.0762 - l1_loss: 2.0992\n",
      "Epoch 465/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 16.2276 - class_loss: 0.4965 - l1_loss: 1.5731\n",
      "Epoch 466/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.4320 - class_loss: 0.0126 - l1_loss: 0.8419\n",
      "Epoch 467/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.4394 - class_loss: 0.0032 - l1_loss: 1.0436\n",
      "Epoch 468/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.4666 - class_loss: 0.0091 - l1_loss: 0.8458\n",
      "Epoch 469/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.3886 - class_loss: 0.0041 - l1_loss: 0.8385\n",
      "Epoch 470/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.9365 - class_loss: 0.0114 - l1_loss: 0.8925\n",
      "Epoch 471/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.3097 - class_loss: 0.0065 - l1_loss: 0.6303\n",
      "Epoch 472/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.1636 - class_loss: 0.0354 - l1_loss: 0.7128\n",
      "Epoch 473/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.1456 - class_loss: 0.0049 - l1_loss: 0.4141\n",
      "Epoch 474/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.0922 - class_loss: 0.0089 - l1_loss: 0.6083\n",
      "Epoch 475/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.4970 - class_loss: 0.0365 - l1_loss: 0.5461\n",
      "Epoch 476/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.5403 - class_loss: 0.0077 - l1_loss: 0.5533\n",
      "Epoch 477/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.8526 - class_loss: 0.0049 - l1_loss: 0.4848\n",
      "Epoch 478/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.5007 - class_loss: 0.0077 - l1_loss: 0.3493\n",
      "Epoch 479/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.4685 - class_loss: 0.0032 - l1_loss: 0.4465\n",
      "Epoch 480/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.6345 - class_loss: 0.0143 - l1_loss: 0.3620\n",
      "Epoch 481/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.5911 - class_loss: 0.0349 - l1_loss: 0.3556\n",
      "Epoch 482/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.8200 - class_loss: 0.0090 - l1_loss: 0.3811\n",
      "Epoch 483/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.4482 - class_loss: 0.0042 - l1_loss: 0.3444\n",
      "Epoch 484/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.9459 - class_loss: 0.0120 - l1_loss: 0.4934\n",
      "Epoch 485/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8078 - class_loss: 0.0226 - l1_loss: 0.4785\n",
      "Epoch 486/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7346 - class_loss: 0.0049 - l1_loss: 0.2730\n",
      "Epoch 487/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2919 - class_loss: 0.0021 - l1_loss: 0.3290\n",
      "Epoch 488/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7343 - class_loss: 0.0019 - l1_loss: 0.2732\n",
      "Epoch 489/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8340 - class_loss: 0.0140 - l1_loss: 0.2820\n",
      "Epoch 490/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2874 - class_loss: 0.0084 - l1_loss: 0.3279\n",
      "Epoch 491/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9198 - class_loss: 0.0069 - l1_loss: 0.2913\n",
      "Epoch 492/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.5478 - class_loss: 0.0030 - l1_loss: 0.2545\n",
      "Epoch 493/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.8176 - class_loss: 0.0093 - l1_loss: 0.3808\n",
      "Epoch 494/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2369 - class_loss: 0.0070 - l1_loss: 0.3230\n",
      "Epoch 495/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.4483 - class_loss: 0.0093 - l1_loss: 0.3439\n",
      "Epoch 496/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.7197 - class_loss: 0.0108 - l1_loss: 0.3709\n",
      "Epoch 497/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8674 - class_loss: 0.0063 - l1_loss: 0.2861\n",
      "Epoch 498/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0576 - class_loss: 0.0070 - l1_loss: 0.2051\n",
      "Epoch 499/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7041 - class_loss: 0.0118 - l1_loss: 0.2692\n",
      "Epoch 500/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.5789 - class_loss: 0.0174 - l1_loss: 0.2561\n",
      "Epoch 501/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8662 - class_loss: 0.0085 - l1_loss: 0.2858\n",
      "Epoch 502/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6662 - class_loss: 0.0098 - l1_loss: 0.2656\n",
      "Epoch 503/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2361 - class_loss: 0.0065 - l1_loss: 0.2230\n",
      "Epoch 504/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0998 - class_loss: 0.0047 - l1_loss: 0.3095\n",
      "Epoch 505/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.4984 - class_loss: 0.0046 - l1_loss: 0.3494\n",
      "Epoch 506/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2680 - class_loss: 0.0054 - l1_loss: 0.3263\n",
      "Epoch 507/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.7209 - class_loss: 0.0123 - l1_loss: 0.3709\n",
      "Epoch 508/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.1083 - class_loss: 0.0192 - l1_loss: 0.3089\n",
      "Epoch 509/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.0315 - class_loss: 0.0128 - l1_loss: 0.3019\n",
      "Epoch 510/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.2389 - class_loss: 0.0267 - l1_loss: 0.4212\n",
      "Epoch 511/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.7371 - class_loss: 0.0174 - l1_loss: 0.4720\n",
      "Epoch 512/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 7.8566 - class_loss: 0.0105 - l1_loss: 0.7846\n",
      "Epoch 513/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 7.2171 - class_loss: 0.0077 - l1_loss: 0.7209\n",
      "Epoch 514/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 12.6250 - class_loss: 0.0138 - l1_loss: 1.2611\n",
      "Epoch 515/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.0526 - class_loss: 0.0119 - l1_loss: 0.7041\n",
      "Epoch 516/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.9448 - class_loss: 0.0111 - l1_loss: 0.4934\n",
      "Epoch 517/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.1773 - class_loss: 0.0075 - l1_loss: 0.5170\n",
      "Epoch 518/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.1865 - class_loss: 0.0127 - l1_loss: 0.5174 0s - loss: 3.6896 - class_loss: 0.0032 - l1_loss: 0.\n",
      "Epoch 519/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.5339 - class_loss: 0.0114 - l1_loss: 0.5523\n",
      "Epoch 520/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.0478 - class_loss: 0.0071 - l1_loss: 0.5041\n",
      "Epoch 521/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.1049 - class_loss: 0.0088 - l1_loss: 0.5096\n",
      "Epoch 522/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.9920 - class_loss: 0.0199 - l1_loss: 0.5972\n",
      "Epoch 523/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.6051 - class_loss: 0.0060 - l1_loss: 0.5599\n",
      "Epoch 524/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 29ms/step - loss: 5.1426 - class_loss: 0.0035 - l1_loss: 0.5139\n",
      "Epoch 525/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.7291 - class_loss: 0.0172 - l1_loss: 0.5712\n",
      "Epoch 526/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.7109 - class_loss: 0.0255 - l1_loss: 0.6685\n",
      "Epoch 527/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.6149 - class_loss: 0.0119 - l1_loss: 0.5603\n",
      "Epoch 528/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.6493 - class_loss: 0.0047 - l1_loss: 0.5645\n",
      "Epoch 529/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.3065 - class_loss: 0.0197 - l1_loss: 0.6287\n",
      "Epoch 530/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.0702 - class_loss: 0.0068 - l1_loss: 0.8063\n",
      "Epoch 531/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.2845 - class_loss: 0.0059 - l1_loss: 0.7279\n",
      "Epoch 532/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.1268 - class_loss: 0.0114 - l1_loss: 0.7115\n",
      "Epoch 533/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.3022 - class_loss: 0.0224 - l1_loss: 0.8280 0s - loss: 6.9759 - class_loss: 0.0473 - l1_loss: \n",
      "Epoch 534/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.1928 - class_loss: 0.0069 - l1_loss: 0.8186\n",
      "Epoch 535/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.6599 - class_loss: 0.0249 - l1_loss: 1.0635\n",
      "Epoch 536/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.5772 - class_loss: 0.0109 - l1_loss: 1.0566\n",
      "Epoch 537/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.8342 - class_loss: 0.0065 - l1_loss: 1.1828\n",
      "Epoch 538/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 20.9748 - class_loss: 0.0032 - l1_loss: 2.0972\n",
      "Epoch 539/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 19.4274 - class_loss: 0.0721 - l1_loss: 1.9355\n",
      "Epoch 540/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 11.1084 - class_loss: 0.0085 - l1_loss: 1.1100\n",
      "Epoch 541/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.3619 - class_loss: 0.0042 - l1_loss: 1.2358\n",
      "Epoch 542/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.3063 - class_loss: 0.0089 - l1_loss: 1.1297\n",
      "Epoch 543/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.3878 - class_loss: 0.0679 - l1_loss: 1.5320\n",
      "Epoch 544/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.1441 - class_loss: 0.0152 - l1_loss: 1.2129\n",
      "Epoch 545/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.6405 - class_loss: 0.0085 - l1_loss: 0.6632\n",
      "Epoch 546/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.1183 - class_loss: 0.0556 - l1_loss: 1.30630s - loss: 11.5302 - class_loss: 0.0017 - l1_loss: 1.1\n",
      "Epoch 547/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.6815 - class_loss: 0.0335 - l1_loss: 0.9648\n",
      "Epoch 548/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.2371 - class_loss: 0.0062 - l1_loss: 1.1231\n",
      "Epoch 549/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.6399 - class_loss: 0.0252 - l1_loss: 0.9615\n",
      "Epoch 550/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.8584 - class_loss: 0.0101 - l1_loss: 1.0848\n",
      "Epoch 551/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 11.1105 - class_loss: 0.0041 - l1_loss: 1.1106\n",
      "Epoch 552/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.1574 - class_loss: 0.0357 - l1_loss: 1.0122\n",
      "Epoch 553/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.5031 - class_loss: 0.0091 - l1_loss: 0.9494\n",
      "Epoch 554/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.1474 - class_loss: 0.0038 - l1_loss: 1.2144\n",
      "Epoch 555/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.1332 - class_loss: 0.0084 - l1_loss: 0.9125\n",
      "Epoch 556/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.2251 - class_loss: 0.0185 - l1_loss: 0.8207\n",
      "Epoch 557/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.9402 - class_loss: 0.0060 - l1_loss: 0.5934ETA: 0s - loss: 6.2514 - class_loss: 0.0069 - l1_loss: 0.\n",
      "Epoch 558/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.2570 - class_loss: 0.0203 - l1_loss: 0.8237\n",
      "Epoch 559/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.5356 - class_loss: 0.0213 - l1_loss: 0.9514\n",
      "Epoch 560/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.7134 - class_loss: 0.0146 - l1_loss: 1.0699\n",
      "Epoch 561/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.3247 - class_loss: 0.0096 - l1_loss: 0.9315\n",
      "Epoch 562/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.5446 - class_loss: 0.0240 - l1_loss: 0.8521\n",
      "Epoch 563/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.7744 - class_loss: 0.0119 - l1_loss: 0.7763\n",
      "Epoch 564/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.2255 - class_loss: 0.0060 - l1_loss: 0.8219\n",
      "Epoch 565/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.1507 - class_loss: 0.0215 - l1_loss: 1.6129\n",
      "Epoch 566/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.8013 - class_loss: 0.0541 - l1_loss: 1.1747\n",
      "Epoch 567/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.1307 - class_loss: 0.0146 - l1_loss: 0.9116\n",
      "Epoch 568/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.3493 - class_loss: 0.0968 - l1_loss: 1.62530s - loss: 14.4035 - class_loss: 0.2502 - l1_loss: 1.4\n",
      "Epoch 569/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 30.6215 - class_loss: 0.0071 - l1_loss: 3.0614\n",
      "Epoch 570/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 35.5928 - class_loss: 0.0108 - l1_loss: 3.5582\n",
      "Epoch 571/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 32.9844 - class_loss: 0.0085 - l1_loss: 3.2976\n",
      "Epoch 572/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 26.1307 - class_loss: 0.0042 - l1_loss: 2.6127\n",
      "Epoch 573/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 26.4714 - class_loss: 0.0027 - l1_loss: 2.6469\n",
      "Epoch 574/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 28.3328 - class_loss: 0.0033 - l1_loss: 2.8329\n",
      "Epoch 575/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 24.4421 - class_loss: 0.0513 - l1_loss: 2.4391\n",
      "Epoch 576/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 23.6339 - class_loss: 0.0135 - l1_loss: 2.3620\n",
      "Epoch 577/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 20.0260 - class_loss: 0.0378 - l1_loss: 1.9988: 0s - loss: 19.7526 - class_loss: 0.0582 - l1_loss: 1.96\n",
      "Epoch 578/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.7021 - class_loss: 0.0103 - l1_loss: 1.3692\n",
      "Epoch 579/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.9035 - class_loss: 0.0333 - l1_loss: 1.6870\n",
      "Epoch 580/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.0211 - class_loss: 0.0269 - l1_loss: 0.9994\n",
      "Epoch 581/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.1449 - class_loss: 0.0935 - l1_loss: 1.0051\n",
      "Epoch 582/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.7725 - class_loss: 0.0139 - l1_loss: 1.0759\n",
      "Epoch 583/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.9418 - class_loss: 0.0055 - l1_loss: 0.6936\n",
      "Epoch 584/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.5482 - class_loss: 0.0100 - l1_loss: 0.9538\n",
      "Epoch 585/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.8727 - class_loss: 0.0466 - l1_loss: 1.0826\n",
      "Epoch 586/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.2688 - class_loss: 0.0109 - l1_loss: 0.9258\n",
      "Epoch 587/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.5028 - class_loss: 0.0254 - l1_loss: 0.7477\n",
      "Epoch 588/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.7348 - class_loss: 0.0076 - l1_loss: 0.8727\n",
      "Epoch 589/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.0421 - class_loss: 0.0396 - l1_loss: 0.7002\n",
      "Epoch 590/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.9115 - class_loss: 0.0054 - l1_loss: 0.6906\n",
      "Epoch 591/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.8132 - class_loss: 0.0061 - l1_loss: 0.5807\n",
      "Epoch 592/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3969 - class_loss: 0.0051 - l1_loss: 0.4392\n",
      "Epoch 593/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.5706 - class_loss: 0.0092 - l1_loss: 0.5561 0s - loss: 5.6804 - class_loss: 0.0085 - l1_loss: 0.\n",
      "Epoch 594/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.5316 - class_loss: 0.0071 - l1_loss: 0.5524\n",
      "Epoch 595/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.0074 - class_loss: 0.0078 - l1_loss: 0.7000\n",
      "Epoch 596/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.4764 - class_loss: 0.0758 - l1_loss: 0.4401\n",
      "Epoch 597/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.9667 - class_loss: 0.0078 - l1_loss: 0.4959\n",
      "Epoch 598/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.2286 - class_loss: 0.0033 - l1_loss: 0.5225\n",
      "Epoch 599/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5128 - class_loss: 0.0383 - l1_loss: 0.3474\n",
      "Epoch 600/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.8793 - class_loss: 0.0207 - l1_loss: 0.3859\n",
      "Epoch 601/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8854 - class_loss: 0.0051 - l1_loss: 0.4880\n",
      "Epoch 602/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0245 - class_loss: 0.0017 - l1_loss: 0.4023\n",
      "Epoch 603/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.0269 - class_loss: 0.0022 - l1_loss: 0.4025 0s - loss: 4.4396 - class_loss: 0.0015 - l1_loss: \n",
      "Epoch 604/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3806 - class_loss: 0.0055 - l1_loss: 0.4375\n",
      "Epoch 605/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.7887 - class_loss: 0.0083 - l1_loss: 0.3780\n",
      "Epoch 606/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1775 - class_loss: 0.0034 - l1_loss: 0.3174\n",
      "Epoch 607/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1025 - class_loss: 0.0104 - l1_loss: 0.4092\n",
      "Epoch 608/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.8004 - class_loss: 0.0091 - l1_loss: 0.2791\n",
      "Epoch 609/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6702 - class_loss: 0.0072 - l1_loss: 0.2663\n",
      "Epoch 610/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8694 - class_loss: 0.0166 - l1_loss: 0.2853\n",
      "Epoch 611/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2676 - class_loss: 0.0079 - l1_loss: 0.2260\n",
      "Epoch 612/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2834 - class_loss: 0.0133 - l1_loss: 0.3270\n",
      "Epoch 613/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6952 - class_loss: 0.0130 - l1_loss: 0.1682\n",
      "Epoch 614/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8473 - class_loss: 0.0076 - l1_loss: 0.1840\n",
      "Epoch 615/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.7379 - class_loss: 0.0043 - l1_loss: 0.1734\n",
      "Epoch 616/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4443 - class_loss: 0.0046 - l1_loss: 0.1440\n",
      "Epoch 617/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6193 - class_loss: 0.0220 - l1_loss: 0.1597 0s - loss: 1.6594 - class_loss: 0.0246 - l1_loss: 0.16\n",
      "Epoch 618/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3089 - class_loss: 0.0040 - l1_loss: 0.1305\n",
      "Epoch 619/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6946 - class_loss: 0.0091 - l1_loss: 0.1685\n",
      "Epoch 620/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7995 - class_loss: 0.0179 - l1_loss: 0.1782\n",
      "Epoch 621/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4432 - class_loss: 0.0067 - l1_loss: 0.1437\n",
      "Epoch 622/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5428 - class_loss: 0.0031 - l1_loss: 0.1540\n",
      "Epoch 623/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9775 - class_loss: 0.0067 - l1_loss: 0.1971 0s - loss: 2.1802 - class_loss: 0.0062 - l1_loss: 0.\n",
      "Epoch 624/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.3308 - class_loss: 0.0280 - l1_loss: 0.2303\n",
      "Epoch 625/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8386 - class_loss: 0.0072 - l1_loss: 0.1831\n",
      "Epoch 626/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5966 - class_loss: 0.0064 - l1_loss: 0.1590 0s - loss: 1.6620 - class_loss: 0.0091 - l1_loss: \n",
      "Epoch 627/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7074 - class_loss: 0.0139 - l1_loss: 0.1693\n",
      "Epoch 628/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2100 - class_loss: 0.0054 - l1_loss: 0.1205\n",
      "Epoch 629/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9124 - class_loss: 0.0090 - l1_loss: 0.1903\n",
      "Epoch 630/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7947 - class_loss: 0.0238 - l1_loss: 0.1771\n",
      "Epoch 631/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4827 - class_loss: 0.0052 - l1_loss: 0.1477\n",
      "Epoch 632/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4372 - class_loss: 0.0064 - l1_loss: 0.2431\n",
      "Epoch 633/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7597 - class_loss: 0.0151 - l1_loss: 0.2745\n",
      "Epoch 634/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3933 - class_loss: 0.0117 - l1_loss: 0.3382\n",
      "Epoch 635/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6521 - class_loss: 0.0071 - l1_loss: 0.2645\n",
      "Epoch 636/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2580 - class_loss: 0.0233 - l1_loss: 0.3235\n",
      "Epoch 637/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.1028 - class_loss: 0.0042 - l1_loss: 0.3099\n",
      "Epoch 638/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.3132 - class_loss: 0.0032 - l1_loss: 0.3310\n",
      "Epoch 639/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2927 - class_loss: 0.0067 - l1_loss: 0.4286\n",
      "Epoch 640/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9410 - class_loss: 0.0059 - l1_loss: 0.1935\n",
      "Epoch 641/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8588 - class_loss: 0.0230 - l1_loss: 0.1836\n",
      "Epoch 642/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5159 - class_loss: 0.0076 - l1_loss: 0.2508\n",
      "Epoch 643/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5814 - class_loss: 0.0081 - l1_loss: 0.2573\n",
      "Epoch 644/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.1925 - class_loss: 0.0070 - l1_loss: 0.4185\n",
      "Epoch 645/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.9398 - class_loss: 0.0075 - l1_loss: 0.3932\n",
      "Epoch 646/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2080 - class_loss: 0.0059 - l1_loss: 0.3202 0s - loss: 3.3066 - class_loss: 0.0066 - l1_loss: 0.33\n",
      "Epoch 647/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5054 - class_loss: 0.0045 - l1_loss: 0.2501\n",
      "Epoch 648/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3941 - class_loss: 0.0136 - l1_loss: 0.2380\n",
      "Epoch 649/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.8422 - class_loss: 0.0243 - l1_loss: 0.2818\n",
      "Epoch 650/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1810 - class_loss: 0.0281 - l1_loss: 0.3153\n",
      "Epoch 651/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3858 - class_loss: 0.0053 - l1_loss: 0.3380\n",
      "Epoch 652/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4690 - class_loss: 0.0031 - l1_loss: 0.3466\n",
      "Epoch 653/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1656 - class_loss: 0.0099 - l1_loss: 0.3156\n",
      "Epoch 654/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4779 - class_loss: 0.0099 - l1_loss: 0.3468\n",
      "Epoch 655/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8395 - class_loss: 0.0125 - l1_loss: 0.3827\n",
      "Epoch 656/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5826 - class_loss: 0.0086 - l1_loss: 0.3574\n",
      "Epoch 657/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.1846 - class_loss: 0.0566 - l1_loss: 0.5128\n",
      "Epoch 658/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.8261 - class_loss: 0.0089 - l1_loss: 0.3817\n",
      "Epoch 659/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.5086 - class_loss: 0.0112 - l1_loss: 0.3497\n",
      "Epoch 660/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6000 - class_loss: 0.0130 - l1_loss: 0.3587\n",
      "Epoch 661/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7550 - class_loss: 0.0093 - l1_loss: 0.2746\n",
      "Epoch 662/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2727 - class_loss: 0.0073 - l1_loss: 0.3265\n",
      "Epoch 663/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.4523 - class_loss: 0.0060 - l1_loss: 0.2446\n",
      "Epoch 664/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8184 - class_loss: 0.0093 - l1_loss: 0.2809\n",
      "Epoch 665/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7321 - class_loss: 0.0030 - l1_loss: 0.2729\n",
      "Epoch 666/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3727 - class_loss: 0.0089 - l1_loss: 0.2364\n",
      "Epoch 667/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2456 - class_loss: 0.0117 - l1_loss: 0.2234\n",
      "Epoch 668/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6901 - class_loss: 0.0065 - l1_loss: 0.2684\n",
      "Epoch 669/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2530 - class_loss: 0.0029 - l1_loss: 0.2250\n",
      "Epoch 670/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9375 - class_loss: 0.0071 - l1_loss: 0.1930\n",
      "Epoch 671/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0518 - class_loss: 0.0075 - l1_loss: 0.2044\n",
      "Epoch 672/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1463 - class_loss: 0.0077 - l1_loss: 0.2139\n",
      "Epoch 673/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1157 - class_loss: 0.0083 - l1_loss: 0.2107\n",
      "Epoch 674/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.5550 - class_loss: 0.0074 - l1_loss: 0.2548\n",
      "Epoch 675/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.5706 - class_loss: 0.0208 - l1_loss: 0.1550\n",
      "Epoch 676/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.7636 - class_loss: 0.0151 - l1_loss: 0.1748\n",
      "Epoch 677/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4668 - class_loss: 0.0059 - l1_loss: 0.1461\n",
      "Epoch 678/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3252 - class_loss: 0.0049 - l1_loss: 0.2320\n",
      "Epoch 679/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6464 - class_loss: 0.0299 - l1_loss: 0.1616\n",
      "Epoch 680/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4775 - class_loss: 0.0102 - l1_loss: 0.1467\n",
      "Epoch 681/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5712 - class_loss: 0.0026 - l1_loss: 0.1569\n",
      "Epoch 682/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4268 - class_loss: 0.0115 - l1_loss: 0.1415\n",
      "Epoch 683/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4723 - class_loss: 0.0105 - l1_loss: 0.1462\n",
      "Epoch 684/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6140 - class_loss: 0.0097 - l1_loss: 0.1604\n",
      "Epoch 685/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7514 - class_loss: 0.0081 - l1_loss: 0.1743\n",
      "Epoch 686/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3810 - class_loss: 0.0076 - l1_loss: 0.2373\n",
      "Epoch 687/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8667 - class_loss: 0.0049 - l1_loss: 0.1862\n",
      "Epoch 688/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8185 - class_loss: 0.0124 - l1_loss: 0.1806\n",
      "Epoch 689/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5955 - class_loss: 0.0043 - l1_loss: 0.1591\n",
      "Epoch 690/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0360 - class_loss: 0.0114 - l1_loss: 0.2025\n",
      "Epoch 691/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.2612 - class_loss: 0.0271 - l1_loss: 0.2234\n",
      "Epoch 692/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.5258 - class_loss: 0.0042 - l1_loss: 0.2522\n",
      "Epoch 693/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.2546 - class_loss: 0.0128 - l1_loss: 0.4242\n",
      "Epoch 694/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.6231 - class_loss: 0.0084 - l1_loss: 0.9615\n",
      "Epoch 695/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.8783 - class_loss: 0.0102 - l1_loss: 0.7868\n",
      "Epoch 696/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.5592 - class_loss: 0.0111 - l1_loss: 0.5548\n",
      "Epoch 697/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.3199 - class_loss: 0.0077 - l1_loss: 0.8312\n",
      "Epoch 698/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.0091 - class_loss: 0.0128 - l1_loss: 0.7996\n",
      "Epoch 699/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.6269 - class_loss: 0.0067 - l1_loss: 0.5620\n",
      "Epoch 700/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 8.1232 - class_loss: 0.1181 - l1_loss: 0.8005\n",
      "Epoch 701/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.1057 - class_loss: 0.0157 - l1_loss: 0.6090\n",
      "Epoch 702/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 6.8497 - class_loss: 0.0071 - l1_loss: 0.6843\n",
      "Epoch 703/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.3925 - class_loss: 0.0244 - l1_loss: 0.7368\n",
      "Epoch 704/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.3606 - class_loss: 0.0031 - l1_loss: 1.0358\n",
      "Epoch 705/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.6155 - class_loss: 0.0038 - l1_loss: 0.9612\n",
      "Epoch 706/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.5811 - class_loss: 0.0198 - l1_loss: 0.7561\n",
      "Epoch 707/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 22.2588 - class_loss: 0.0209 - l1_loss: 2.2238\n",
      "Epoch 708/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.4701 - class_loss: 0.0033 - l1_loss: 1.4467\n",
      "Epoch 709/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 21.3162 - class_loss: 0.0096 - l1_loss: 2.1307\n",
      "Epoch 710/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 19.7710 - class_loss: 0.0267 - l1_loss: 1.9744\n",
      "Epoch 711/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 18.2702 - class_loss: 0.0137 - l1_loss: 1.8257\n",
      "Epoch 712/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 22.0034 - class_loss: 0.0837 - l1_loss: 2.1920\n",
      "Epoch 713/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 19.6729 - class_loss: 0.0285 - l1_loss: 1.9644\n",
      "Epoch 714/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.8415 - class_loss: 0.0047 - l1_loss: 1.5837\n",
      "Epoch 715/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 39.9691 - class_loss: 0.0154 - l1_loss: 3.9954\n",
      "Epoch 716/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 18.0429 - class_loss: 0.0415 - l1_loss: 1.8001\n",
      "Epoch 717/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 19.7066 - class_loss: 0.0153 - l1_loss: 1.9691\n",
      "Epoch 718/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 19.5695 - class_loss: 0.0237 - l1_loss: 1.9546\n",
      "Epoch 719/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 19.5178 - class_loss: 0.0644 - l1_loss: 1.9453\n",
      "Epoch 720/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.4672 - class_loss: 0.0046 - l1_loss: 1.4463: 0s - loss: 15.4426 - class_loss: 0.0052 - l1_loss: 1.543\n",
      "Epoch 721/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.1622 - class_loss: 0.0156 - l1_loss: 1.6147\n",
      "Epoch 722/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 26.3407 - class_loss: 0.0058 - l1_loss: 2.6335\n",
      "Epoch 723/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 22.0027 - class_loss: 0.0907 - l1_loss: 2.1912\n",
      "Epoch 724/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 26.2846 - class_loss: 0.0189 - l1_loss: 2.6266\n",
      "Epoch 725/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 26.2671 - class_loss: 0.0365 - l1_loss: 2.6231\n",
      "Epoch 726/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 25.7273 - class_loss: 0.0358 - l1_loss: 2.5691\n",
      "Epoch 727/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 35.4784 - class_loss: 0.0264 - l1_loss: 3.5452\n",
      "Epoch 728/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 32.6985 - class_loss: 0.0619 - l1_loss: 3.2637\n",
      "Epoch 729/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 19.7894 - class_loss: 0.0089 - l1_loss: 1.9781\n",
      "Epoch 730/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 21.6898 - class_loss: 0.0140 - l1_loss: 2.1676\n",
      "Epoch 731/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.2271 - class_loss: 0.0267 - l1_loss: 1.6200\n",
      "Epoch 732/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 21.8396 - class_loss: 0.0234 - l1_loss: 2.1816\n",
      "Epoch 733/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.3625 - class_loss: 0.0051 - l1_loss: 1.4357\n",
      "Epoch 734/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.6035 - class_loss: 0.0159 - l1_loss: 1.5588\n",
      "Epoch 735/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.8957 - class_loss: 0.0095 - l1_loss: 1.4886\n",
      "Epoch 736/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.8183 - class_loss: 0.0178 - l1_loss: 0.9800 0s - loss: 9.3212 - class_loss: 0.0051 - l1_loss: 0.93\n",
      "Epoch 737/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.9734 - class_loss: 0.0356 - l1_loss: 1.2938\n",
      "Epoch 738/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.8892 - class_loss: 0.0021 - l1_loss: 0.8887\n",
      "Epoch 739/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.4125 - class_loss: 0.0017 - l1_loss: 1.2411\n",
      "Epoch 740/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.6789 - class_loss: 0.0678 - l1_loss: 0.9611\n",
      "Epoch 741/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.1644 - class_loss: 0.0075 - l1_loss: 1.0157\n",
      "Epoch 742/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.6289 - class_loss: 0.0024 - l1_loss: 0.6627\n",
      "Epoch 743/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.0543 - class_loss: 0.0039 - l1_loss: 0.6050\n",
      "Epoch 744/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.0735 - class_loss: 0.0313 - l1_loss: 0.7042\n",
      "Epoch 745/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.4582 - class_loss: 0.0072 - l1_loss: 0.4451\n",
      "Epoch 746/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.1749 - class_loss: 0.0078 - l1_loss: 0.6167\n",
      "Epoch 747/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.1062 - class_loss: 0.0068 - l1_loss: 0.6099\n",
      "Epoch 748/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.8709 - class_loss: 0.0227 - l1_loss: 0.4848\n",
      "Epoch 749/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.6362 - class_loss: 0.0118 - l1_loss: 0.3624\n",
      "Epoch 750/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6804 - class_loss: 0.0049 - l1_loss: 0.2676\n",
      "Epoch 751/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.4254 - class_loss: 0.0042 - l1_loss: 0.4421\n",
      "Epoch 752/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6900 - class_loss: 0.0118 - l1_loss: 0.2678\n",
      "Epoch 753/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.6687 - class_loss: 0.0154 - l1_loss: 0.3653\n",
      "Epoch 754/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0002 - class_loss: 0.0086 - l1_loss: 0.2992\n",
      "Epoch 755/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.8830 - class_loss: 0.0073 - l1_loss: 0.2876\n",
      "Epoch 756/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8246 - class_loss: 0.0082 - l1_loss: 0.3816\n",
      "Epoch 757/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2458 - class_loss: 0.0162 - l1_loss: 0.3230\n",
      "Epoch 758/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4783 - class_loss: 0.0164 - l1_loss: 0.3462 0s - loss: 3.0791 - class_loss: 0.0234 - l1_loss: 0.\n",
      "Epoch 759/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.8928 - class_loss: 0.0030 - l1_loss: 0.2890\n",
      "Epoch 760/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.7662 - class_loss: 0.0076 - l1_loss: 0.2759\n",
      "Epoch 761/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.4209 - class_loss: 0.0116 - l1_loss: 0.3409\n",
      "Epoch 762/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1629 - class_loss: 0.0067 - l1_loss: 0.3156\n",
      "Epoch 763/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.5912 - class_loss: 0.0061 - l1_loss: 0.2585\n",
      "Epoch 764/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6082 - class_loss: 0.0046 - l1_loss: 0.1604\n",
      "Epoch 765/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.5619 - class_loss: 0.0115 - l1_loss: 0.2550\n",
      "Epoch 766/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9964 - class_loss: 0.0042 - l1_loss: 0.1992\n",
      "Epoch 767/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0237 - class_loss: 0.0072 - l1_loss: 0.2017\n",
      "Epoch 768/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5751 - class_loss: 0.0058 - l1_loss: 0.1569\n",
      "Epoch 769/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8296 - class_loss: 0.0079 - l1_loss: 0.1822\n",
      "Epoch 770/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6784 - class_loss: 0.0046 - l1_loss: 0.1674\n",
      "Epoch 771/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4029 - class_loss: 0.0100 - l1_loss: 0.1393\n",
      "Epoch 772/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.7435 - class_loss: 0.0131 - l1_loss: 0.1730\n",
      "Epoch 773/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.0956 - class_loss: 0.0241 - l1_loss: 0.2072\n",
      "Epoch 774/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6986 - class_loss: 0.0041 - l1_loss: 0.1695\n",
      "Epoch 775/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.5186 - class_loss: 0.0047 - l1_loss: 0.2514\n",
      "Epoch 776/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.4858 - class_loss: 0.0134 - l1_loss: 0.2472\n",
      "Epoch 777/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2810 - class_loss: 0.0058 - l1_loss: 0.3275 0s - loss: 2.7688 - class_loss: 0.0058 - l1_loss: 0.\n",
      "Epoch 778/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5172 - class_loss: 0.0031 - l1_loss: 0.2514\n",
      "Epoch 779/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6175 - class_loss: 0.0030 - l1_loss: 0.2615\n",
      "Epoch 780/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7940 - class_loss: 0.0243 - l1_loss: 0.1770\n",
      "Epoch 781/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8011 - class_loss: 0.0217 - l1_loss: 0.1779\n",
      "Epoch 782/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4914 - class_loss: 0.0019 - l1_loss: 0.2490\n",
      "Epoch 783/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4699 - class_loss: 0.0028 - l1_loss: 0.2467\n",
      "Epoch 784/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.6592 - class_loss: 0.0223 - l1_loss: 0.3637\n",
      "Epoch 785/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1574 - class_loss: 0.0069 - l1_loss: 0.4150\n",
      "Epoch 786/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1500 - class_loss: 0.0070 - l1_loss: 0.4143\n",
      "Epoch 787/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 31ms/step - loss: 5.6947 - class_loss: 0.0268 - l1_loss: 0.5668\n",
      "Epoch 788/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.2364 - class_loss: 0.0557 - l1_loss: 0.5181\n",
      "Epoch 789/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.5081 - class_loss: 0.0027 - l1_loss: 0.7505\n",
      "Epoch 790/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.7753 - class_loss: 0.0148 - l1_loss: 0.5761\n",
      "Epoch 791/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.6685 - class_loss: 0.0114 - l1_loss: 0.4657\n",
      "Epoch 792/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.3150 - class_loss: 0.0083 - l1_loss: 0.5307\n",
      "Epoch 793/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.3441 - class_loss: 0.0182 - l1_loss: 0.6326\n",
      "Epoch 794/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.8154 - class_loss: 0.0488 - l1_loss: 0.5767\n",
      "Epoch 795/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.7950 - class_loss: 0.0034 - l1_loss: 0.6792\n",
      "Epoch 796/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.5878 - class_loss: 0.0033 - l1_loss: 0.6584\n",
      "Epoch 797/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.0478 - class_loss: 0.0167 - l1_loss: 0.7031\n",
      "Epoch 798/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.4786 - class_loss: 0.0641 - l1_loss: 0.9415\n",
      "Epoch 799/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 12.9412 - class_loss: 0.0471 - l1_loss: 1.2894\n",
      "Epoch 800/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.7327 - class_loss: 0.0050 - l1_loss: 1.6728\n",
      "Epoch 801/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.2541 - class_loss: 0.0255 - l1_loss: 1.1229\n",
      "Epoch 802/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.7259 - class_loss: 0.0190 - l1_loss: 1.0707\n",
      "Epoch 803/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.1839 - class_loss: 0.0046 - l1_loss: 1.2179\n",
      "Epoch 804/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 13.7154 - class_loss: 0.0805 - l1_loss: 1.3635\n",
      "Epoch 805/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 12.0386 - class_loss: 0.0142 - l1_loss: 1.2024\n",
      "Epoch 806/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.8289 - class_loss: 0.0086 - l1_loss: 0.8820\n",
      "Epoch 807/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.6989 - class_loss: 0.0096 - l1_loss: 0.7689\n",
      "Epoch 808/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.4639 - class_loss: 0.0403 - l1_loss: 0.7424\n",
      "Epoch 809/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.7466 - class_loss: 0.0037 - l1_loss: 0.7743\n",
      "Epoch 810/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.9080 - class_loss: 0.0131 - l1_loss: 0.8895\n",
      "Epoch 811/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.7796 - class_loss: 0.0305 - l1_loss: 0.7749\n",
      "Epoch 812/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.5629 - class_loss: 0.0080 - l1_loss: 0.7555A: 0s - loss: 10.6263 - class_loss: 0.0102 - l1_loss: 1\n",
      "Epoch 813/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.5248 - class_loss: 0.0300 - l1_loss: 0.6495\n",
      "Epoch 814/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.9211 - class_loss: 0.0024 - l1_loss: 0.6919\n",
      "Epoch 815/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.2511 - class_loss: 0.0173 - l1_loss: 0.6234 0s - loss: 6.9140 - class_loss: 0.0208 - l1_loss: 0.\n",
      "Epoch 816/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.3257 - class_loss: 0.0229 - l1_loss: 0.8303\n",
      "Epoch 817/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.5737 - class_loss: 0.0022 - l1_loss: 0.6571\n",
      "Epoch 818/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.6315 - class_loss: 0.0132 - l1_loss: 0.4618\n",
      "Epoch 819/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.3806 - class_loss: 0.0096 - l1_loss: 0.5371\n",
      "Epoch 820/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.0713 - class_loss: 0.0072 - l1_loss: 0.6064\n",
      "Epoch 821/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.6242 - class_loss: 0.0464 - l1_loss: 0.4578\n",
      "Epoch 822/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.4864 - class_loss: 0.0159 - l1_loss: 0.4471 0s - loss: 4.7567 - class_loss: 0.0238 - l1_loss: 0.\n",
      "Epoch 823/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3452 - class_loss: 0.0024 - l1_loss: 0.3343\n",
      "Epoch 824/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6855 - class_loss: 0.0027 - l1_loss: 0.3683\n",
      "Epoch 825/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.1238 - class_loss: 0.0126 - l1_loss: 0.4111\n",
      "Epoch 826/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.3538 - class_loss: 0.0075 - l1_loss: 0.4346\n",
      "Epoch 827/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2733 - class_loss: 0.0102 - l1_loss: 0.4263 0s - loss: 3.9500 - class_loss: 0.0062 - l1_loss: 0.\n",
      "Epoch 828/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.8588 - class_loss: 0.0563 - l1_loss: 0.4802\n",
      "Epoch 829/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.7114 - class_loss: 0.0047 - l1_loss: 0.5707\n",
      "Epoch 830/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.9211 - class_loss: 0.0058 - l1_loss: 0.6915 0s - loss: 7.4139 - class_loss: 0.0051 - l1_loss: 0.74\n",
      "Epoch 831/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.4974 - class_loss: 0.0039 - l1_loss: 0.7494 0s - loss: 7.3615 - class_loss: 0.0043 - l1_loss: 0.73\n",
      "Epoch 832/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.8923 - class_loss: 0.0070 - l1_loss: 0.5885\n",
      "Epoch 833/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.9086 - class_loss: 0.0322 - l1_loss: 0.5876\n",
      "Epoch 834/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.0288 - class_loss: 0.0051 - l1_loss: 0.5024\n",
      "Epoch 835/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.6375 - class_loss: 0.0119 - l1_loss: 0.6626\n",
      "Epoch 836/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.4474 - class_loss: 0.0516 - l1_loss: 0.4396\n",
      "Epoch 837/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.8368 - class_loss: 0.0089 - l1_loss: 0.5828\n",
      "Epoch 838/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.8983 - class_loss: 0.0087 - l1_loss: 0.5890\n",
      "Epoch 839/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.3824 - class_loss: 0.0088 - l1_loss: 0.5374\n",
      "Epoch 840/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.3988 - class_loss: 0.0065 - l1_loss: 0.7392\n",
      "Epoch 841/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.8898 - class_loss: 0.0060 - l1_loss: 1.0884\n",
      "Epoch 842/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.5700 - class_loss: 0.0124 - l1_loss: 0.9558A: 0s - loss: 8.8094 - class_loss: 0.0022 - l1_loss: 0.8\n",
      "Epoch 843/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.7908 - class_loss: 0.0071 - l1_loss: 0.6784\n",
      "Epoch 844/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.2056 - class_loss: 0.0097 - l1_loss: 0.5196\n",
      "Epoch 845/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8170 - class_loss: 0.0221 - l1_loss: 0.4795 0s - loss: 3.2565 - class_loss: 0.0065 - l1_loss: \n",
      "Epoch 846/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.6044 - class_loss: 0.0116 - l1_loss: 0.4593\n",
      "Epoch 847/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.0868 - class_loss: 0.0054 - l1_loss: 0.4081\n",
      "Epoch 848/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9011 - class_loss: 0.0155 - l1_loss: 0.2886\n",
      "Epoch 849/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0064 - class_loss: 0.0041 - l1_loss: 0.4002 0s - loss: 3.4475 - class_loss: 0.0044 - l1_loss: 0.\n",
      "Epoch 850/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9398 - class_loss: 0.0036 - l1_loss: 0.2936\n",
      "Epoch 851/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.9580 - class_loss: 0.0092 - l1_loss: 0.3949\n",
      "Epoch 852/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1646 - class_loss: 0.0431 - l1_loss: 0.3121\n",
      "Epoch 853/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9907 - class_loss: 0.0060 - l1_loss: 0.2985\n",
      "Epoch 854/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6805 - class_loss: 0.0053 - l1_loss: 0.3675\n",
      "Epoch 855/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6358 - class_loss: 0.0112 - l1_loss: 0.3625\n",
      "Epoch 856/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0224 - class_loss: 0.0139 - l1_loss: 0.3009 0s - loss: 3.1336 - class_loss: 0.0130 - l1_loss: 0.31\n",
      "Epoch 857/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.9196 - class_loss: 0.0087 - l1_loss: 0.2911\n",
      "Epoch 858/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3175 - class_loss: 0.0022 - l1_loss: 0.3315\n",
      "Epoch 859/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8245 - class_loss: 0.0084 - l1_loss: 0.2816\n",
      "Epoch 860/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.3530 - class_loss: 0.0162 - l1_loss: 0.3337\n",
      "Epoch 861/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5324 - class_loss: 0.0030 - l1_loss: 0.3529\n",
      "Epoch 862/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8854 - class_loss: 0.0028 - l1_loss: 0.2883\n",
      "Epoch 863/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.4670 - class_loss: 0.0036 - l1_loss: 0.3463\n",
      "Epoch 864/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.5185 - class_loss: 0.0071 - l1_loss: 0.2511\n",
      "Epoch 865/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.0684 - class_loss: 0.0060 - l1_loss: 0.3062\n",
      "Epoch 866/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.0374 - class_loss: 0.0074 - l1_loss: 0.2030\n",
      "Epoch 867/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2016 - class_loss: 0.0198 - l1_loss: 0.2182\n",
      "Epoch 868/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3130 - class_loss: 0.0460 - l1_loss: 0.2267\n",
      "Epoch 869/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.9549 - class_loss: 0.0019 - l1_loss: 0.2953\n",
      "Epoch 870/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.7480 - class_loss: 0.0033 - l1_loss: 0.2745\n",
      "Epoch 871/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.2123 - class_loss: 0.0311 - l1_loss: 0.3181\n",
      "Epoch 872/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.9286 - class_loss: 0.0058 - l1_loss: 0.3923\n",
      "Epoch 873/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8100 - class_loss: 0.0025 - l1_loss: 0.2808\n",
      "Epoch 874/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.3765 - class_loss: 0.0063 - l1_loss: 0.2370\n",
      "Epoch 875/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7858 - class_loss: 0.0036 - l1_loss: 0.2782\n",
      "Epoch 876/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.4230 - class_loss: 0.0148 - l1_loss: 0.3408\n",
      "Epoch 877/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1290 - class_loss: 0.0159 - l1_loss: 0.3113\n",
      "Epoch 878/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8915 - class_loss: 0.0149 - l1_loss: 0.2877\n",
      "Epoch 879/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5236 - class_loss: 0.0044 - l1_loss: 0.2519\n",
      "Epoch 880/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7831 - class_loss: 0.0100 - l1_loss: 0.3773\n",
      "Epoch 881/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9369 - class_loss: 0.0071 - l1_loss: 0.2930\n",
      "Epoch 882/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2602 - class_loss: 0.0036 - l1_loss: 0.2257\n",
      "Epoch 883/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.6107 - class_loss: 0.0144 - l1_loss: 0.2596\n",
      "Epoch 884/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0880 - class_loss: 0.0062 - l1_loss: 0.3082\n",
      "Epoch 885/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.5651 - class_loss: 0.0051 - l1_loss: 0.3560\n",
      "Epoch 886/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.2015 - class_loss: 0.0138 - l1_loss: 0.3188\n",
      "Epoch 887/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7682 - class_loss: 0.0066 - l1_loss: 0.3762\n",
      "Epoch 888/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.8527 - class_loss: 0.0019 - l1_loss: 0.3851\n",
      "Epoch 889/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1224 - class_loss: 0.0059 - l1_loss: 0.3116\n",
      "Epoch 890/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.5568 - class_loss: 0.0041 - l1_loss: 0.3553\n",
      "Epoch 891/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.4162 - class_loss: 0.0068 - l1_loss: 0.3409\n",
      "Epoch 892/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8814 - class_loss: 0.0182 - l1_loss: 0.2863\n",
      "Epoch 893/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9389 - class_loss: 0.0138 - l1_loss: 0.2925\n",
      "Epoch 894/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.5561 - class_loss: 0.0036 - l1_loss: 0.4553\n",
      "Epoch 895/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 5.9286 - class_loss: 0.0048 - l1_loss: 0.5924\n",
      "Epoch 896/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.5821 - class_loss: 0.0113 - l1_loss: 0.5571\n",
      "Epoch 897/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 6.3631 - class_loss: 0.0043 - l1_loss: 0.6359\n",
      "Epoch 898/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 7.6899 - class_loss: 0.0146 - l1_loss: 0.7675\n",
      "Epoch 899/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.5052 - class_loss: 0.0052 - l1_loss: 0.9500\n",
      "Epoch 900/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 7.5963 - class_loss: 0.0066 - l1_loss: 0.7590\n",
      "Epoch 901/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 7.3253 - class_loss: 0.0188 - l1_loss: 0.7307\n",
      "Epoch 902/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.5264 - class_loss: 0.0052 - l1_loss: 0.7521\n",
      "Epoch 903/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 9.4316 - class_loss: 0.0092 - l1_loss: 0.9422\n",
      "Epoch 904/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.8068 - class_loss: 0.0151 - l1_loss: 0.6792\n",
      "Epoch 905/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 8.1182 - class_loss: 0.0309 - l1_loss: 0.8087\n",
      "Epoch 906/5000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 9.7997 - class_loss: 0.0558 - l1_loss: 0.9744\n",
      "Epoch 907/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 7.7923 - class_loss: 0.0077 - l1_loss: 0.7785\n",
      "Epoch 908/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 5.9955 - class_loss: 0.0142 - l1_loss: 0.5981\n",
      "Epoch 909/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 5.1868 - class_loss: 0.0049 - l1_loss: 0.5182\n",
      "Epoch 910/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.1794 - class_loss: 0.0030 - l1_loss: 0.6176\n",
      "Epoch 911/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.2973 - class_loss: 0.0092 - l1_loss: 0.9288\n",
      "Epoch 912/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.3429 - class_loss: 0.0053 - l1_loss: 0.7338\n",
      "Epoch 913/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.2503 - class_loss: 0.0331 - l1_loss: 0.9217\n",
      "Epoch 914/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.6268 - class_loss: 0.0298 - l1_loss: 0.9597\n",
      "Epoch 915/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.4891 - class_loss: 0.0028 - l1_loss: 1.2486\n",
      "Epoch 916/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.0645 - class_loss: 0.0016 - l1_loss: 1.6063\n",
      "Epoch 917/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 31ms/step - loss: 11.9976 - class_loss: 0.0035 - l1_loss: 1.1994\n",
      "Epoch 918/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 21.5340 - class_loss: 0.0091 - l1_loss: 2.1525\n",
      "Epoch 919/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 19.8556 - class_loss: 0.0762 - l1_loss: 1.9779\n",
      "Epoch 920/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 36.8797 - class_loss: 0.0046 - l1_loss: 3.6875\n",
      "Epoch 921/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 33.0425 - class_loss: 0.0209 - l1_loss: 3.3022\n",
      "Epoch 922/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 24.0979 - class_loss: 0.0196 - l1_loss: 2.4078\n",
      "Epoch 923/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 31.9230 - class_loss: 0.0076 - l1_loss: 3.1915\n",
      "Epoch 924/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 20.6469 - class_loss: 0.0404 - l1_loss: 2.0606\n",
      "Epoch 925/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 24.2458 - class_loss: 0.0049 - l1_loss: 2.4241\n",
      "Epoch 926/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 15.5373 - class_loss: 0.0335 - l1_loss: 1.5504\n",
      "Epoch 927/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 16.3705 - class_loss: 0.0211 - l1_loss: 1.6349\n",
      "Epoch 928/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 14.1914 - class_loss: 0.0032 - l1_loss: 1.4188\n",
      "Epoch 929/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 17.8088 - class_loss: 0.0026 - l1_loss: 1.7806\n",
      "Epoch 930/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 20.3053 - class_loss: 0.0084 - l1_loss: 2.0297\n",
      "Epoch 931/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 19.5371 - class_loss: 0.0014 - l1_loss: 1.9536\n",
      "Epoch 932/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 21.8445 - class_loss: 0.0239 - l1_loss: 2.1821\n",
      "Epoch 933/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 23.3604 - class_loss: 0.0023 - l1_loss: 2.3358\n",
      "Epoch 934/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 19.8892 - class_loss: 0.0115 - l1_loss: 1.9878\n",
      "Epoch 935/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 19.7100 - class_loss: 0.0063 - l1_loss: 1.9704\n",
      "Epoch 936/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 27.2575 - class_loss: 0.2061 - l1_loss: 2.7051\n",
      "Epoch 937/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 24.3356 - class_loss: 0.0157 - l1_loss: 2.4320\n",
      "Epoch 938/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 23.2143 - class_loss: 0.0234 - l1_loss: 2.3191\n",
      "Epoch 939/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 22.7128 - class_loss: 0.0072 - l1_loss: 2.2706\n",
      "Epoch 940/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 19.3907 - class_loss: 0.1265 - l1_loss: 1.9264\n",
      "Epoch 941/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 25.3335 - class_loss: 0.0053 - l1_loss: 2.5328\n",
      "Epoch 942/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 24.6553 - class_loss: 0.0044 - l1_loss: 2.4651\n",
      "Epoch 943/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 24.4176 - class_loss: 0.0139 - l1_loss: 2.4404\n",
      "Epoch 944/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 22.1837 - class_loss: 0.0308 - l1_loss: 2.2153\n",
      "Epoch 945/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 24.9513 - class_loss: 0.0229 - l1_loss: 2.4928\n",
      "Epoch 946/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 30.9949 - class_loss: 0.0122 - l1_loss: 3.0983\n",
      "Epoch 947/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 17.1465 - class_loss: 0.0038 - l1_loss: 1.7143: 0s - loss: 17.2938 - class_loss: 0.0053 - l1_loss: 1.72\n",
      "Epoch 948/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.2375 - class_loss: 0.0081 - l1_loss: 1.2229\n",
      "Epoch 949/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.4262 - class_loss: 0.0100 - l1_loss: 1.5416\n",
      "Epoch 950/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.3107 - class_loss: 0.0086 - l1_loss: 0.9302\n",
      "Epoch 951/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.4611 - class_loss: 0.0097 - l1_loss: 1.2451\n",
      "Epoch 952/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.4370 - class_loss: 0.0058 - l1_loss: 1.1431\n",
      "Epoch 953/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.2958 - class_loss: 0.0098 - l1_loss: 0.7286\n",
      "Epoch 954/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.6959 - class_loss: 0.0030 - l1_loss: 0.6693\n",
      "Epoch 955/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.4404 - class_loss: 0.0267 - l1_loss: 0.5414\n",
      "Epoch 956/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0067 - class_loss: 0.0075 - l1_loss: 0.3999\n",
      "Epoch 957/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.1650 - class_loss: 0.0026 - l1_loss: 0.4162\n",
      "Epoch 958/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8493 - class_loss: 0.0032 - l1_loss: 0.3846\n",
      "Epoch 959/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.7342 - class_loss: 0.0218 - l1_loss: 0.3712\n",
      "Epoch 960/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.4370 - class_loss: 0.0057 - l1_loss: 0.5431\n",
      "Epoch 961/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3643 - class_loss: 0.0051 - l1_loss: 0.4359\n",
      "Epoch 962/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.2754 - class_loss: 0.0074 - l1_loss: 0.4268\n",
      "Epoch 963/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.3836 - class_loss: 0.0399 - l1_loss: 0.4344\n",
      "Epoch 964/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.2142 - class_loss: 0.0041 - l1_loss: 0.3210\n",
      "Epoch 965/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.6269 - class_loss: 0.0036 - l1_loss: 0.3623\n",
      "Epoch 966/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3213 - class_loss: 0.0053 - l1_loss: 0.3316\n",
      "Epoch 967/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3439 - class_loss: 0.0853 - l1_loss: 0.4259\n",
      "Epoch 968/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6754 - class_loss: 0.0049 - l1_loss: 0.3671\n",
      "Epoch 969/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.4697 - class_loss: 0.0027 - l1_loss: 0.3467\n",
      "Epoch 970/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.7323 - class_loss: 0.0022 - l1_loss: 0.4730\n",
      "Epoch 971/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5192 - class_loss: 0.0034 - l1_loss: 0.2516\n",
      "Epoch 972/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5004 - class_loss: 0.0045 - l1_loss: 0.2496\n",
      "Epoch 973/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9462 - class_loss: 0.0055 - l1_loss: 0.1941 0s - loss: 1.8281 - class_loss: 0.0059 - l1_loss: 0.18\n",
      "Epoch 974/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9545 - class_loss: 0.0091 - l1_loss: 0.1945\n",
      "Epoch 975/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7774 - class_loss: 0.0134 - l1_loss: 0.1764 0s - loss: 1.7221 - class_loss: 0.0099 - l1_loss: 0.\n",
      "Epoch 976/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8559 - class_loss: 0.0040 - l1_loss: 0.1852\n",
      "Epoch 977/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5326 - class_loss: 0.0036 - l1_loss: 0.1529\n",
      "Epoch 978/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3938 - class_loss: 0.0289 - l1_loss: 0.1365\n",
      "Epoch 979/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2188 - class_loss: 0.0046 - l1_loss: 0.1214\n",
      "Epoch 980/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5248 - class_loss: 0.0018 - l1_loss: 0.1523\n",
      "Epoch 981/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6580 - class_loss: 0.0047 - l1_loss: 0.1653\n",
      "Epoch 982/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4548 - class_loss: 0.0041 - l1_loss: 0.1451\n",
      "Epoch 983/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0259 - class_loss: 0.0040 - l1_loss: 0.1022\n",
      "Epoch 984/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9371 - class_loss: 0.0035 - l1_loss: 0.0934\n",
      "Epoch 985/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8720 - class_loss: 0.0041 - l1_loss: 0.0868\n",
      "Epoch 986/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9808 - class_loss: 0.0046 - l1_loss: 0.0976\n",
      "Epoch 987/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9281 - class_loss: 0.0072 - l1_loss: 0.0921\n",
      "Epoch 988/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1988 - class_loss: 0.0045 - l1_loss: 0.1194\n",
      "Epoch 989/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1516 - class_loss: 0.0067 - l1_loss: 0.1145\n",
      "Epoch 990/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0696 - class_loss: 0.0039 - l1_loss: 0.1066\n",
      "Epoch 991/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0026 - class_loss: 0.0060 - l1_loss: 0.0997\n",
      "Epoch 992/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0952 - class_loss: 0.0054 - l1_loss: 0.1090\n",
      "Epoch 993/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3635 - class_loss: 0.0096 - l1_loss: 0.1354\n",
      "Epoch 994/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3680 - class_loss: 0.0038 - l1_loss: 0.1364\n",
      "Epoch 995/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3277 - class_loss: 0.0101 - l1_loss: 0.1318\n",
      "Epoch 996/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2967 - class_loss: 0.0048 - l1_loss: 0.1292\n",
      "Epoch 997/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3346 - class_loss: 0.0031 - l1_loss: 0.1331\n",
      "Epoch 998/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2133 - class_loss: 0.0046 - l1_loss: 0.1209\n",
      "Epoch 999/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1541 - class_loss: 0.0092 - l1_loss: 0.1145\n",
      "Epoch 1000/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0344 - class_loss: 0.0065 - l1_loss: 0.1028\n",
      "Epoch 1001/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.9816 - class_loss: 0.0079 - l1_loss: 0.0974\n",
      "Epoch 1002/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1405 - class_loss: 0.0056 - l1_loss: 0.1135\n",
      "Epoch 1003/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1986 - class_loss: 0.0022 - l1_loss: 0.1196\n",
      "Epoch 1004/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0727 - class_loss: 0.0025 - l1_loss: 0.1070\n",
      "Epoch 1005/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5724 - class_loss: 0.0046 - l1_loss: 0.1568\n",
      "Epoch 1006/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1643 - class_loss: 0.0055 - l1_loss: 0.1159\n",
      "Epoch 1007/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9583 - class_loss: 0.0064 - l1_loss: 0.0952\n",
      "Epoch 1008/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0838 - class_loss: 0.0054 - l1_loss: 0.1078\n",
      "Epoch 1009/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1005 - class_loss: 0.0056 - l1_loss: 0.1095\n",
      "Epoch 1010/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0927 - class_loss: 0.0065 - l1_loss: 0.1086\n",
      "Epoch 1011/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0616 - class_loss: 0.0061 - l1_loss: 0.1056 0s - loss: 1.1019 - class_loss: 0.0022 - l1_loss: 0.\n",
      "Epoch 1012/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8230 - class_loss: 0.0058 - l1_loss: 0.0817\n",
      "Epoch 1013/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9288 - class_loss: 0.0069 - l1_loss: 0.0922\n",
      "Epoch 1014/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8562 - class_loss: 0.0046 - l1_loss: 0.0852\n",
      "Epoch 1015/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8336 - class_loss: 0.0079 - l1_loss: 0.0826\n",
      "Epoch 1016/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9137 - class_loss: 0.0029 - l1_loss: 0.0911\n",
      "Epoch 1017/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0978 - class_loss: 0.0054 - l1_loss: 0.1092\n",
      "Epoch 1018/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0506 - class_loss: 0.0034 - l1_loss: 0.1047\n",
      "Epoch 1019/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1615 - class_loss: 0.0055 - l1_loss: 0.1156\n",
      "Epoch 1020/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5415 - class_loss: 0.0046 - l1_loss: 0.1537\n",
      "Epoch 1021/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2688 - class_loss: 0.0092 - l1_loss: 0.1260\n",
      "Epoch 1022/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4623 - class_loss: 0.0126 - l1_loss: 0.1450\n",
      "Epoch 1023/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5356 - class_loss: 0.0025 - l1_loss: 0.1533\n",
      "Epoch 1024/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6303 - class_loss: 0.0119 - l1_loss: 0.1618\n",
      "Epoch 1025/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6169 - class_loss: 0.0050 - l1_loss: 0.1612\n",
      "Epoch 1026/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9119 - class_loss: 0.0055 - l1_loss: 0.1906\n",
      "Epoch 1027/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0387 - class_loss: 0.0060 - l1_loss: 0.3033\n",
      "Epoch 1028/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8842 - class_loss: 0.0177 - l1_loss: 0.3866\n",
      "Epoch 1029/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.7464 - class_loss: 0.0058 - l1_loss: 0.3741\n",
      "Epoch 1030/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3390 - class_loss: 0.0022 - l1_loss: 0.4337\n",
      "Epoch 1031/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.9155 - class_loss: 0.0019 - l1_loss: 0.5914\n",
      "Epoch 1032/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.9794 - class_loss: 0.0112 - l1_loss: 0.3968 0s - loss: 2.6016 - class_loss: 0.0102 - l1_loss: \n",
      "Epoch 1033/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3183 - class_loss: 0.0040 - l1_loss: 0.3314\n",
      "Epoch 1034/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.6722 - class_loss: 0.0284 - l1_loss: 0.4644\n",
      "Epoch 1035/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.7416 - class_loss: 0.0017 - l1_loss: 0.6740\n",
      "Epoch 1036/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.1478 - class_loss: 0.0019 - l1_loss: 0.4146\n",
      "Epoch 1037/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.8435 - class_loss: 0.0039 - l1_loss: 0.3840\n",
      "Epoch 1038/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.8370 - class_loss: 0.0064 - l1_loss: 0.3831\n",
      "Epoch 1039/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0442 - class_loss: 0.0100 - l1_loss: 0.3034\n",
      "Epoch 1040/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.8350 - class_loss: 0.0214 - l1_loss: 0.3814\n",
      "Epoch 1041/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8591 - class_loss: 0.0149 - l1_loss: 0.2844 0s - loss: 2.9994 - class_loss: 0.0156 - l1_loss: 0.\n",
      "Epoch 1042/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1449 - class_loss: 0.0281 - l1_loss: 0.4117\n",
      "Epoch 1043/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8257 - class_loss: 0.0033 - l1_loss: 0.2822 0s - loss: 2.7076 - class_loss: 0.0037 - l1_loss: 0.27\n",
      "Epoch 1044/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.8470 - class_loss: 0.0230 - l1_loss: 0.3824\n",
      "Epoch 1045/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.5605 - class_loss: 0.0215 - l1_loss: 0.2539\n",
      "Epoch 1046/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1525 - class_loss: 0.0104 - l1_loss: 0.3142\n",
      "Epoch 1047/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5798 - class_loss: 0.0036 - l1_loss: 0.3576\n",
      "Epoch 1048/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6613 - class_loss: 0.0250 - l1_loss: 0.3636\n",
      "Epoch 1049/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5459 - class_loss: 0.0054 - l1_loss: 0.4541\n",
      "Epoch 1050/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.9588 - class_loss: 0.0025 - l1_loss: 0.5956\n",
      "Epoch 1051/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8443 - class_loss: 0.0364 - l1_loss: 0.4808\n",
      "Epoch 1052/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.8852 - class_loss: 0.0073 - l1_loss: 0.5878\n",
      "Epoch 1053/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.4484 - class_loss: 0.0041 - l1_loss: 0.5444\n",
      "Epoch 1054/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.2853 - class_loss: 0.0103 - l1_loss: 0.7275\n",
      "Epoch 1055/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.6180 - class_loss: 0.0094 - l1_loss: 0.9609\n",
      "Epoch 1056/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 11.1152 - class_loss: 0.0071 - l1_loss: 1.1108\n",
      "Epoch 1057/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.8526 - class_loss: 0.0216 - l1_loss: 1.3831\n",
      "Epoch 1058/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.9623 - class_loss: 0.0041 - l1_loss: 1.2958: 0s - loss: 12.8076 - class_loss: 0.0045 - l1_loss: 1.280\n",
      "Epoch 1059/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.5226 - class_loss: 0.0023 - l1_loss: 1.1520\n",
      "Epoch 1060/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.0014 - class_loss: 0.0024 - l1_loss: 1.2999\n",
      "Epoch 1061/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.3538 - class_loss: 0.0076 - l1_loss: 1.3346\n",
      "Epoch 1062/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.3166 - class_loss: 0.0130 - l1_loss: 0.9304\n",
      "Epoch 1063/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 11.9333 - class_loss: 0.0343 - l1_loss: 1.1899\n",
      "Epoch 1064/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.6383 - class_loss: 0.0073 - l1_loss: 1.2631\n",
      "Epoch 1065/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.4986 - class_loss: 0.0180 - l1_loss: 1.6481\n",
      "Epoch 1066/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.0701 - class_loss: 0.0787 - l1_loss: 1.3991\n",
      "Epoch 1067/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.8968 - class_loss: 0.0113 - l1_loss: 1.3886\n",
      "Epoch 1068/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.5292 - class_loss: 0.0040 - l1_loss: 1.4525\n",
      "Epoch 1069/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 23.4481 - class_loss: 0.0140 - l1_loss: 2.3434\n",
      "Epoch 1070/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 17.9608 - class_loss: 0.5732 - l1_loss: 1.7388\n",
      "Epoch 1071/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 21.3392 - class_loss: 0.0932 - l1_loss: 2.1246\n",
      "Epoch 1072/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 23.7719 - class_loss: 0.0737 - l1_loss: 2.3698\n",
      "Epoch 1073/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 39.5509 - class_loss: 0.0065 - l1_loss: 3.9544\n",
      "Epoch 1074/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 27.2669 - class_loss: 0.0110 - l1_loss: 2.7256\n",
      "Epoch 1075/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 20.2960 - class_loss: 1.0872 - l1_loss: 1.9209    - 0s 31ms/step - loss: 23.0474 - class_loss: 0.9513 - l1_loss: 2.2096\n",
      "Epoch 1076/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 24.3950 - class_loss: 0.1803 - l1_loss: 2.4215\n",
      "Epoch 1077/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 25.3985 - class_loss: 0.2039 - l1_loss: 2.5195\n",
      "Epoch 1078/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 19.2771 - class_loss: 0.0218 - l1_loss: 1.9255\n",
      "Epoch 1079/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 24.5470 - class_loss: 0.0171 - l1_loss: 2.4530\n",
      "Epoch 1080/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 24.1414 - class_loss: 0.0156 - l1_loss: 2.4126\n",
      "Epoch 1081/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.5009 - class_loss: 0.3357 - l1_loss: 1.7165\n",
      "Epoch 1082/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 20.7112 - class_loss: 0.0213 - l1_loss: 2.0690: 0s - loss: 21.2624 - class_loss: 0.0250 - l1_loss: 2.1\n",
      "Epoch 1083/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 23.1764 - class_loss: 0.0149 - l1_loss: 2.3161\n",
      "Epoch 1084/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 15.8654 - class_loss: 0.0205 - l1_loss: 1.5845\n",
      "Epoch 1085/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.0235 - class_loss: 0.0072 - l1_loss: 1.2016\n",
      "Epoch 1086/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.2229 - class_loss: 0.0026 - l1_loss: 1.1220\n",
      "Epoch 1087/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.7011 - class_loss: 0.0019 - l1_loss: 1.5699\n",
      "Epoch 1088/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.1996 - class_loss: 0.0293 - l1_loss: 0.8170\n",
      "Epoch 1089/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.8043 - class_loss: 0.0055 - l1_loss: 0.7799\n",
      "Epoch 1090/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.5905 - class_loss: 0.0066 - l1_loss: 0.6584\n",
      "Epoch 1091/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.0844 - class_loss: 0.0034 - l1_loss: 0.7081\n",
      "Epoch 1092/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.2058 - class_loss: 0.0480 - l1_loss: 0.7158\n",
      "Epoch 1093/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.7858 - class_loss: 0.0025 - l1_loss: 0.6783\n",
      "Epoch 1094/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.4078 - class_loss: 0.0078 - l1_loss: 0.7400\n",
      "Epoch 1095/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.1513 - class_loss: 0.0075 - l1_loss: 0.6144\n",
      "Epoch 1096/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9771 - class_loss: 0.0083 - l1_loss: 0.4969\n",
      "Epoch 1097/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9658 - class_loss: 0.0277 - l1_loss: 0.3938\n",
      "Epoch 1098/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0355 - class_loss: 0.0112 - l1_loss: 0.4024\n",
      "Epoch 1099/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.2077 - class_loss: 0.0102 - l1_loss: 0.4197\n",
      "Epoch 1100/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.6136 - class_loss: 0.0117 - l1_loss: 0.4602\n",
      "Epoch 1101/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2713 - class_loss: 0.0104 - l1_loss: 0.3261\n",
      "Epoch 1102/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0439 - class_loss: 0.0043 - l1_loss: 0.4040\n",
      "Epoch 1103/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5945 - class_loss: 0.0064 - l1_loss: 0.3588\n",
      "Epoch 1104/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4704 - class_loss: 0.0024 - l1_loss: 0.3468\n",
      "Epoch 1105/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2421 - class_loss: 0.0281 - l1_loss: 0.3214\n",
      "Epoch 1106/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.3373 - class_loss: 0.0038 - l1_loss: 0.3333\n",
      "Epoch 1107/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0885 - class_loss: 7.8042e-04 - l1_loss: 0.3088\n",
      "Epoch 1108/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8247 - class_loss: 0.0012 - l1_loss: 0.2823\n",
      "Epoch 1109/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.1300 - class_loss: 0.0090 - l1_loss: 0.4121\n",
      "Epoch 1110/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.8649 - class_loss: 0.0036 - l1_loss: 0.3861\n",
      "Epoch 1111/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.7126 - class_loss: 0.0043 - l1_loss: 0.5708\n",
      "Epoch 1112/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.6117 - class_loss: 0.0053 - l1_loss: 0.5606\n",
      "Epoch 1113/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.8021 - class_loss: 0.0080 - l1_loss: 0.3794\n",
      "Epoch 1114/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.3493 - class_loss: 0.0033 - l1_loss: 0.7346\n",
      "Epoch 1115/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3751 - class_loss: 0.0027 - l1_loss: 0.3372\n",
      "Epoch 1116/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5496 - class_loss: 0.0102 - l1_loss: 0.4539\n",
      "Epoch 1117/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9381 - class_loss: 0.0133 - l1_loss: 0.2925\n",
      "Epoch 1118/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9712 - class_loss: 0.0052 - l1_loss: 0.2966\n",
      "Epoch 1119/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.6703 - class_loss: 0.0030 - l1_loss: 0.2667\n",
      "Epoch 1120/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.0159 - class_loss: 0.0102 - l1_loss: 0.2006\n",
      "Epoch 1121/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7532 - class_loss: 0.0034 - l1_loss: 0.1750\n",
      "Epoch 1122/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.7855 - class_loss: 0.0037 - l1_loss: 0.1782\n",
      "Epoch 1123/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.5165 - class_loss: 0.0059 - l1_loss: 0.1511\n",
      "Epoch 1124/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1024 - class_loss: 0.0041 - l1_loss: 0.2098\n",
      "Epoch 1125/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.0066 - class_loss: 0.0092 - l1_loss: 0.1997\n",
      "Epoch 1126/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4397 - class_loss: 0.0049 - l1_loss: 0.2435\n",
      "Epoch 1127/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8817 - class_loss: 0.0095 - l1_loss: 0.1872\n",
      "Epoch 1128/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7528 - class_loss: 0.0020 - l1_loss: 0.2751\n",
      "Epoch 1129/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7055 - class_loss: 0.0021 - l1_loss: 0.2703\n",
      "Epoch 1130/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3231 - class_loss: 0.0056 - l1_loss: 0.2318\n",
      "Epoch 1131/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7697 - class_loss: 0.0054 - l1_loss: 0.1764\n",
      "Epoch 1132/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6979 - class_loss: 0.0047 - l1_loss: 0.2693\n",
      "Epoch 1133/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0371 - class_loss: 0.0200 - l1_loss: 0.2017\n",
      "Epoch 1134/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3073 - class_loss: 0.0063 - l1_loss: 0.2301\n",
      "Epoch 1135/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8377 - class_loss: 0.0035 - l1_loss: 0.1834\n",
      "Epoch 1136/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4987 - class_loss: 0.0037 - l1_loss: 0.1495\n",
      "Epoch 1137/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3324 - class_loss: 0.0037 - l1_loss: 0.1329\n",
      "Epoch 1138/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3231 - class_loss: 0.0036 - l1_loss: 0.1319\n",
      "Epoch 1139/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3898 - class_loss: 0.0033 - l1_loss: 0.1386\n",
      "Epoch 1140/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1886 - class_loss: 0.0194 - l1_loss: 0.1169\n",
      "Epoch 1141/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3537 - class_loss: 0.0026 - l1_loss: 0.1351\n",
      "Epoch 1142/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1916 - class_loss: 0.0031 - l1_loss: 0.1188\n",
      "Epoch 1143/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0451 - class_loss: 0.0035 - l1_loss: 0.1042\n",
      "Epoch 1144/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0343 - class_loss: 0.0080 - l1_loss: 0.1026\n",
      "Epoch 1145/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1101 - class_loss: 0.0040 - l1_loss: 0.1106\n",
      "Epoch 1146/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0468 - class_loss: 0.0058 - l1_loss: 0.1041\n",
      "Epoch 1147/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1404 - class_loss: 0.0069 - l1_loss: 0.1133\n",
      "Epoch 1148/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0617 - class_loss: 0.0059 - l1_loss: 0.1056\n",
      "Epoch 1149/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8903 - class_loss: 0.0019 - l1_loss: 0.0888\n",
      "Epoch 1150/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8643 - class_loss: 0.0036 - l1_loss: 0.0861\n",
      "Epoch 1151/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9953 - class_loss: 0.0057 - l1_loss: 0.0990\n",
      "Epoch 1152/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2948 - class_loss: 0.0070 - l1_loss: 0.1288\n",
      "Epoch 1153/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3070 - class_loss: 0.0029 - l1_loss: 0.1304\n",
      "Epoch 1154/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6143 - class_loss: 0.0027 - l1_loss: 0.1612\n",
      "Epoch 1155/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3950 - class_loss: 0.0045 - l1_loss: 0.1391\n",
      "Epoch 1156/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5624 - class_loss: 0.0031 - l1_loss: 0.1559\n",
      "Epoch 1157/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4366 - class_loss: 0.0169 - l1_loss: 0.1420\n",
      "Epoch 1158/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3709 - class_loss: 0.0038 - l1_loss: 0.1367\n",
      "Epoch 1159/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1326 - class_loss: 0.0021 - l1_loss: 0.1130\n",
      "Epoch 1160/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1773 - class_loss: 0.0037 - l1_loss: 0.1174ETA: 0s - loss: 1.1237 - class_loss: 0.0016 - l1_loss: 0.\n",
      "Epoch 1161/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2716 - class_loss: 0.0047 - l1_loss: 0.1267\n",
      "Epoch 1162/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3409 - class_loss: 0.0073 - l1_loss: 0.1334\n",
      "Epoch 1163/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1426 - class_loss: 0.0033 - l1_loss: 0.1139\n",
      "Epoch 1164/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2529 - class_loss: 0.0043 - l1_loss: 0.1249 0s - loss: 1.0752 - class_loss: 0.0025 - l1_loss: 0.10 - ETA: 0s - loss: 1.2610 - class_loss: 0.0044 - l1_loss: 0.12\n",
      "Epoch 1165/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2913 - class_loss: 0.0040 - l1_loss: 0.1287\n",
      "Epoch 1166/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1120 - class_loss: 0.0040 - l1_loss: 0.1108\n",
      "Epoch 1167/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0931 - class_loss: 0.0085 - l1_loss: 0.1085\n",
      "Epoch 1168/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9909 - class_loss: 0.0095 - l1_loss: 0.0981\n",
      "Epoch 1169/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1483 - class_loss: 0.0083 - l1_loss: 0.1140\n",
      "Epoch 1170/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2183 - class_loss: 0.0043 - l1_loss: 0.1214\n",
      "Epoch 1171/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3944 - class_loss: 0.0029 - l1_loss: 0.1392 0s - loss: 1.3105 - class_loss: 0.0040 - l1_loss: 0.\n",
      "Epoch 1172/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5277 - class_loss: 0.0055 - l1_loss: 0.1522\n",
      "Epoch 1173/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6690 - class_loss: 0.0051 - l1_loss: 0.1664\n",
      "Epoch 1174/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.7993 - class_loss: 0.0078 - l1_loss: 0.1792\n",
      "Epoch 1175/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2812 - class_loss: 0.0022 - l1_loss: 0.3279\n",
      "Epoch 1176/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.1431 - class_loss: 0.0084 - l1_loss: 0.4135\n",
      "Epoch 1177/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2209 - class_loss: 0.0028 - l1_loss: 0.3218\n",
      "Epoch 1178/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 28ms/step - loss: 3.6281 - class_loss: 0.0115 - l1_loss: 0.3617\n",
      "Epoch 1179/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0791 - class_loss: 0.0052 - l1_loss: 0.3074\n",
      "Epoch 1180/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9430 - class_loss: 0.0020 - l1_loss: 0.4941\n",
      "Epoch 1181/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4605 - class_loss: 0.0015 - l1_loss: 0.3459\n",
      "Epoch 1182/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.6257 - class_loss: 0.0026 - l1_loss: 0.5623ETA: 0s - loss: 6.2830 - class_loss: 0.0022 - l1_loss: 0.\n",
      "Epoch 1183/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.2011 - class_loss: 0.0041 - l1_loss: 0.4197\n",
      "Epoch 1184/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.7134 - class_loss: 0.0091 - l1_loss: 0.6704\n",
      "Epoch 1185/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9976 - class_loss: 0.0149 - l1_loss: 0.4983\n",
      "Epoch 1186/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.4359 - class_loss: 0.0134 - l1_loss: 0.4423\n",
      "Epoch 1187/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5749 - class_loss: 0.0140 - l1_loss: 0.4561\n",
      "Epoch 1188/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.5620 - class_loss: 0.0163 - l1_loss: 0.4546 0s - loss: 4.6383 - class_loss: 0.0107 - l1_loss: 0.\n",
      "Epoch 1189/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.8607 - class_loss: 0.0159 - l1_loss: 0.4845\n",
      "Epoch 1190/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.5825 - class_loss: 0.0232 - l1_loss: 0.5559 0s - loss: 5.7996 - class_loss: 0.0429 - l1_loss: \n",
      "Epoch 1191/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.8592 - class_loss: 0.0100 - l1_loss: 0.7849\n",
      "Epoch 1192/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.6431 - class_loss: 0.0087 - l1_loss: 1.1634\n",
      "Epoch 1193/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.2250 - class_loss: 0.0122 - l1_loss: 0.8213\n",
      "Epoch 1194/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.8323 - class_loss: 0.0088 - l1_loss: 0.8824\n",
      "Epoch 1195/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.7763 - class_loss: 0.0201 - l1_loss: 0.7756\n",
      "Epoch 1196/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.0825 - class_loss: 0.1485 - l1_loss: 0.8934\n",
      "Epoch 1197/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.8366 - class_loss: 0.0088 - l1_loss: 0.9828\n",
      "Epoch 1198/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.4252 - class_loss: 0.0078 - l1_loss: 0.8417\n",
      "Epoch 1199/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.1040 - class_loss: 0.1118 - l1_loss: 0.7992\n",
      "Epoch 1200/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.4772 - class_loss: 0.0089 - l1_loss: 0.7468\n",
      "Epoch 1201/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.5766 - class_loss: 0.0133 - l1_loss: 0.7563\n",
      "Epoch 1202/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.5297 - class_loss: 0.0085 - l1_loss: 0.6521\n",
      "Epoch 1203/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.1740 - class_loss: 0.0247 - l1_loss: 0.8149\n",
      "Epoch 1204/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.6714 - class_loss: 0.0035 - l1_loss: 0.9668\n",
      "Epoch 1205/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.1571 - class_loss: 0.0380 - l1_loss: 1.5119\n",
      "Epoch 1206/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.8692 - class_loss: 0.0028 - l1_loss: 1.08660s - loss: 12.5069 - class_loss: 0.0038 - l1_loss: 1.25\n",
      "Epoch 1207/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.3936 - class_loss: 0.0021 - l1_loss: 1.4391\n",
      "Epoch 1208/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.5774 - class_loss: 0.0044 - l1_loss: 1.0573\n",
      "Epoch 1209/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.9012 - class_loss: 0.0070 - l1_loss: 1.0894\n",
      "Epoch 1210/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.9701 - class_loss: 0.0099 - l1_loss: 1.1960\n",
      "Epoch 1211/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.9140 - class_loss: 0.0087 - l1_loss: 0.9905\n",
      "Epoch 1212/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 10.4507 - class_loss: 0.0055 - l1_loss: 1.0445\n",
      "Epoch 1213/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 9.2849 - class_loss: 0.0243 - l1_loss: 0.9261\n",
      "Epoch 1214/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.7920 - class_loss: 0.0125 - l1_loss: 0.9780\n",
      "Epoch 1215/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.8162 - class_loss: 0.0033 - l1_loss: 1.0813ETA: 0s - loss: 11.0032 - class_loss: 0.0036 - l1_loss: 1.100\n",
      "Epoch 1216/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.5688 - class_loss: 0.0166 - l1_loss: 1.1552\n",
      "Epoch 1217/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.9300 - class_loss: 0.0043 - l1_loss: 1.0926: 0s - loss: 11.2453 - class_loss: 0.0036 - l1_loss: 1.124\n",
      "Epoch 1218/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.9179 - class_loss: 0.0076 - l1_loss: 0.9910 0s - loss: 10.4642 - class_loss: 0.0101 - l1_loss: 1.\n",
      "Epoch 1219/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.6570 - class_loss: 0.0046 - l1_loss: 1.4652\n",
      "Epoch 1220/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.8652 - class_loss: 0.0160 - l1_loss: 1.0849\n",
      "Epoch 1221/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.9357 - class_loss: 0.2434 - l1_loss: 1.4692\n",
      "Epoch 1222/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.5346 - class_loss: 0.0066 - l1_loss: 1.0528\n",
      "Epoch 1223/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.9842 - class_loss: 0.0078 - l1_loss: 0.9976\n",
      "Epoch 1224/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.2239 - class_loss: 0.0057 - l1_loss: 0.8218\n",
      "Epoch 1225/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.3955 - class_loss: 0.0056 - l1_loss: 0.8390\n",
      "Epoch 1226/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.7838 - class_loss: 0.0868 - l1_loss: 0.8697\n",
      "Epoch 1227/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.6263 - class_loss: 0.0041 - l1_loss: 0.6622\n",
      "Epoch 1228/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.4914 - class_loss: 0.0093 - l1_loss: 0.7482\n",
      "Epoch 1229/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 9.4477 - class_loss: 0.0395 - l1_loss: 0.9408\n",
      "Epoch 1230/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.7206 - class_loss: 0.0106 - l1_loss: 0.7710\n",
      "Epoch 1231/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.2524 - class_loss: 0.0030 - l1_loss: 1.0249\n",
      "Epoch 1232/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.6073 - class_loss: 8.9138e-04 - l1_loss: 0.7606\n",
      "Epoch 1233/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.7470 - class_loss: 0.0242 - l1_loss: 0.9723ETA: 0s - loss: 11.1028 - class_loss: 0.0026 - l1_loss: 1.1100\n",
      "Epoch 1234/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.3942 - class_loss: 0.0040 - l1_loss: 0.8390\n",
      "Epoch 1235/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.7056 - class_loss: 0.0060 - l1_loss: 0.8700- ETA: 0s - loss: 9.8832 - class_loss: 0.0036 - l1_loss: 0.9\n",
      "Epoch 1236/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.6029 - class_loss: 0.0259 - l1_loss: 0.6577\n",
      "Epoch 1237/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.5750 - class_loss: 0.0112 - l1_loss: 0.7564\n",
      "Epoch 1238/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.5348 - class_loss: 0.0018 - l1_loss: 0.6533\n",
      "Epoch 1239/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9943 - class_loss: 0.0015 - l1_loss: 0.4993\n",
      "Epoch 1240/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.5231 - class_loss: 0.0013 - l1_loss: 0.5522ETA: 0s - loss: 5.1557 - class_loss: 0.0019 - l1_loss: 0.\n",
      "Epoch 1241/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.0934 - class_loss: 0.0022 - l1_loss: 0.5091\n",
      "Epoch 1242/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.5685 - class_loss: 0.0992 - l1_loss: 0.8469\n",
      "Epoch 1243/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9275 - class_loss: 0.0121 - l1_loss: 0.4915\n",
      "Epoch 1244/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.6523 - class_loss: 0.0189 - l1_loss: 0.5633\n",
      "Epoch 1245/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.2006 - class_loss: 0.0218 - l1_loss: 0.8179\n",
      "Epoch 1246/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.0424 - class_loss: 0.0071 - l1_loss: 0.6035\n",
      "Epoch 1247/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.1964 - class_loss: 0.0033 - l1_loss: 0.6193\n",
      "Epoch 1248/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.9512 - class_loss: 0.0044 - l1_loss: 0.5947\n",
      "Epoch 1249/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.2854 - class_loss: 0.0072 - l1_loss: 0.4278\n",
      "Epoch 1250/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.4044 - class_loss: 0.0164 - l1_loss: 0.4388\n",
      "Epoch 1251/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.3421 - class_loss: 0.0020 - l1_loss: 0.5340\n",
      "Epoch 1252/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.4670 - class_loss: 0.0025 - l1_loss: 0.6465\n",
      "Epoch 1253/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.3005 - class_loss: 0.0060 - l1_loss: 0.6294\n",
      "Epoch 1254/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7757 - class_loss: 0.0031 - l1_loss: 0.4773\n",
      "Epoch 1255/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0244 - class_loss: 0.0081 - l1_loss: 0.4016\n",
      "Epoch 1256/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.7674 - class_loss: 0.0103 - l1_loss: 0.3757\n",
      "Epoch 1257/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.7663 - class_loss: 0.0053 - l1_loss: 0.4761\n",
      "Epoch 1258/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7006 - class_loss: 0.0019 - l1_loss: 0.3699\n",
      "Epoch 1259/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.2015 - class_loss: 0.0144 - l1_loss: 0.5187 0s - loss: 5.6810 - class_loss: 0.0118 - l1_loss: 0.\n",
      "Epoch 1260/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5464 - class_loss: 0.0079 - l1_loss: 0.3538\n",
      "Epoch 1261/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.2962 - class_loss: 0.0043 - l1_loss: 0.3292 0s - loss: 4.4634 - class_loss: 0.0061 - l1_loss: \n",
      "Epoch 1262/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.9155 - class_loss: 0.0038 - l1_loss: 0.3912\n",
      "Epoch 1263/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9252 - class_loss: 0.0042 - l1_loss: 0.3921\n",
      "Epoch 1264/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8165 - class_loss: 0.0044 - l1_loss: 0.4812\n",
      "Epoch 1265/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.4915 - class_loss: 0.0065 - l1_loss: 0.4485 0s - loss: 4.4439 - class_loss: 0.0044 - l1_loss: 0.44\n",
      "Epoch 1266/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.8553 - class_loss: 0.0220 - l1_loss: 0.3833\n",
      "Epoch 1267/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9280 - class_loss: 0.0027 - l1_loss: 0.2925\n",
      "Epoch 1268/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9840 - class_loss: 0.0018 - l1_loss: 0.2982\n",
      "Epoch 1269/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0111 - class_loss: 0.0025 - l1_loss: 0.3009\n",
      "Epoch 1270/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4858 - class_loss: 0.0032 - l1_loss: 0.3483\n",
      "Epoch 1271/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.5992 - class_loss: 0.0074 - l1_loss: 0.3592ETA: 0s - loss: 2.7963 - class_loss: 4.5383e-04 - l1_loss: \n",
      "Epoch 1272/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2702 - class_loss: 0.0035 - l1_loss: 0.4267\n",
      "Epoch 1273/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.3516 - class_loss: 0.0012 - l1_loss: 0.5350\n",
      "Epoch 1274/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.1153 - class_loss: 0.0013 - l1_loss: 0.5114\n",
      "Epoch 1275/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9149 - class_loss: 9.6657e-04 - l1_loss: 0.2914\n",
      "Epoch 1276/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.0963 - class_loss: 0.0023 - l1_loss: 0.3094\n",
      "Epoch 1277/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.7496 - class_loss: 0.0040 - l1_loss: 0.3746\n",
      "Epoch 1278/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2936 - class_loss: 0.0054 - l1_loss: 0.3288\n",
      "Epoch 1279/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5889 - class_loss: 0.0019 - l1_loss: 0.3587\n",
      "Epoch 1280/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.6841 - class_loss: 0.0098 - l1_loss: 0.4674\n",
      "Epoch 1281/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.9833 - class_loss: 0.0044 - l1_loss: 0.4979\n",
      "Epoch 1282/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.1331 - class_loss: 0.0045 - l1_loss: 0.5129\n",
      "Epoch 1283/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 8.3328 - class_loss: 0.0155 - l1_loss: 0.8317\n",
      "Epoch 1284/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.6673 - class_loss: 0.0018 - l1_loss: 0.6666\n",
      "Epoch 1285/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.2877 - class_loss: 0.0131 - l1_loss: 0.4275\n",
      "Epoch 1286/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.3965 - class_loss: 0.0075 - l1_loss: 0.6389\n",
      "Epoch 1287/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 7.2621 - class_loss: 0.0068 - l1_loss: 0.7255\n",
      "Epoch 1288/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.9659 - class_loss: 0.0171 - l1_loss: 0.4949\n",
      "Epoch 1289/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.1856 - class_loss: 0.0061 - l1_loss: 0.6180\n",
      "Epoch 1290/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.4245 - class_loss: 0.0027 - l1_loss: 0.6422\n",
      "Epoch 1291/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.9417 - class_loss: 0.0025 - l1_loss: 0.3939\n",
      "Epoch 1292/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.6303 - class_loss: 0.0027 - l1_loss: 0.4628\n",
      "Epoch 1293/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.9155 - class_loss: 0.0020 - l1_loss: 0.3914\n",
      "Epoch 1294/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.1120 - class_loss: 7.7588e-04 - l1_loss: 0.3111\n",
      "Epoch 1295/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.8078 - class_loss: 0.0014 - l1_loss: 0.2806\n",
      "Epoch 1296/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.1124 - class_loss: 0.0039 - l1_loss: 0.3108\n",
      "Epoch 1297/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7493 - class_loss: 0.0085 - l1_loss: 0.2741\n",
      "Epoch 1298/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.5493 - class_loss: 0.0070 - l1_loss: 0.3542\n",
      "Epoch 1299/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.4798 - class_loss: 0.0037 - l1_loss: 0.3476\n",
      "Epoch 1300/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6555 - class_loss: 0.0063 - l1_loss: 0.2649\n",
      "Epoch 1301/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.1276 - class_loss: 0.0035 - l1_loss: 0.3124\n",
      "Epoch 1302/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.4453 - class_loss: 0.0017 - l1_loss: 0.2444\n",
      "Epoch 1303/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7269 - class_loss: 0.0023 - l1_loss: 0.2725\n",
      "Epoch 1304/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2643 - class_loss: 0.0023 - l1_loss: 0.2262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1305/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.5870 - class_loss: 0.0085 - l1_loss: 0.2579\n",
      "Epoch 1306/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2578 - class_loss: 0.0074 - l1_loss: 0.2250\n",
      "Epoch 1307/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7058 - class_loss: 0.0091 - l1_loss: 0.2697\n",
      "Epoch 1308/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6210 - class_loss: 0.0209 - l1_loss: 0.2600\n",
      "Epoch 1309/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9054 - class_loss: 0.0178 - l1_loss: 0.2888\n",
      "Epoch 1310/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.8537 - class_loss: 0.0022 - l1_loss: 0.3852\n",
      "Epoch 1311/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.1075 - class_loss: 0.0040 - l1_loss: 0.5103\n",
      "Epoch 1312/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.0057 - class_loss: 0.0051 - l1_loss: 0.5001\n",
      "Epoch 1313/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.6007 - class_loss: 0.0047 - l1_loss: 0.4596\n",
      "Epoch 1314/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.5404 - class_loss: 0.0024 - l1_loss: 0.4538\n",
      "Epoch 1315/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.1395 - class_loss: 7.8161e-04 - l1_loss: 0.5139\n",
      "Epoch 1316/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.8178 - class_loss: 0.0017 - l1_loss: 0.4816\n",
      "Epoch 1317/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.8640 - class_loss: 0.0132 - l1_loss: 0.4851\n",
      "Epoch 1318/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.3947 - class_loss: 0.0036 - l1_loss: 0.6391\n",
      "Epoch 1319/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.5261 - class_loss: 0.0035 - l1_loss: 0.7523\n",
      "Epoch 1320/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.1396 - class_loss: 0.0028 - l1_loss: 0.4137\n",
      "Epoch 1321/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.4521 - class_loss: 0.0063 - l1_loss: 0.4446\n",
      "Epoch 1322/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.1650 - class_loss: 0.0082 - l1_loss: 0.5157\n",
      "Epoch 1323/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7704 - class_loss: 0.0243 - l1_loss: 0.3746\n",
      "Epoch 1324/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.2793 - class_loss: 0.0056 - l1_loss: 0.4274\n",
      "Epoch 1325/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.5063 - class_loss: 0.0087 - l1_loss: 0.4498\n",
      "Epoch 1326/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.5415 - class_loss: 0.0075 - l1_loss: 0.3534\n",
      "Epoch 1327/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.8701 - class_loss: 0.0029 - l1_loss: 0.3867\n",
      "Epoch 1328/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.4983 - class_loss: 0.0030 - l1_loss: 0.4495\n",
      "Epoch 1329/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.3513 - class_loss: 0.0030 - l1_loss: 0.4348\n",
      "Epoch 1330/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.9127 - class_loss: 0.0140 - l1_loss: 0.3899\n",
      "Epoch 1331/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.3475 - class_loss: 0.0013 - l1_loss: 0.4346\n",
      "Epoch 1332/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.5386 - class_loss: 0.0027 - l1_loss: 0.5536\n",
      "Epoch 1333/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.1688 - class_loss: 0.0050 - l1_loss: 0.5164\n",
      "Epoch 1334/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.5378 - class_loss: 0.0056 - l1_loss: 0.5532\n",
      "Epoch 1335/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.8589 - class_loss: 0.0117 - l1_loss: 0.7847\n",
      "Epoch 1336/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.9612 - class_loss: 0.0080 - l1_loss: 0.9953\n",
      "Epoch 1337/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.7789 - class_loss: 0.0028 - l1_loss: 0.8776\n",
      "Epoch 1338/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 11.1329 - class_loss: 0.0166 - l1_loss: 1.1116\n",
      "Epoch 1339/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 14.6238 - class_loss: 0.0028 - l1_loss: 1.4621\n",
      "Epoch 1340/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.0281 - class_loss: 6.8741e-04 - l1_loss: 1.8027\n",
      "Epoch 1341/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 24.3445 - class_loss: 0.0040 - l1_loss: 2.4340\n",
      "Epoch 1342/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 24.9564 - class_loss: 0.0029 - l1_loss: 2.4953\n",
      "Epoch 1343/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 23.4532 - class_loss: 0.0032 - l1_loss: 2.3450\n",
      "Epoch 1344/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 24.5182 - class_loss: 0.0035 - l1_loss: 2.4515\n",
      "Epoch 1345/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 25.3792 - class_loss: 0.0025 - l1_loss: 2.5377\n",
      "Epoch 1346/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 27.5797 - class_loss: 7.5911e-04 - l1_loss: 2.7579\n",
      "Epoch 1347/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 27.1697 - class_loss: 0.0651 - l1_loss: 2.7105\n",
      "Epoch 1348/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 28.5696 - class_loss: 0.0433 - l1_loss: 2.8526\n",
      "Epoch 1349/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 16.5646 - class_loss: 0.0054 - l1_loss: 1.6559\n",
      "Epoch 1350/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 19.8558 - class_loss: 0.0086 - l1_loss: 1.9847\n",
      "Epoch 1351/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 18.5139 - class_loss: 0.0088 - l1_loss: 1.8505\n",
      "Epoch 1352/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 16.2087 - class_loss: 0.0014 - l1_loss: 1.6207\n",
      "Epoch 1353/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.8865 - class_loss: 0.0018 - l1_loss: 1.4885\n",
      "Epoch 1354/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 12.4561 - class_loss: 0.0041 - l1_loss: 1.2452\n",
      "Epoch 1355/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 19.5808 - class_loss: 0.0401 - l1_loss: 1.9541\n",
      "Epoch 1356/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.7358 - class_loss: 0.0146 - l1_loss: 1.2721\n",
      "Epoch 1357/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.3040 - class_loss: 7.0754e-04 - l1_loss: 1.5303\n",
      "Epoch 1358/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.1722 - class_loss: 0.0617 - l1_loss: 1.6110\n",
      "Epoch 1359/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 24.7022 - class_loss: 0.0142 - l1_loss: 2.4688\n",
      "Epoch 1360/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.1201 - class_loss: 0.0091 - l1_loss: 1.3111\n",
      "Epoch 1361/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 21.1547 - class_loss: 0.0037 - l1_loss: 2.1151\n",
      "Epoch 1362/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 22.9359 - class_loss: 5.2852e-04 - l1_loss: 2.2935\n",
      "Epoch 1363/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.9111 - class_loss: 0.0012 - l1_loss: 1.6910\n",
      "Epoch 1364/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 15.6795 - class_loss: 0.0086 - l1_loss: 1.5671\n",
      "Epoch 1365/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.9118 - class_loss: 0.0343 - l1_loss: 1.6877\n",
      "Epoch 1366/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 23.6183 - class_loss: 0.0196 - l1_loss: 2.3599: 0s - loss: 26.8239 - class_loss: 0.0198 - l1_loss: 2.68\n",
      "Epoch 1367/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 20.8160 - class_loss: 0.0171 - l1_loss: 2.0799\n",
      "Epoch 1368/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 26.4660 - class_loss: 0.0242 - l1_loss: 2.6442\n",
      "Epoch 1369/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 17.8208 - class_loss: 0.0098 - l1_loss: 1.7811\n",
      "Epoch 1370/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 17.0102 - class_loss: 0.0025 - l1_loss: 1.7008\n",
      "Epoch 1371/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 21.4731 - class_loss: 0.0015 - l1_loss: 2.1472\n",
      "Epoch 1372/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.8545 - class_loss: 0.2856 - l1_loss: 1.5569\n",
      "Epoch 1373/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.5090 - class_loss: 0.1741 - l1_loss: 1.4335\n",
      "Epoch 1374/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 19.4092 - class_loss: 0.0826 - l1_loss: 1.9327\n",
      "Epoch 1375/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 15.5039 - class_loss: 0.0085 - l1_loss: 1.5495\n",
      "Epoch 1376/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.3536 - class_loss: 0.1252 - l1_loss: 1.7228\n",
      "Epoch 1377/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 18.7912 - class_loss: 0.0407 - l1_loss: 1.8751\n",
      "Epoch 1378/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.0247 - class_loss: 0.0168 - l1_loss: 1.6008\n",
      "Epoch 1379/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.9500 - class_loss: 0.0074 - l1_loss: 1.0943\n",
      "Epoch 1380/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.2170 - class_loss: 0.1339 - l1_loss: 0.7083A: 0s - loss: 6.8828 - class_loss: 0.1530 - l1_loss: 0.67\n",
      "Epoch 1381/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.6224 - class_loss: 0.0020 - l1_loss: 1.0620\n",
      "Epoch 1382/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.9639 - class_loss: 0.0048 - l1_loss: 0.8959\n",
      "Epoch 1383/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.6146 - class_loss: 0.0042 - l1_loss: 0.8610\n",
      "Epoch 1384/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.4468 - class_loss: 0.0078 - l1_loss: 0.8439\n",
      "Epoch 1385/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.7401 - class_loss: 0.0024 - l1_loss: 0.8738\n",
      "Epoch 1386/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.9349 - class_loss: 0.0063 - l1_loss: 0.5929\n",
      "Epoch 1387/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.7783 - class_loss: 0.0290 - l1_loss: 0.5749: 0s - loss: 6.2712 - class_loss: 0.0454 - l1_loss: 0.\n",
      "Epoch 1388/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.9543 - class_loss: 3.0204e-04 - l1_loss: 0.4954\n",
      "Epoch 1389/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.5364 - class_loss: 0.0015 - l1_loss: 0.4535\n",
      "Epoch 1390/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.6927 - class_loss: 0.0054 - l1_loss: 0.4687\n",
      "Epoch 1391/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3611 - class_loss: 0.0137 - l1_loss: 0.3347\n",
      "Epoch 1392/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3371 - class_loss: 0.0090 - l1_loss: 0.3328\n",
      "Epoch 1393/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2659 - class_loss: 0.0058 - l1_loss: 0.3260\n",
      "Epoch 1394/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2801 - class_loss: 0.0146 - l1_loss: 0.3266\n",
      "Epoch 1395/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.7390 - class_loss: 0.0017 - l1_loss: 0.3737\n",
      "Epoch 1396/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.5882 - class_loss: 0.0123 - l1_loss: 0.3576\n",
      "Epoch 1397/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8957 - class_loss: 0.0091 - l1_loss: 0.2887\n",
      "Epoch 1398/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8153 - class_loss: 0.0036 - l1_loss: 0.3812\n",
      "Epoch 1399/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1611 - class_loss: 0.0016 - l1_loss: 0.3160\n",
      "Epoch 1400/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.1362 - class_loss: 0.0077 - l1_loss: 0.4129\n",
      "Epoch 1401/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9393 - class_loss: 0.0079 - l1_loss: 0.3931\n",
      "Epoch 1402/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1998 - class_loss: 0.0156 - l1_loss: 0.3184\n",
      "Epoch 1403/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.5739 - class_loss: 0.0046 - l1_loss: 0.4569\n",
      "Epoch 1404/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5891 - class_loss: 0.0020 - l1_loss: 0.3587\n",
      "Epoch 1405/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.7962 - class_loss: 0.0028 - l1_loss: 0.3793ETA: 0s - loss: 3.7679 - class_loss: 0.0035 - l1_loss: 0.3764\n",
      "Epoch 1406/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3266 - class_loss: 0.0054 - l1_loss: 0.2321ETA: 0s - loss: 2.5643 - class_loss: 0.0068 - l1_loss: 0.\n",
      "Epoch 1407/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6814 - class_loss: 0.0140 - l1_loss: 0.2667\n",
      "Epoch 1408/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4353 - class_loss: 0.0058 - l1_loss: 0.2429\n",
      "Epoch 1409/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8896 - class_loss: 0.0017 - l1_loss: 0.1888\n",
      "Epoch 1410/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.6280 - class_loss: 0.0029 - l1_loss: 0.2625\n",
      "Epoch 1411/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0203 - class_loss: 0.0028 - l1_loss: 0.3018\n",
      "Epoch 1412/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.4430 - class_loss: 0.0042 - l1_loss: 0.3439\n",
      "Epoch 1413/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9810 - class_loss: 0.0063 - l1_loss: 0.4975\n",
      "Epoch 1414/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.2642 - class_loss: 0.0118 - l1_loss: 0.4252\n",
      "Epoch 1415/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.6790 - class_loss: 0.0044 - l1_loss: 0.4675\n",
      "Epoch 1416/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.5107 - class_loss: 0.0223 - l1_loss: 0.4488\n",
      "Epoch 1417/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.4411 - class_loss: 0.0088 - l1_loss: 0.3432\n",
      "Epoch 1418/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3055 - class_loss: 0.0016 - l1_loss: 0.3304\n",
      "Epoch 1419/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.8523 - class_loss: 0.0023 - l1_loss: 0.3850 0s - loss: 3.8920 - class_loss: 0.0025 - l1_loss: 0.38\n",
      "Epoch 1420/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9218 - class_loss: 0.0027 - l1_loss: 0.2919\n",
      "Epoch 1421/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.3534 - class_loss: 0.0031 - l1_loss: 0.2350\n",
      "Epoch 1422/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7985 - class_loss: 0.0022 - l1_loss: 0.2796\n",
      "Epoch 1423/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4792 - class_loss: 0.0074 - l1_loss: 0.2472\n",
      "Epoch 1424/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6919 - class_loss: 0.0039 - l1_loss: 0.2688\n",
      "Epoch 1425/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9073 - class_loss: 0.0015 - l1_loss: 0.1906\n",
      "Epoch 1426/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9501 - class_loss: 0.0053 - l1_loss: 0.1945\n",
      "Epoch 1427/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5485 - class_loss: 0.0067 - l1_loss: 0.1542\n",
      "Epoch 1428/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7924 - class_loss: 0.0033 - l1_loss: 0.1789\n",
      "Epoch 1429/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0572 - class_loss: 0.0051 - l1_loss: 0.2052\n",
      "Epoch 1430/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0874 - class_loss: 0.0047 - l1_loss: 0.2083\n",
      "Epoch 1431/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1326 - class_loss: 0.0048 - l1_loss: 0.2128\n",
      "Epoch 1432/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1913 - class_loss: 0.0058 - l1_loss: 0.2185\n",
      "Epoch 1433/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0748 - class_loss: 0.0025 - l1_loss: 0.2072ETA: 0s - loss: 2.5562 - class_loss: 0.0029 - l1_loss: 0.25\n",
      "Epoch 1434/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2422 - class_loss: 0.0047 - l1_loss: 0.2238\n",
      "Epoch 1435/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7320 - class_loss: 0.0071 - l1_loss: 0.1725\n",
      "Epoch 1436/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9194 - class_loss: 0.0110 - l1_loss: 0.1908\n",
      "Epoch 1437/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0375 - class_loss: 0.0046 - l1_loss: 0.2033\n",
      "Epoch 1438/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.3825 - class_loss: 0.0014 - l1_loss: 0.2381\n",
      "Epoch 1439/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.2435 - class_loss: 0.0022 - l1_loss: 0.2241\n",
      "Epoch 1440/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8334 - class_loss: 0.0011 - l1_loss: 0.1832\n",
      "Epoch 1441/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.7042 - class_loss: 0.0013 - l1_loss: 0.1703ETA: 0s - loss: 1.7987 - class_loss: 0.0015 - l1_loss: 0.1797\n",
      "Epoch 1442/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7828 - class_loss: 0.0051 - l1_loss: 0.1778\n",
      "Epoch 1443/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7550 - class_loss: 0.0086 - l1_loss: 0.1746 0s - loss: 1.7143 - class_loss: 0.0054 - l1_loss: 0.\n",
      "Epoch 1444/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8181 - class_loss: 0.0054 - l1_loss: 0.1813\n",
      "Epoch 1445/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0763 - class_loss: 7.2586e-04 - l1_loss: 0.2076\n",
      "Epoch 1446/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6899 - class_loss: 0.0012 - l1_loss: 0.1689\n",
      "Epoch 1447/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1248 - class_loss: 0.0019 - l1_loss: 0.2123\n",
      "Epoch 1448/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9964 - class_loss: 0.0033 - l1_loss: 0.2993\n",
      "Epoch 1449/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9823 - class_loss: 0.0014 - l1_loss: 0.1981\n",
      "Epoch 1450/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9624 - class_loss: 0.0033 - l1_loss: 0.1959\n",
      "Epoch 1451/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1726 - class_loss: 0.0424 - l1_loss: 0.2130\n",
      "Epoch 1452/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8480 - class_loss: 0.0014 - l1_loss: 0.1847\n",
      "Epoch 1453/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.8384 - class_loss: 5.3718e-04 - l1_loss: 0.2838\n",
      "Epoch 1454/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0510 - class_loss: 0.0011 - l1_loss: 0.3050\n",
      "Epoch 1455/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5332 - class_loss: 0.0029 - l1_loss: 0.3530\n",
      "Epoch 1456/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2388 - class_loss: 0.0051 - l1_loss: 0.3234\n",
      "Epoch 1457/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.1340 - class_loss: 0.0049 - l1_loss: 0.3129\n",
      "Epoch 1458/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6458 - class_loss: 9.1857e-04 - l1_loss: 0.3645\n",
      "Epoch 1459/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5217 - class_loss: 0.0024 - l1_loss: 0.2519\n",
      "Epoch 1460/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6024 - class_loss: 0.0021 - l1_loss: 0.2600\n",
      "Epoch 1461/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4432 - class_loss: 0.0068 - l1_loss: 0.2436\n",
      "Epoch 1462/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1775 - class_loss: 0.0057 - l1_loss: 0.2172\n",
      "Epoch 1463/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2708 - class_loss: 0.0017 - l1_loss: 0.2269\n",
      "Epoch 1464/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.5113 - class_loss: 0.0042 - l1_loss: 0.2507\n",
      "Epoch 1465/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6039 - class_loss: 0.0053 - l1_loss: 0.2599\n",
      "Epoch 1466/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2961 - class_loss: 0.0090 - l1_loss: 0.3287\n",
      "Epoch 1467/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.5435 - class_loss: 0.0079 - l1_loss: 0.3536\n",
      "Epoch 1468/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9355 - class_loss: 0.0057 - l1_loss: 0.2930\n",
      "Epoch 1469/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3558 - class_loss: 9.0405e-04 - l1_loss: 0.3355\n",
      "Epoch 1470/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5247 - class_loss: 9.4816e-04 - l1_loss: 0.2524\n",
      "Epoch 1471/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8710 - class_loss: 0.0013 - l1_loss: 0.2870\n",
      "Epoch 1472/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7962 - class_loss: 0.0014 - l1_loss: 0.2795\n",
      "Epoch 1473/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7313 - class_loss: 0.0019 - l1_loss: 0.2729\n",
      "Epoch 1474/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4137 - class_loss: 0.0210 - l1_loss: 0.2393\n",
      "Epoch 1475/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5335 - class_loss: 0.0021 - l1_loss: 0.2531ETA: 0s - loss: 1.7792 - class_loss: 0.0031 - l1_loss: 0.17\n",
      "Epoch 1476/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.3229 - class_loss: 0.0076 - l1_loss: 0.2315\n",
      "Epoch 1477/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1724 - class_loss: 0.0075 - l1_loss: 0.3165\n",
      "Epoch 1478/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.8875 - class_loss: 0.0126 - l1_loss: 0.5875\n",
      "Epoch 1479/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.2438 - class_loss: 0.0217 - l1_loss: 0.6222\n",
      "Epoch 1480/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.1988 - class_loss: 0.0057 - l1_loss: 0.6193\n",
      "Epoch 1481/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0399 - class_loss: 0.0013 - l1_loss: 0.4039\n",
      "Epoch 1482/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9549 - class_loss: 0.0015 - l1_loss: 0.2953\n",
      "Epoch 1483/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1261 - class_loss: 7.5975e-04 - l1_loss: 0.4125\n",
      "Epoch 1484/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3217 - class_loss: 7.9358e-04 - l1_loss: 0.3321\n",
      "Epoch 1485/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6349 - class_loss: 0.0011 - l1_loss: 0.2634\n",
      "Epoch 1486/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.4322 - class_loss: 0.0048 - l1_loss: 0.5427\n",
      "Epoch 1487/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.7373 - class_loss: 9.6452e-04 - l1_loss: 0.4736\n",
      "Epoch 1488/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.1566 - class_loss: 0.0134 - l1_loss: 0.5143\n",
      "Epoch 1489/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.8822 - class_loss: 0.0357 - l1_loss: 0.4846\n",
      "Epoch 1490/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.3101 - class_loss: 0.0105 - l1_loss: 0.6300\n",
      "Epoch 1491/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.1338 - class_loss: 0.0038 - l1_loss: 0.7130\n",
      "Epoch 1492/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.9456 - class_loss: 0.0050 - l1_loss: 0.7941\n",
      "Epoch 1493/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.8539 - class_loss: 0.0065 - l1_loss: 0.4847\n",
      "Epoch 1494/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.0058 - class_loss: 0.0102 - l1_loss: 0.4996 0s - loss: 5.2860 - class_loss: 0.0018 - l1_loss: \n",
      "Epoch 1495/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.7929 - class_loss: 0.0072 - l1_loss: 0.4786\n",
      "Epoch 1496/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.8476 - class_loss: 0.0014 - l1_loss: 0.3846\n",
      "Epoch 1497/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6138 - class_loss: 0.0039 - l1_loss: 0.3610\n",
      "Epoch 1498/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.2789 - class_loss: 0.0219 - l1_loss: 0.4257\n",
      "Epoch 1499/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.8379 - class_loss: 0.0031 - l1_loss: 0.4835\n",
      "Epoch 1500/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.9900 - class_loss: 9.7059e-04 - l1_loss: 0.4989\n",
      "Epoch 1501/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.2206 - class_loss: 0.0015 - l1_loss: 0.6219\n",
      "Epoch 1502/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 8.0026 - class_loss: 0.0054 - l1_loss: 0.79 - 0s 31ms/step - loss: 7.5050 - class_loss: 0.0088 - l1_loss: 0.7496\n",
      "Epoch 1503/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.2177 - class_loss: 0.0111 - l1_loss: 0.6207\n",
      "Epoch 1504/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.4276 - class_loss: 0.0096 - l1_loss: 1.4418\n",
      "Epoch 1505/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.0754 - class_loss: 0.0031 - l1_loss: 1.3072\n",
      "Epoch 1506/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 17.8538 - class_loss: 0.0492 - l1_loss: 1.7805\n",
      "Epoch 1507/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.3888 - class_loss: 0.0064 - l1_loss: 1.8382\n",
      "Epoch 1508/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.0505 - class_loss: 0.0223 - l1_loss: 1.4028\n",
      "Epoch 1509/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 19.1663 - class_loss: 0.0137 - l1_loss: 1.9153\n",
      "Epoch 1510/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.5860 - class_loss: 0.0022 - l1_loss: 1.6584\n",
      "Epoch 1511/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.9273 - class_loss: 0.0153 - l1_loss: 1.5912\n",
      "Epoch 1512/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.1555 - class_loss: 0.0037 - l1_loss: 1.3152\n",
      "Epoch 1513/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.3973 - class_loss: 0.0015 - l1_loss: 1.4396\n",
      "Epoch 1514/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.5887 - class_loss: 0.0023 - l1_loss: 1.3586\n",
      "Epoch 1515/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.5124 - class_loss: 0.0360 - l1_loss: 1.2476\n",
      "Epoch 1516/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.3143 - class_loss: 0.0124 - l1_loss: 1.5302\n",
      "Epoch 1517/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.9807 - class_loss: 0.0035 - l1_loss: 1.8977\n",
      "Epoch 1518/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.0894 - class_loss: 0.0177 - l1_loss: 1.6072\n",
      "Epoch 1519/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 20.0788 - class_loss: 0.0412 - l1_loss: 2.0038\n",
      "Epoch 1520/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.4875 - class_loss: 0.0048 - l1_loss: 1.5483\n",
      "Epoch 1521/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.8489 - class_loss: 0.0017 - l1_loss: 1.3847\n",
      "Epoch 1522/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.6136 - class_loss: 0.0016 - l1_loss: 1.4612\n",
      "Epoch 1523/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.6327 - class_loss: 0.0410 - l1_loss: 1.4592\n",
      "Epoch 1524/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.1892 - class_loss: 0.0084 - l1_loss: 0.7181\n",
      "Epoch 1525/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.0216 - class_loss: 0.0077 - l1_loss: 0.9014\n",
      "Epoch 1526/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.5501 - class_loss: 0.0025 - l1_loss: 0.7548\n",
      "Epoch 1527/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.6973 - class_loss: 7.9075e-04 - l1_loss: 1.0697\n",
      "Epoch 1528/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.0818 - class_loss: 0.0022 - l1_loss: 0.7080\n",
      "Epoch 1529/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.4232 - class_loss: 0.0091 - l1_loss: 0.7414\n",
      "Epoch 1530/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.7525 - class_loss: 0.0079 - l1_loss: 0.8745\n",
      "Epoch 1531/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.0651 - class_loss: 0.0159 - l1_loss: 0.9049\n",
      "Epoch 1532/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.8630 - class_loss: 3.2617e-04 - l1_loss: 0.8863\n",
      "Epoch 1533/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.1878 - class_loss: 0.0029 - l1_loss: 1.3185\n",
      "Epoch 1534/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.3934 - class_loss: 0.0010 - l1_loss: 0.8392\n",
      "Epoch 1535/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.9623 - class_loss: 0.0015 - l1_loss: 0.6961\n",
      "Epoch 1536/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.3707 - class_loss: 0.0377 - l1_loss: 0.8333\n",
      "Epoch 1537/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.8425 - class_loss: 6.1434e-04 - l1_loss: 0.5842\n",
      "Epoch 1538/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.8624 - class_loss: 0.0019 - l1_loss: 0.6860\n",
      "Epoch 1539/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.9634 - class_loss: 6.6548e-04 - l1_loss: 0.3963\n",
      "Epoch 1540/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.5343 - class_loss: 0.6006 - l1_loss: 0.4934\n",
      "Epoch 1541/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.6801 - class_loss: 0.5737 - l1_loss: 1.2106\n",
      "Epoch 1542/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.0259 - class_loss: 0.6154 - l1_loss: 1.4411\n",
      "Epoch 1543/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.1323 - class_loss: 0.1400 - l1_loss: 1.5992\n",
      "Epoch 1544/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 17.2266 - class_loss: 0.0237 - l1_loss: 1.7203\n",
      "Epoch 1545/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.7660 - class_loss: 0.0049 - l1_loss: 1.2761\n",
      "Epoch 1546/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.0576 - class_loss: 0.0179 - l1_loss: 1.0040\n",
      "Epoch 1547/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.7808 - class_loss: 0.0075 - l1_loss: 1.1773\n",
      "Epoch 1548/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.2668 - class_loss: 0.0108 - l1_loss: 1.1256\n",
      "Epoch 1549/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.1013 - class_loss: 0.0225 - l1_loss: 0.8079\n",
      "Epoch 1550/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.8568 - class_loss: 0.0010 - l1_loss: 0.7856\n",
      "Epoch 1551/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.0514 - class_loss: 0.0016 - l1_loss: 1.5050\n",
      "Epoch 1552/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.8620 - class_loss: 0.0062 - l1_loss: 1.2856\n",
      "Epoch 1553/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 10.5836 - class_loss: 0.2113 - l1_loss: 1.0372\n",
      "Epoch 1554/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.3383 - class_loss: 0.0096 - l1_loss: 1.3329\n",
      "Epoch 1555/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 11.5669 - class_loss: 0.0039 - l1_loss: 1.156 - 0s 30ms/step - loss: 11.5341 - class_loss: 0.0034 - l1_loss: 1.1531\n",
      "Epoch 1556/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.6099 - class_loss: 1.8635e-04 - l1_loss: 0.7610\n",
      "Epoch 1557/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.7038 - class_loss: 3.8018e-04 - l1_loss: 0.6703 0s - loss: 6.8287 - class_loss: 3.2391e-04 - l1_loss: 0.68\n",
      "Epoch 1558/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.1734 - class_loss: 0.0048 - l1_loss: 0.7169 0s - loss: 6.5039 - class_loss: 0.0025 - l1_loss: 0.65\n",
      "Epoch 1559/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.1454 - class_loss: 0.0099 - l1_loss: 0.6136\n",
      "Epoch 1560/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.4518 - class_loss: 5.4249e-04 - l1_loss: 0.6451\n",
      "Epoch 1561/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.6915 - class_loss: 0.0051 - l1_loss: 0.5686\n",
      "Epoch 1562/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.7830 - class_loss: 0.0055 - l1_loss: 0.7778\n",
      "Epoch 1563/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.9602 - class_loss: 0.0155 - l1_loss: 0.6945\n",
      "Epoch 1564/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 31ms/step - loss: 10.8386 - class_loss: 7.1687e-04 - l1_loss: 1.0838\n",
      "Epoch 1565/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.6614 - class_loss: 0.0018 - l1_loss: 1.0660\n",
      "Epoch 1566/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 11.2777 - class_loss: 0.0160 - l1_loss: 1.1262\n",
      "Epoch 1567/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 9.3661 - class_loss: 0.0033 - l1_loss: 0.9363\n",
      "Epoch 1568/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.8270 - class_loss: 0.0831 - l1_loss: 1.0744\n",
      "Epoch 1569/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.1643 - class_loss: 9.9028e-04 - l1_loss: 1.1163\n",
      "Epoch 1570/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.8118 - class_loss: 3.0273e-04 - l1_loss: 1.0811\n",
      "Epoch 1571/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.7031 - class_loss: 4.7303e-04 - l1_loss: 1.0703\n",
      "Epoch 1572/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.5012 - class_loss: 2.7354e-04 - l1_loss: 0.8501\n",
      "Epoch 1573/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.1081 - class_loss: 0.0024 - l1_loss: 0.8106\n",
      "Epoch 1574/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.0293 - class_loss: 0.0173 - l1_loss: 0.9012\n",
      "Epoch 1575/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.3969 - class_loss: 0.0061 - l1_loss: 0.7391\n",
      "Epoch 1576/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.9639 - class_loss: 0.0320 - l1_loss: 0.5932\n",
      "Epoch 1577/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.9962 - class_loss: 8.6721e-04 - l1_loss: 0.5995\n",
      "Epoch 1578/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.8719 - class_loss: 2.5125e-04 - l1_loss: 0.5872\n",
      "Epoch 1579/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7697 - class_loss: 7.7841e-04 - l1_loss: 0.4769\n",
      "Epoch 1580/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.0823 - class_loss: 0.0033 - l1_loss: 0.5079\n",
      "Epoch 1581/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7595 - class_loss: 0.0034 - l1_loss: 0.4756\n",
      "Epoch 1582/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.9674 - class_loss: 0.0027 - l1_loss: 0.4965\n",
      "Epoch 1583/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.0095 - class_loss: 0.0018 - l1_loss: 0.5008\n",
      "Epoch 1584/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.0513 - class_loss: 0.0044 - l1_loss: 0.5047\n",
      "Epoch 1585/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.6419 - class_loss: 9.5203e-04 - l1_loss: 0.4641\n",
      "Epoch 1586/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7291 - class_loss: 7.1822e-04 - l1_loss: 0.3728\n",
      "Epoch 1587/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.0968 - class_loss: 0.0029 - l1_loss: 0.7094\n",
      "Epoch 1588/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.4429 - class_loss: 0.0101 - l1_loss: 0.7433\n",
      "Epoch 1589/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.7991 - class_loss: 0.0031 - l1_loss: 0.6796\n",
      "Epoch 1590/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.2677 - class_loss: 0.0067 - l1_loss: 0.9261\n",
      "Epoch 1591/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.7142 - class_loss: 0.0139 - l1_loss: 0.5700\n",
      "Epoch 1592/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.5423 - class_loss: 0.0051 - l1_loss: 0.5537 0s - loss: 5.5663 - class_loss: 0.0073 - l1_loss: 0.\n",
      "Epoch 1593/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.6080 - class_loss: 0.0018 - l1_loss: 0.6606\n",
      "Epoch 1594/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.4982 - class_loss: 0.0024 - l1_loss: 0.7496\n",
      "Epoch 1595/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.2663 - class_loss: 0.0079 - l1_loss: 0.8258\n",
      "Epoch 1596/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.7081 - class_loss: 0.0111 - l1_loss: 0.7697\n",
      "Epoch 1597/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.5350 - class_loss: 0.0478 - l1_loss: 0.8487\n",
      "Epoch 1598/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.4695 - class_loss: 6.6237e-04 - l1_loss: 0.7469\n",
      "Epoch 1599/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.2918 - class_loss: 7.3954e-04 - l1_loss: 0.6291\n",
      "Epoch 1600/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.7788 - class_loss: 0.0027 - l1_loss: 0.5776\n",
      "Epoch 1601/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.8714 - class_loss: 0.0060 - l1_loss: 0.5865\n",
      "Epoch 1602/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9408 - class_loss: 0.0025 - l1_loss: 0.3938\n",
      "Epoch 1603/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.7177 - class_loss: 0.0016 - l1_loss: 0.6716ETA: 0s - loss: 7.0399 - class_loss: 0.0016 - l1_loss: 0.\n",
      "Epoch 1604/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.4108 - class_loss: 0.0028 - l1_loss: 0.6408\n",
      "Epoch 1605/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.6701 - class_loss: 0.0040 - l1_loss: 0.4666\n",
      "Epoch 1606/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9956 - class_loss: 0.0053 - l1_loss: 0.3990 0s - loss: 3.2266 - class_loss: 0.0072 - l1_loss: 0.\n",
      "Epoch 1607/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3303 - class_loss: 0.0012 - l1_loss: 0.4329\n",
      "Epoch 1608/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6235 - class_loss: 0.0082 - l1_loss: 0.3615\n",
      "Epoch 1609/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2575 - class_loss: 0.0021 - l1_loss: 0.3255\n",
      "Epoch 1610/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8553 - class_loss: 0.0011 - l1_loss: 0.3854\n",
      "Epoch 1611/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.0607 - class_loss: 0.0041 - l1_loss: 0.4057 0s - loss: 4.5770 - class_loss: 0.0015 - l1_loss: 0.\n",
      "Epoch 1612/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2749 - class_loss: 0.0038 - l1_loss: 0.3271\n",
      "Epoch 1613/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.1642 - class_loss: 0.0029 - l1_loss: 0.4161\n",
      "Epoch 1614/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.7782 - class_loss: 0.0091 - l1_loss: 0.2769\n",
      "Epoch 1615/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.9487 - class_loss: 0.0041 - l1_loss: 0.29 - 0s 30ms/step - loss: 3.0047 - class_loss: 0.0037 - l1_loss: 0.3001\n",
      "Epoch 1616/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3284 - class_loss: 0.0046 - l1_loss: 0.3324\n",
      "Epoch 1617/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8175 - class_loss: 0.0021 - l1_loss: 0.2815\n",
      "Epoch 1618/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7934 - class_loss: 0.0011 - l1_loss: 0.2792\n",
      "Epoch 1619/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3599 - class_loss: 0.0010 - l1_loss: 0.3359\n",
      "Epoch 1620/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7151 - class_loss: 9.4435e-04 - l1_loss: 0.2714\n",
      "Epoch 1621/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5914 - class_loss: 0.0037 - l1_loss: 0.2588\n",
      "Epoch 1622/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6427 - class_loss: 0.0046 - l1_loss: 0.2638\n",
      "Epoch 1623/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4965 - class_loss: 0.0079 - l1_loss: 0.2489\n",
      "Epoch 1624/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6677 - class_loss: 0.0060 - l1_loss: 0.2662\n",
      "Epoch 1625/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.2028 - class_loss: 0.0039 - l1_loss: 0.2199\n",
      "Epoch 1626/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9549 - class_loss: 0.0030 - l1_loss: 0.1952\n",
      "Epoch 1627/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7761 - class_loss: 0.0044 - l1_loss: 0.2772\n",
      "Epoch 1628/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2390 - class_loss: 0.0026 - l1_loss: 0.2236 0s - loss: 2.2874 - class_loss: 0.0027 - l1_loss: 0.22\n",
      "Epoch 1629/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.7178 - class_loss: 0.0040 - l1_loss: 0.2714\n",
      "Epoch 1630/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6773 - class_loss: 0.0053 - l1_loss: 0.1672\n",
      "Epoch 1631/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3729 - class_loss: 0.0018 - l1_loss: 0.2371\n",
      "Epoch 1632/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2657 - class_loss: 0.0031 - l1_loss: 0.2263\n",
      "Epoch 1633/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7955 - class_loss: 0.0052 - l1_loss: 0.2790\n",
      "Epoch 1634/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.3757 - class_loss: 0.0021 - l1_loss: 0.2374\n",
      "Epoch 1635/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7090 - class_loss: 0.0037 - l1_loss: 0.2705\n",
      "Epoch 1636/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3736 - class_loss: 0.0038 - l1_loss: 0.2370\n",
      "Epoch 1637/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3297 - class_loss: 0.0035 - l1_loss: 0.2326\n",
      "Epoch 1638/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1687 - class_loss: 0.0049 - l1_loss: 0.2164\n",
      "Epoch 1639/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8884 - class_loss: 0.0017 - l1_loss: 0.2887\n",
      "Epoch 1640/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 3.5163 - class_loss: 7.3882e-04 - l1_loss: 0.3516\n",
      "Epoch 1641/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6358 - class_loss: 0.0101 - l1_loss: 0.2626\n",
      "Epoch 1642/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.2217 - class_loss: 0.0024 - l1_loss: 0.3219 0s - loss: 2.8956 - class_loss: 0.0027 - l1_loss: 0.\n",
      "Epoch 1643/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.3326 - class_loss: 0.0016 - l1_loss: 0.3331\n",
      "Epoch 1644/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2761 - class_loss: 0.0021 - l1_loss: 0.3274\n",
      "Epoch 1645/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0811 - class_loss: 0.0019 - l1_loss: 0.3079\n",
      "Epoch 1646/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1672 - class_loss: 0.0101 - l1_loss: 0.3157\n",
      "Epoch 1647/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5684 - class_loss: 0.0070 - l1_loss: 0.4561\n",
      "Epoch 1648/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.7231 - class_loss: 0.0163 - l1_loss: 0.3707\n",
      "Epoch 1649/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.8734 - class_loss: 9.9368e-04 - l1_loss: 0.6872\n",
      "Epoch 1650/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.1908 - class_loss: 0.0010 - l1_loss: 0.7190\n",
      "Epoch 1651/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.3963 - class_loss: 0.0011 - l1_loss: 0.5395\n",
      "Epoch 1652/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.9446 - class_loss: 0.0016 - l1_loss: 0.8943\n",
      "Epoch 1653/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.0096 - class_loss: 0.0126 - l1_loss: 0.7997\n",
      "Epoch 1654/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.4205 - class_loss: 0.0081 - l1_loss: 0.6412\n",
      "Epoch 1655/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.0963 - class_loss: 0.0045 - l1_loss: 0.6092\n",
      "Epoch 1656/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.5934 - class_loss: 0.0021 - l1_loss: 0.5591\n",
      "Epoch 1657/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.0031 - class_loss: 0.0277 - l1_loss: 0.5975\n",
      "Epoch 1658/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3253 - class_loss: 0.0045 - l1_loss: 0.4321\n",
      "Epoch 1659/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2371 - class_loss: 0.0018 - l1_loss: 0.4235\n",
      "Epoch 1660/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.5161 - class_loss: 0.0020 - l1_loss: 0.6514 0s - loss: 6.3981 - class_loss: 0.0024 - l1_loss: 0.\n",
      "Epoch 1661/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.2202 - class_loss: 0.0042 - l1_loss: 0.5216\n",
      "Epoch 1662/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.8406 - class_loss: 0.0016 - l1_loss: 1.1839\n",
      "Epoch 1663/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.3170 - class_loss: 0.0015 - l1_loss: 0.5316\n",
      "Epoch 1664/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.8481 - class_loss: 0.0026 - l1_loss: 0.7845\n",
      "Epoch 1665/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 8.0118 - class_loss: 0.0047 - l1_loss: 0.8007\n",
      "Epoch 1666/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.6868 - class_loss: 0.0713 - l1_loss: 0.6615\n",
      "Epoch 1667/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.7666 - class_loss: 9.4445e-04 - l1_loss: 0.5766\n",
      "Epoch 1668/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.9954 - class_loss: 4.1711e-04 - l1_loss: 0.8995\n",
      "Epoch 1669/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.3711 - class_loss: 0.0284 - l1_loss: 1.4343\n",
      "Epoch 1670/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.8013 - class_loss: 0.0014 - l1_loss: 1.0800\n",
      "Epoch 1671/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.7772 - class_loss: 0.0029 - l1_loss: 0.9774\n",
      "Epoch 1672/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.7014 - class_loss: 0.0159 - l1_loss: 0.8685\n",
      "Epoch 1673/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.1400 - class_loss: 0.0041 - l1_loss: 0.9136\n",
      "Epoch 1674/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.3278 - class_loss: 0.0024 - l1_loss: 0.6325\n",
      "Epoch 1675/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.9693 - class_loss: 0.0037 - l1_loss: 0.7966\n",
      "Epoch 1676/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.6937 - class_loss: 0.0369 - l1_loss: 0.8657\n",
      "Epoch 1677/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.8296 - class_loss: 0.0014 - l1_loss: 0.6828\n",
      "Epoch 1678/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.9627 - class_loss: 0.0017 - l1_loss: 0.6961\n",
      "Epoch 1679/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.7031 - class_loss: 5.2084e-04 - l1_loss: 0.6703\n",
      "Epoch 1680/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.6626 - class_loss: 4.9177e-04 - l1_loss: 0.7662\n",
      "Epoch 1681/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 5.5797 - class_loss: 0.0427 - l1_loss: 0.55 - 0s 31ms/step - loss: 5.3720 - class_loss: 0.0375 - l1_loss: 0.5335\n",
      "Epoch 1682/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.5098 - class_loss: 0.0377 - l1_loss: 0.7472\n",
      "Epoch 1683/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4218 - class_loss: 1.3550e-04 - l1_loss: 0.3422\n",
      "Epoch 1684/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6458 - class_loss: 1.8667e-04 - l1_loss: 0.3646\n",
      "Epoch 1685/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.8349 - class_loss: 0.0014 - l1_loss: 0.3834\n",
      "Epoch 1686/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 4.2321 - class_loss: 0.0078 - l1_loss: 0.42 - 0s 31ms/step - loss: 3.9406 - class_loss: 0.0071 - l1_loss: 0.3934\n",
      "Epoch 1687/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1827 - class_loss: 0.0142 - l1_loss: 0.4168\n",
      "Epoch 1688/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7522 - class_loss: 0.0014 - l1_loss: 0.3751\n",
      "Epoch 1689/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7764 - class_loss: 0.0019 - l1_loss: 0.3774\n",
      "Epoch 1690/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1953 - class_loss: 0.0049 - l1_loss: 0.4190\n",
      "Epoch 1691/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7725 - class_loss: 0.0038 - l1_loss: 0.3769\n",
      "Epoch 1692/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4596 - class_loss: 0.0018 - l1_loss: 0.3458\n",
      "Epoch 1693/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3125 - class_loss: 0.0021 - l1_loss: 0.3310\n",
      "Epoch 1694/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 3.9474 - class_loss: 0.0019 - l1_loss: 0.3945\n",
      "Epoch 1695/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5307 - class_loss: 0.0206 - l1_loss: 0.3510\n",
      "Epoch 1696/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2592 - class_loss: 0.0040 - l1_loss: 0.3255\n",
      "Epoch 1697/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6934 - class_loss: 0.0019 - l1_loss: 0.3692\n",
      "Epoch 1698/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8904 - class_loss: 0.0025 - l1_loss: 0.2888\n",
      "Epoch 1699/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7787 - class_loss: 0.0074 - l1_loss: 0.3771\n",
      "Epoch 1700/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6230 - class_loss: 0.0049 - l1_loss: 0.3618\n",
      "Epoch 1701/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0637 - class_loss: 0.0040 - l1_loss: 0.3060\n",
      "Epoch 1702/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 4.9568 - class_loss: 8.6612e-04 - l1_loss: 0.4956\n",
      "Epoch 1703/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 4.9799 - class_loss: 0.0037 - l1_loss: 0.4976\n",
      "Epoch 1704/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.9125 - class_loss: 0.0028 - l1_loss: 0.4910\n",
      "Epoch 1705/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.8462 - class_loss: 0.0087 - l1_loss: 0.2837\n",
      "Epoch 1706/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.2322 - class_loss: 0.0034 - l1_loss: 0.3229\n",
      "Epoch 1707/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 3.1484 - class_loss: 0.0066 - l1_loss: 0.3142\n",
      "Epoch 1708/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.9975 - class_loss: 0.0065 - l1_loss: 0.2991\n",
      "Epoch 1709/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.2844 - class_loss: 0.0024 - l1_loss: 0.2282\n",
      "Epoch 1710/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.2333 - class_loss: 0.0020 - l1_loss: 0.3231\n",
      "Epoch 1711/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.6063 - class_loss: 0.0017 - l1_loss: 0.3605\n",
      "Epoch 1712/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 2.9620 - class_loss: 0.0017 - l1_loss: 0.2960\n",
      "Epoch 1713/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 3.1103 - class_loss: 0.0020 - l1_loss: 0.3108\n",
      "Epoch 1714/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 4.7184 - class_loss: 0.0019 - l1_loss: 0.4717\n",
      "Epoch 1715/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 5.7724 - class_loss: 0.0028 - l1_loss: 0.5770\n",
      "Epoch 1716/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 5.7405 - class_loss: 0.0031 - l1_loss: 0.5737\n",
      "Epoch 1717/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.3591 - class_loss: 0.0093 - l1_loss: 0.3350\n",
      "Epoch 1718/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 6.8113 - class_loss: 0.0101 - l1_loss: 0.6801\n",
      "Epoch 1719/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.6018 - class_loss: 0.0238 - l1_loss: 0.4578\n",
      "Epoch 1720/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.7794 - class_loss: 0.0020 - l1_loss: 0.4777\n",
      "Epoch 1721/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.1777 - class_loss: 0.0051 - l1_loss: 0.6173\n",
      "Epoch 1722/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 6.9345 - class_loss: 0.0041 - l1_loss: 0.6930\n",
      "Epoch 1723/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 14.1657 - class_loss: 0.0037 - l1_loss: 1.4162\n",
      "Epoch 1724/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 8.7068 - class_loss: 0.0017 - l1_loss: 0.8705\n",
      "Epoch 1725/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 11.3567 - class_loss: 0.0061 - l1_loss: 1.1351\n",
      "Epoch 1726/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.1987 - class_loss: 0.0070 - l1_loss: 0.7192\n",
      "Epoch 1727/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.6468 - class_loss: 0.0170 - l1_loss: 0.6630\n",
      "Epoch 1728/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.8833 - class_loss: 0.0199 - l1_loss: 0.5863\n",
      "Epoch 1729/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 7.0742 - class_loss: 0.0078 - l1_loss: 0.7066\n",
      "Epoch 1730/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.3223 - class_loss: 0.0059 - l1_loss: 0.6316\n",
      "Epoch 1731/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.5164 - class_loss: 0.0038 - l1_loss: 0.4513\n",
      "Epoch 1732/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.3341 - class_loss: 0.0037 - l1_loss: 0.9330\n",
      "Epoch 1733/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.4846 - class_loss: 0.0021 - l1_loss: 0.6482\n",
      "Epoch 1734/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.3019 - class_loss: 0.0040 - l1_loss: 0.9298\n",
      "Epoch 1735/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.2884 - class_loss: 0.0305 - l1_loss: 0.9258\n",
      "Epoch 1736/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 11.0186 - class_loss: 0.3552 - l1_loss: 1.0663\n",
      "Epoch 1737/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 20.8697 - class_loss: 0.2841 - l1_loss: 2.0586\n",
      "Epoch 1738/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.8464 - class_loss: 0.1161 - l1_loss: 1.5730\n",
      "Epoch 1739/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 14.8498 - class_loss: 0.0183 - l1_loss: 1.4832\n",
      "Epoch 1740/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 15.2141 - class_loss: 0.0264 - l1_loss: 1.5188\n",
      "Epoch 1741/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 21.3579 - class_loss: 0.0129 - l1_loss: 2.1345\n",
      "Epoch 1742/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 18.9567 - class_loss: 0.0076 - l1_loss: 1.8949\n",
      "Epoch 1743/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 15.2143 - class_loss: 0.0021 - l1_loss: 1.5212\n",
      "Epoch 1744/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 13.9707 - class_loss: 0.0086 - l1_loss: 1.3962\n",
      "Epoch 1745/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 13.5750 - class_loss: 0.0027 - l1_loss: 1.3572\n",
      "Epoch 1746/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 10.2938 - class_loss: 0.0018 - l1_loss: 1.0292\n",
      "Epoch 1747/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 11.8108 - class_loss: 0.0030 - l1_loss: 1.1808\n",
      "Epoch 1748/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 11.5478 - class_loss: 0.0067 - l1_loss: 1.1541\n",
      "Epoch 1749/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.1689 - class_loss: 0.0057 - l1_loss: 1.0163\n",
      "Epoch 1750/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 13.2715 - class_loss: 0.0425 - l1_loss: 1.3229\n",
      "Epoch 1751/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.7223 - class_loss: 0.0226 - l1_loss: 1.0700\n",
      "Epoch 1752/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 12.7474 - class_loss: 0.0017 - l1_loss: 1.2746\n",
      "Epoch 1753/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 21.3213 - class_loss: 0.0016 - l1_loss: 2.1320\n",
      "Epoch 1754/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.8186 - class_loss: 8.9189e-04 - l1_loss: 1.6818\n",
      "Epoch 1755/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 16.3668 - class_loss: 7.0006e-04 - l1_loss: 1.6366\n",
      "Epoch 1756/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 10.4662 - class_loss: 0.0011 - l1_loss: 1.0465\n",
      "Epoch 1757/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 10.7350 - class_loss: 0.0015 - l1_loss: 1.0734\n",
      "Epoch 1758/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 14.2452 - class_loss: 0.0525 - l1_loss: 1.4193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1759/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.4524 - class_loss: 0.0033 - l1_loss: 1.0449\n",
      "Epoch 1760/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 10.5796 - class_loss: 0.0025 - l1_loss: 1.0577\n",
      "Epoch 1761/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.7256 - class_loss: 0.0023 - l1_loss: 0.9723\n",
      "Epoch 1762/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.2342 - class_loss: 0.0041 - l1_loss: 0.9230\n",
      "Epoch 1763/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.3261 - class_loss: 0.0410 - l1_loss: 0.9285\n",
      "Epoch 1764/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 11.6104 - class_loss: 0.0350 - l1_loss: 1.1575\n",
      "Epoch 1765/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.3667 - class_loss: 0.0013 - l1_loss: 1.3365\n",
      "Epoch 1766/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.2485 - class_loss: 0.0028 - l1_loss: 1.0246\n",
      "Epoch 1767/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.2196 - class_loss: 0.0051 - l1_loss: 1.5214\n",
      "Epoch 1768/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.5520 - class_loss: 0.0090 - l1_loss: 0.8543\n",
      "Epoch 1769/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 11.8182 - class_loss: 0.0374 - l1_loss: 1.1781\n",
      "Epoch 1770/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.8952 - class_loss: 0.0058 - l1_loss: 0.9889\n",
      "Epoch 1771/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.4197 - class_loss: 0.0011 - l1_loss: 1.0419\n",
      "Epoch 1772/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 11.1778 - class_loss: 0.0016 - l1_loss: 1.1176\n",
      "Epoch 1773/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.4466 - class_loss: 7.1396e-04 - l1_loss: 1.1446\n",
      "Epoch 1774/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.6477 - class_loss: 0.0029 - l1_loss: 0.9645\n",
      "Epoch 1775/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.7739 - class_loss: 0.0013 - l1_loss: 1.2773\n",
      "Epoch 1776/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.1981 - class_loss: 0.0081 - l1_loss: 1.3190\n",
      "Epoch 1777/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.1575 - class_loss: 0.1847 - l1_loss: 0.7973\n",
      "Epoch 1778/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.5990 - class_loss: 0.0068 - l1_loss: 0.7592\n",
      "Epoch 1779/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.7989 - class_loss: 0.0020 - l1_loss: 0.6797\n",
      "Epoch 1780/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.0582 - class_loss: 0.0011 - l1_loss: 1.0057\n",
      "Epoch 1781/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.9422 - class_loss: 4.4192e-04 - l1_loss: 1.0942\n",
      "Epoch 1782/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.9461 - class_loss: 0.0273 - l1_loss: 1.2919\n",
      "Epoch 1783/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 11.2730 - class_loss: 0.0013 - l1_loss: 1.1272\n",
      "Epoch 1784/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.0795 - class_loss: 9.5059e-04 - l1_loss: 1.1079\n",
      "Epoch 1785/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.5785 - class_loss: 0.0025 - l1_loss: 1.1576\n",
      "Epoch 1786/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.9089 - class_loss: 0.0267 - l1_loss: 1.58820s - loss: 17.3484 - class_loss: 0.0103 - l1_loss: 1.7\n",
      "Epoch 1787/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.9802 - class_loss: 0.0091 - l1_loss: 1.1971: 0s - loss: 10.8327 - class_loss: 0.0206 - l1_loss: 1.0\n",
      "Epoch 1788/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.8876 - class_loss: 0.0136 - l1_loss: 1.2874\n",
      "Epoch 1789/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.7279 - class_loss: 0.0080 - l1_loss: 1.6720\n",
      "Epoch 1790/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.3128 - class_loss: 0.0251 - l1_loss: 1.5288\n",
      "Epoch 1791/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.7455 - class_loss: 0.0024 - l1_loss: 1.1743 ETA: 0s - loss: 12.2522 - class_loss: 0.0026 - l1_loss: 1.225\n",
      "Epoch 1792/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.0613 - class_loss: 0.0019 - l1_loss: 1.1059\n",
      "Epoch 1793/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 17.1134 - class_loss: 0.0066 - l1_loss: 1.7107\n",
      "Epoch 1794/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 16.8073 - class_loss: 0.0012 - l1_loss: 1.6806\n",
      "Epoch 1795/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 12.2049 - class_loss: 2.3889e-04 - l1_loss: 1.2205\n",
      "Epoch 1796/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.5453 - class_loss: 0.0011 - l1_loss: 1.0544\n",
      "Epoch 1797/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.4534 - class_loss: 0.0045 - l1_loss: 1.4449\n",
      "Epoch 1798/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.1289 - class_loss: 0.0017 - l1_loss: 1.1127\n",
      "Epoch 1799/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.8497 - class_loss: 0.0046 - l1_loss: 1.2845\n",
      "Epoch 1800/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.9291 - class_loss: 0.0035 - l1_loss: 0.7926\n",
      "Epoch 1801/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.5764 - class_loss: 0.3545 - l1_loss: 1.0222 ETA: 0s - loss: 11.2383 - class_loss: 0.0122 - l1_loss: 1.12\n",
      "Epoch 1802/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.4600 - class_loss: 0.0214 - l1_loss: 0.9439\n",
      "Epoch 1803/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.9598 - class_loss: 0.0476 - l1_loss: 0.7912\n",
      "Epoch 1804/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.0934 - class_loss: 0.0108 - l1_loss: 0.5083 0s - loss: 4.8452 - class_loss: 0.0146 - l1_loss: \n",
      "Epoch 1805/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.7297 - class_loss: 0.0052 - l1_loss: 0.7725\n",
      "Epoch 1806/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.9603 - class_loss: 0.0024 - l1_loss: 0.5958\n",
      "Epoch 1807/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.7196 - class_loss: 0.0099 - l1_loss: 0.7710\n",
      "Epoch 1808/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.7534 - class_loss: 0.0056 - l1_loss: 0.8748\n",
      "Epoch 1809/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.1677 - class_loss: 0.0032 - l1_loss: 0.7164\n",
      "Epoch 1810/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.7795 - class_loss: 0.0048 - l1_loss: 0.7775\n",
      "Epoch 1811/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.8972 - class_loss: 0.0024 - l1_loss: 0.6895\n",
      "Epoch 1812/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.9863 - class_loss: 0.0118 - l1_loss: 0.6975\n",
      "Epoch 1813/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.9711 - class_loss: 0.0383 - l1_loss: 0.6933\n",
      "Epoch 1814/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.5057 - class_loss: 0.0021 - l1_loss: 0.5504\n",
      "Epoch 1815/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8536 - class_loss: 0.0020 - l1_loss: 0.4852\n",
      "Epoch 1816/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7941 - class_loss: 0.0029 - l1_loss: 0.4791\n",
      "Epoch 1817/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.6178 - class_loss: 0.0045 - l1_loss: 0.4613\n",
      "Epoch 1818/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7765 - class_loss: 0.0067 - l1_loss: 0.4770\n",
      "Epoch 1819/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.3731 - class_loss: 0.0235 - l1_loss: 0.3350\n",
      "Epoch 1820/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7073 - class_loss: 0.0061 - l1_loss: 0.2701\n",
      "Epoch 1821/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1934 - class_loss: 0.0015 - l1_loss: 0.3192\n",
      "Epoch 1822/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4199 - class_loss: 0.0030 - l1_loss: 0.2417\n",
      "Epoch 1823/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5388 - class_loss: 0.0049 - l1_loss: 0.2534\n",
      "Epoch 1824/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7423 - class_loss: 0.0077 - l1_loss: 0.2735\n",
      "Epoch 1825/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7815 - class_loss: 0.0013 - l1_loss: 0.1780\n",
      "Epoch 1826/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0536 - class_loss: 0.0023 - l1_loss: 0.2051\n",
      "Epoch 1827/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7677 - class_loss: 0.0030 - l1_loss: 0.1765\n",
      "Epoch 1828/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5058 - class_loss: 0.0016 - l1_loss: 0.1504 0s - loss: 1.4375 - class_loss: 0.0014 - l1_loss: 0.\n",
      "Epoch 1829/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7770 - class_loss: 0.0019 - l1_loss: 0.1775\n",
      "Epoch 1830/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.1290 - class_loss: 0.0064 - l1_loss: 0.2123\n",
      "Epoch 1831/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7663 - class_loss: 0.0052 - l1_loss: 0.2761\n",
      "Epoch 1832/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4477 - class_loss: 0.0035 - l1_loss: 0.2444\n",
      "Epoch 1833/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0595 - class_loss: 0.0194 - l1_loss: 0.3040\n",
      "Epoch 1834/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7613 - class_loss: 0.0014 - l1_loss: 0.2760\n",
      "Epoch 1835/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1684 - class_loss: 0.0020 - l1_loss: 0.2166\n",
      "Epoch 1836/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3016 - class_loss: 0.0043 - l1_loss: 0.2297\n",
      "Epoch 1837/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2013 - class_loss: 0.0026 - l1_loss: 0.3199\n",
      "Epoch 1838/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.0895 - class_loss: 0.0027 - l1_loss: 0.2087\n",
      "Epoch 1839/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4226 - class_loss: 0.0037 - l1_loss: 0.2419\n",
      "Epoch 1840/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8761 - class_loss: 0.0017 - l1_loss: 0.1874\n",
      "Epoch 1841/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.7756 - class_loss: 0.0033 - l1_loss: 0.2772\n",
      "Epoch 1842/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.3262 - class_loss: 0.0036 - l1_loss: 0.2323\n",
      "Epoch 1843/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.1576 - class_loss: 0.0105 - l1_loss: 0.3147\n",
      "Epoch 1844/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4052 - class_loss: 0.0171 - l1_loss: 0.3388 0s - loss: 5.0006 - class_loss: 0.0446 - l1_loss: 0.49 - ETA: 0s - loss: 4.2361 - class_loss: 0.0271 - l1_loss: 0.\n",
      "Epoch 1845/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.5896 - class_loss: 4.7042e-04 - l1_loss: 0.2589\n",
      "Epoch 1846/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2266 - class_loss: 0.0012 - l1_loss: 0.3225\n",
      "Epoch 1847/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5218 - class_loss: 0.0029 - l1_loss: 0.2519\n",
      "Epoch 1848/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.2577 - class_loss: 0.0022 - l1_loss: 0.2255\n",
      "Epoch 1849/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6807 - class_loss: 0.0024 - l1_loss: 0.2678\n",
      "Epoch 1850/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.0201 - class_loss: 0.0055 - l1_loss: 0.3015\n",
      "Epoch 1851/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.5183 - class_loss: 0.0072 - l1_loss: 0.2511\n",
      "Epoch 1852/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2928 - class_loss: 0.0126 - l1_loss: 0.3280\n",
      "Epoch 1853/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.5539 - class_loss: 0.0052 - l1_loss: 0.4549\n",
      "Epoch 1854/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.8318 - class_loss: 7.2725e-04 - l1_loss: 0.5831\n",
      "Epoch 1855/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.5924 - class_loss: 7.1188e-04 - l1_loss: 0.5592\n",
      "Epoch 1856/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.4610 - class_loss: 0.0011 - l1_loss: 0.4460\n",
      "Epoch 1857/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.6922 - class_loss: 0.0013 - l1_loss: 0.6691\n",
      "Epoch 1858/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.1404 - class_loss: 0.0042 - l1_loss: 0.4136\n",
      "Epoch 1859/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.1690 - class_loss: 0.0180 - l1_loss: 0.4151\n",
      "Epoch 1860/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.2982 - class_loss: 0.0058 - l1_loss: 0.4292\n",
      "Epoch 1861/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.9719 - class_loss: 9.8353e-04 - l1_loss: 0.4971\n",
      "Epoch 1862/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.4504 - class_loss: 0.0029 - l1_loss: 0.6448\n",
      "Epoch 1863/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.1202 - class_loss: 0.0027 - l1_loss: 0.5118\n",
      "Epoch 1864/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 13.5990 - class_loss: 0.0019 - l1_loss: 1.3597\n",
      "Epoch 1865/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.0100 - class_loss: 0.0047 - l1_loss: 1.6005\n",
      "Epoch 1866/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 43.1487 - class_loss: 0.0915 - l1_loss: 4.3057\n",
      "Epoch 1867/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 43.3008 - class_loss: 0.0213 - l1_loss: 4.3280\n",
      "Epoch 1868/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 92.3421 - class_loss: 0.0361 - l1_loss: 9.2306\n",
      "Epoch 1869/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 73.9443 - class_loss: 0.2535 - l1_loss: 7.3691\n",
      "Epoch 1870/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 98.3163 - class_loss: 0.1684 - l1_loss: 9.8148\n",
      "Epoch 1871/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 102.2203 - class_loss: 0.0187 - l1_loss: 10.2202\n",
      "Epoch 1872/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 94.8767 - class_loss: 5.8385e-04 - l1_loss: 9.4876\n",
      "Epoch 1873/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 46.2128 - class_loss: 0.1395 - l1_loss: 4.6073\n",
      "Epoch 1874/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 49.2204 - class_loss: 0.0011 - l1_loss: 4.9219\n",
      "Epoch 1875/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 65.0785 - class_loss: 4.2296e-04 - l1_loss: 6.5078\n",
      "Epoch 1876/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 36.0300 - class_loss: 5.2048e-04 - l1_loss: 3.6029\n",
      "Epoch 1877/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 26.3381 - class_loss: 2.1315e-04 - l1_loss: 2.6338\n",
      "Epoch 1878/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.1943 - class_loss: 6.0932e-04 - l1_loss: 1.4194\n",
      "Epoch 1879/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.5237 - class_loss: 5.3239e-04 - l1_loss: 1.2523\n",
      "Epoch 1880/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.7831 - class_loss: 0.0249 - l1_loss: 1.0758\n",
      "Epoch 1881/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.2233 - class_loss: 0.0075 - l1_loss: 0.9216\n",
      "Epoch 1882/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.5028 - class_loss: 8.1093e-04 - l1_loss: 0.7502\n",
      "Epoch 1883/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.1516 - class_loss: 5.6959e-04 - l1_loss: 0.9151\n",
      "Epoch 1884/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.4592 - class_loss: 0.0035 - l1_loss: 0.7456\n",
      "Epoch 1885/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.9200 - class_loss: 0.0044 - l1_loss: 0.6916\n",
      "Epoch 1886/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.7532 - class_loss: 0.0011 - l1_loss: 0.5752\n",
      "Epoch 1887/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.9630 - class_loss: 0.0035 - l1_loss: 0.4959\n",
      "Epoch 1888/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 28ms/step - loss: 3.6126 - class_loss: 0.0062 - l1_loss: 0.3606\n",
      "Epoch 1889/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.9728 - class_loss: 0.0248 - l1_loss: 0.3948\n",
      "Epoch 1890/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2866 - class_loss: 6.9344e-04 - l1_loss: 0.3286\n",
      "Epoch 1891/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6474 - class_loss: 0.0013 - l1_loss: 0.2646\n",
      "Epoch 1892/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6698 - class_loss: 0.0036 - l1_loss: 0.2666\n",
      "Epoch 1893/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8849 - class_loss: 0.0111 - l1_loss: 0.1874\n",
      "Epoch 1894/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7271 - class_loss: 0.0111 - l1_loss: 0.1716\n",
      "Epoch 1895/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7875 - class_loss: 0.0044 - l1_loss: 0.1783\n",
      "Epoch 1896/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4159 - class_loss: 0.0035 - l1_loss: 0.1412\n",
      "Epoch 1897/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.4490 - class_loss: 0.0053 - l1_loss: 0.1444\n",
      "Epoch 1898/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1677 - class_loss: 0.0103 - l1_loss: 0.1157\n",
      "Epoch 1899/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9992 - class_loss: 0.0065 - l1_loss: 0.0993\n",
      "Epoch 1900/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2292 - class_loss: 0.0033 - l1_loss: 0.1226\n",
      "Epoch 1901/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2044 - class_loss: 0.0140 - l1_loss: 0.1190\n",
      "Epoch 1902/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2436 - class_loss: 0.0093 - l1_loss: 0.1234\n",
      "Epoch 1903/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2797 - class_loss: 0.0026 - l1_loss: 0.1277\n",
      "Epoch 1904/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0331 - class_loss: 0.0030 - l1_loss: 0.1030 0s - loss: 0.9521 - class_loss: 0.0023 - l1_loss: 0.\n",
      "Epoch 1905/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0279 - class_loss: 0.0040 - l1_loss: 0.1024\n",
      "Epoch 1906/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5039 - class_loss: 0.0035 - l1_loss: 0.1500\n",
      "Epoch 1907/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2100 - class_loss: 0.0046 - l1_loss: 0.1205\n",
      "Epoch 1908/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1133 - class_loss: 0.0041 - l1_loss: 0.1109\n",
      "Epoch 1909/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9863 - class_loss: 0.0033 - l1_loss: 0.0983\n",
      "Epoch 1910/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8108 - class_loss: 0.0045 - l1_loss: 0.0806\n",
      "Epoch 1911/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0271 - class_loss: 0.0031 - l1_loss: 0.1024\n",
      "Epoch 1912/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2573 - class_loss: 0.0037 - l1_loss: 0.1254\n",
      "Epoch 1913/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5536 - class_loss: 0.0026 - l1_loss: 0.1551\n",
      "Epoch 1914/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2319 - class_loss: 0.0118 - l1_loss: 0.1220\n",
      "Epoch 1915/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8515 - class_loss: 0.0050 - l1_loss: 0.1847\n",
      "Epoch 1916/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6401 - class_loss: 0.0032 - l1_loss: 0.1637\n",
      "Epoch 1917/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5160 - class_loss: 0.0039 - l1_loss: 0.1512\n",
      "Epoch 1918/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2848 - class_loss: 0.0033 - l1_loss: 0.1281\n",
      "Epoch 1919/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5264 - class_loss: 0.0052 - l1_loss: 0.1521\n",
      "Epoch 1920/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6275 - class_loss: 0.0030 - l1_loss: 0.1624\n",
      "Epoch 1921/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2129 - class_loss: 0.0019 - l1_loss: 0.1211\n",
      "Epoch 1922/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0966 - class_loss: 0.0024 - l1_loss: 0.1094\n",
      "Epoch 1923/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8959 - class_loss: 0.0040 - l1_loss: 0.0892\n",
      "Epoch 1924/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.4156 - class_loss: 0.0041 - l1_loss: 0.1411 0s - loss: 1.7724 - class_loss: 0.0021 - l1_loss: \n",
      "Epoch 1925/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0352 - class_loss: 0.0086 - l1_loss: 0.1027\n",
      "Epoch 1926/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8231 - class_loss: 0.0067 - l1_loss: 0.0816\n",
      "Epoch 1927/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8185 - class_loss: 0.0045 - l1_loss: 0.0814\n",
      "Epoch 1928/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9083 - class_loss: 0.0030 - l1_loss: 0.0905\n",
      "Epoch 1929/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1839 - class_loss: 0.0017 - l1_loss: 0.1182\n",
      "Epoch 1930/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.9516 - class_loss: 0.0048 - l1_loss: 0.0947\n",
      "Epoch 1931/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2696 - class_loss: 0.0062 - l1_loss: 0.1263\n",
      "Epoch 1932/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8756 - class_loss: 0.0029 - l1_loss: 0.0873\n",
      "Epoch 1933/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0382 - class_loss: 0.0028 - l1_loss: 0.1035\n",
      "Epoch 1934/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9477 - class_loss: 0.0038 - l1_loss: 0.0944\n",
      "Epoch 1935/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8230 - class_loss: 0.0051 - l1_loss: 0.0818\n",
      "Epoch 1936/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0230 - class_loss: 0.0025 - l1_loss: 0.1020 0s - loss: 1.2684 - class_loss: 0.0040 - l1_loss: 0.12 - ETA: 0s - loss: 1.0611 - class_loss: 0.0029 - l1_loss: 0.\n",
      "Epoch 1937/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2113 - class_loss: 0.0032 - l1_loss: 0.1208\n",
      "Epoch 1938/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.8372 - class_loss: 0.0026 - l1_loss: 0.0835\n",
      "Epoch 1939/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9101 - class_loss: 0.0047 - l1_loss: 0.0905\n",
      "Epoch 1940/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.7847 - class_loss: 0.0026 - l1_loss: 0.0782\n",
      "Epoch 1941/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.6676 - class_loss: 0.0031 - l1_loss: 0.0665\n",
      "Epoch 1942/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.6982 - class_loss: 0.0019 - l1_loss: 0.0696\n",
      "Epoch 1943/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.6235 - class_loss: 0.0055 - l1_loss: 0.0618\n",
      "Epoch 1944/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.6781 - class_loss: 0.0056 - l1_loss: 0.0673\n",
      "Epoch 1945/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7518 - class_loss: 0.0031 - l1_loss: 0.0749\n",
      "Epoch 1946/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7569 - class_loss: 0.0025 - l1_loss: 0.0754\n",
      "Epoch 1947/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.7911 - class_loss: 0.0031 - l1_loss: 0.0788 0s - loss: 0.8652 - class_loss: 0.0030 - l1_loss: \n",
      "Epoch 1948/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7097 - class_loss: 0.0040 - l1_loss: 0.0706\n",
      "Epoch 1949/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.5859 - class_loss: 0.0043 - l1_loss: 0.0582\n",
      "Epoch 1950/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.5574 - class_loss: 0.0025 - l1_loss: 0.0555\n",
      "Epoch 1951/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.4683 - class_loss: 0.0020 - l1_loss: 0.0466\n",
      "Epoch 1952/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.5064 - class_loss: 0.0025 - l1_loss: 0.0504\n",
      "Epoch 1953/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 28ms/step - loss: 0.5223 - class_loss: 0.0036 - l1_loss: 0.0519\n",
      "Epoch 1954/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.6606 - class_loss: 0.0027 - l1_loss: 0.0658 0s - loss: 0.6829 - class_loss: 0.0040 - l1_loss: \n",
      "Epoch 1955/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.5948 - class_loss: 0.0032 - l1_loss: 0.0592\n",
      "Epoch 1956/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.5757 - class_loss: 0.0036 - l1_loss: 0.0572\n",
      "Epoch 1957/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.4558 - class_loss: 0.0035 - l1_loss: 0.0452\n",
      "Epoch 1958/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.7755 - class_loss: 0.0032 - l1_loss: 0.0772\n",
      "Epoch 1959/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.5704 - class_loss: 0.0036 - l1_loss: 0.0567 0s - loss: 0.5756 - class_loss: 0.0048 - l1_loss: 0.\n",
      "Epoch 1960/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.5350 - class_loss: 0.0038 - l1_loss: 0.0531\n",
      "Epoch 1961/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.5549 - class_loss: 0.0024 - l1_loss: 0.0552\n",
      "Epoch 1962/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.6200 - class_loss: 0.0024 - l1_loss: 0.0618\n",
      "Epoch 1963/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.5505 - class_loss: 0.0024 - l1_loss: 0.0548\n",
      "Epoch 1964/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.6886 - class_loss: 0.0079 - l1_loss: 0.0681\n",
      "Epoch 1965/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.6999 - class_loss: 0.0047 - l1_loss: 0.0695\n",
      "Epoch 1966/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7478 - class_loss: 0.0035 - l1_loss: 0.0744\n",
      "Epoch 1967/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8168 - class_loss: 0.0031 - l1_loss: 0.0814\n",
      "Epoch 1968/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8232 - class_loss: 0.0073 - l1_loss: 0.0816\n",
      "Epoch 1969/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0953 - class_loss: 0.0054 - l1_loss: 0.1090\n",
      "Epoch 1970/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8028 - class_loss: 0.0031 - l1_loss: 0.0800\n",
      "Epoch 1971/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8599 - class_loss: 0.0027 - l1_loss: 0.0857\n",
      "Epoch 1972/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0133 - class_loss: 0.0024 - l1_loss: 0.1011\n",
      "Epoch 1973/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.7190 - class_loss: 0.0028 - l1_loss: 0.0716\n",
      "Epoch 1974/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.8098 - class_loss: 0.0040 - l1_loss: 0.0806\n",
      "Epoch 1975/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.6722 - class_loss: 0.0028 - l1_loss: 0.0669 0s - loss: 0.5272 - class_loss: 0.0038 - l1_loss: \n",
      "Epoch 1976/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.7648 - class_loss: 0.0036 - l1_loss: 0.0761\n",
      "Epoch 1977/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8611 - class_loss: 0.0034 - l1_loss: 0.0858\n",
      "Epoch 1978/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8379 - class_loss: 0.0020 - l1_loss: 0.0836\n",
      "Epoch 1979/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9123 - class_loss: 0.0020 - l1_loss: 0.0910\n",
      "Epoch 1980/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1932 - class_loss: 0.0058 - l1_loss: 0.1187\n",
      "Epoch 1981/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.8914 - class_loss: 0.0029 - l1_loss: 0.0888\n",
      "Epoch 1982/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0389 - class_loss: 0.0067 - l1_loss: 0.1032\n",
      "Epoch 1983/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3452 - class_loss: 0.0086 - l1_loss: 0.1337\n",
      "Epoch 1984/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4036 - class_loss: 0.0019 - l1_loss: 0.1402\n",
      "Epoch 1985/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5498 - class_loss: 0.0012 - l1_loss: 0.1549\n",
      "Epoch 1986/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7039 - class_loss: 0.0023 - l1_loss: 0.1702\n",
      "Epoch 1987/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6205 - class_loss: 0.0036 - l1_loss: 0.1617\n",
      "Epoch 1988/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6854 - class_loss: 0.0016 - l1_loss: 0.1684\n",
      "Epoch 1989/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9727 - class_loss: 0.0023 - l1_loss: 0.1970\n",
      "Epoch 1990/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4491 - class_loss: 0.0037 - l1_loss: 0.2445\n",
      "Epoch 1991/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0369 - class_loss: 0.0055 - l1_loss: 0.3031\n",
      "Epoch 1992/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.9757 - class_loss: 0.0031 - l1_loss: 0.3973\n",
      "Epoch 1993/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6588 - class_loss: 0.0033 - l1_loss: 0.3656\n",
      "Epoch 1994/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.3399 - class_loss: 0.0024 - l1_loss: 0.6338\n",
      "Epoch 1995/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.0731 - class_loss: 0.0024 - l1_loss: 0.6071\n",
      "Epoch 1996/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.0316 - class_loss: 0.0025 - l1_loss: 0.5029\n",
      "Epoch 1997/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.7643 - class_loss: 0.0178 - l1_loss: 0.4746\n",
      "Epoch 1998/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9820 - class_loss: 0.0023 - l1_loss: 0.2980\n",
      "Epoch 1999/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 3.3907 - class_loss: 0.0030 - l1_loss: 0.3388\n",
      "Epoch 2000/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.7931 - class_loss: 0.0170 - l1_loss: 0.3776\n",
      "Epoch 2001/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.1625 - class_loss: 0.0041 - l1_loss: 0.3158\n",
      "Epoch 2002/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.0820 - class_loss: 0.0026 - l1_loss: 0.3079\n",
      "Epoch 2003/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.6156 - class_loss: 0.0028 - l1_loss: 0.4613\n",
      "Epoch 2004/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.8949 - class_loss: 0.0133 - l1_loss: 0.4882\n",
      "Epoch 2005/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0949 - class_loss: 0.0024 - l1_loss: 0.3093\n",
      "Epoch 2006/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.5055 - class_loss: 0.0061 - l1_loss: 0.6499\n",
      "Epoch 2007/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.1292 - class_loss: 0.0025 - l1_loss: 1.0127\n",
      "Epoch 2008/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.0257 - class_loss: 0.0021 - l1_loss: 1.1024\n",
      "Epoch 2009/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 13.9967 - class_loss: 0.0035 - l1_loss: 1.3993\n",
      "Epoch 2010/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 15.8488 - class_loss: 0.0026 - l1_loss: 1.5846\n",
      "Epoch 2011/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 14.7909 - class_loss: 0.0387 - l1_loss: 1.4752\n",
      "Epoch 2012/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 21.9858 - class_loss: 0.0022 - l1_loss: 2.1984\n",
      "Epoch 2013/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 15.9942 - class_loss: 0.0027 - l1_loss: 1.5991\n",
      "Epoch 2014/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 14.5279 - class_loss: 0.0039 - l1_loss: 1.4524\n",
      "Epoch 2015/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 11.0602 - class_loss: 0.0054 - l1_loss: 1.1055\n",
      "Epoch 2016/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 11.5960 - class_loss: 0.0017 - l1_loss: 1.1594\n",
      "Epoch 2017/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.9629 - class_loss: 0.0015 - l1_loss: 0.9961\n",
      "Epoch 2018/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.4661 - class_loss: 0.2214 - l1_loss: 1.0245\n",
      "Epoch 2019/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 11.0891 - class_loss: 0.0052 - l1_loss: 1.1084\n",
      "Epoch 2020/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 14.0989 - class_loss: 0.0360 - l1_loss: 1.4063\n",
      "Epoch 2021/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 10.9557 - class_loss: 0.0106 - l1_loss: 1.0945\n",
      "Epoch 2022/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 10.4537 - class_loss: 6.5271e-04 - l1_loss: 1.0453\n",
      "Epoch 2023/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 11.7007 - class_loss: 4.6261e-04 - l1_loss: 1.1700\n",
      "Epoch 2024/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 10.6513 - class_loss: 0.0025 - l1_loss: 1.0649\n",
      "Epoch 2025/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 13.7674 - class_loss: 0.0887 - l1_loss: 1.3679\n",
      "Epoch 2026/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 17.6030 - class_loss: 0.0039 - l1_loss: 1.7599\n",
      "Epoch 2027/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 16.6995 - class_loss: 0.0356 - l1_loss: 1.6664\n",
      "Epoch 2028/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 11.9268 - class_loss: 0.0034 - l1_loss: 1.1923\n",
      "Epoch 2029/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 12.2066 - class_loss: 8.6003e-04 - l1_loss: 1.2206\n",
      "Epoch 2030/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 9.1561 - class_loss: 0.0019 - l1_loss: 0.9154\n",
      "Epoch 2031/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 13.8484 - class_loss: 0.0895 - l1_loss: 1.3759\n",
      "Epoch 2032/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 12.8986 - class_loss: 2.6229e-04 - l1_loss: 1.2898\n",
      "Epoch 2033/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 8.7337 - class_loss: 6.4429e-04 - l1_loss: 0.8733\n",
      "Epoch 2034/5000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 7.2043 - class_loss: 0.0035 - l1_loss: 0.7201\n",
      "Epoch 2035/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 9.5100 - class_loss: 0.0369 - l1_loss: 0.9473\n",
      "Epoch 2036/5000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 7.3425 - class_loss: 0.0011 - l1_loss: 0.7341\n",
      "Epoch 2037/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 7.8476 - class_loss: 0.0202 - l1_loss: 0.7827\n",
      "Epoch 2038/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.3437 - class_loss: 0.0110 - l1_loss: 0.6333\n",
      "Epoch 2039/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 8.7474 - class_loss: 0.0030 - l1_loss: 0.8744\n",
      "Epoch 2040/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 11.4434 - class_loss: 0.0131 - l1_loss: 1.1430\n",
      "Epoch 2041/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.0693 - class_loss: 7.3818e-04 - l1_loss: 1.1069\n",
      "Epoch 2042/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 14.0350 - class_loss: 3.3990e-04 - l1_loss: 1.4035\n",
      "Epoch 2043/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.9850 - class_loss: 0.0112 - l1_loss: 1.3974\n",
      "Epoch 2044/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 22.7941 - class_loss: 0.0082 - l1_loss: 2.2786\n",
      "Epoch 2045/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 21.2181 - class_loss: 0.0014 - l1_loss: 2.1217\n",
      "Epoch 2046/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 20.0455 - class_loss: 0.0074 - l1_loss: 2.0038\n",
      "Epoch 2047/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 25.4178 - class_loss: 0.0031 - l1_loss: 2.5415\n",
      "Epoch 2048/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 31.6676 - class_loss: 0.0098 - l1_loss: 3.1658\n",
      "Epoch 2049/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.9976 - class_loss: 0.0799 - l1_loss: 1.8918\n",
      "Epoch 2050/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 17.3683 - class_loss: 0.0013 - l1_loss: 1.7367\n",
      "Epoch 2051/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 22.4735 - class_loss: 1.3845e-04 - l1_loss: 2.2473\n",
      "Epoch 2052/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.4312 - class_loss: 6.1616e-04 - l1_loss: 1.3431\n",
      "Epoch 2053/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 11.1096 - class_loss: 0.0010 - l1_loss: 1.1109\n",
      "Epoch 2054/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 13.5923 - class_loss: 0.0012 - l1_loss: 1.3591\n",
      "Epoch 2055/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.5656 - class_loss: 0.0030 - l1_loss: 1.2563\n",
      "Epoch 2056/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.6689 - class_loss: 0.0020 - l1_loss: 0.9667\n",
      "Epoch 2057/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.5479 - class_loss: 0.0039 - l1_loss: 0.7544\n",
      "Epoch 2058/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.5205 - class_loss: 0.0026 - l1_loss: 0.9518\n",
      "Epoch 2059/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.4926 - class_loss: 0.0105 - l1_loss: 0.4482\n",
      "Epoch 2060/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.2864 - class_loss: 0.0044 - l1_loss: 0.7282\n",
      "Epoch 2061/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.5580 - class_loss: 0.0121 - l1_loss: 0.5546\n",
      "Epoch 2062/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.3024 - class_loss: 0.0035 - l1_loss: 0.4299\n",
      "Epoch 2063/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.6722 - class_loss: 5.3292e-04 - l1_loss: 0.5672\n",
      "Epoch 2064/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6276 - class_loss: 0.0016 - l1_loss: 0.3626\n",
      "Epoch 2065/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.1796 - class_loss: 0.0066 - l1_loss: 0.4173\n",
      "Epoch 2066/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.1311 - class_loss: 0.0017 - l1_loss: 0.4129\n",
      "Epoch 2067/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.7174 - class_loss: 0.0020 - l1_loss: 0.3715\n",
      "Epoch 2068/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.0883 - class_loss: 0.0033 - l1_loss: 0.4085\n",
      "Epoch 2069/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0840 - class_loss: 0.0018 - l1_loss: 0.3082\n",
      "Epoch 2070/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2732 - class_loss: 0.0103 - l1_loss: 0.2263\n",
      "Epoch 2071/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.0299 - class_loss: 0.0118 - l1_loss: 0.4018\n",
      "Epoch 2072/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.4144 - class_loss: 0.0060 - l1_loss: 0.4408\n",
      "Epoch 2073/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6661 - class_loss: 0.0072 - l1_loss: 0.2659\n",
      "Epoch 2074/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5717 - class_loss: 0.0104 - l1_loss: 0.3561\n",
      "Epoch 2075/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2957 - class_loss: 0.0121 - l1_loss: 0.3284\n",
      "Epoch 2076/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5955 - class_loss: 0.0062 - l1_loss: 0.2589\n",
      "Epoch 2077/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.3845 - class_loss: 0.0026 - l1_loss: 0.3382\n",
      "Epoch 2078/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7248 - class_loss: 0.0027 - l1_loss: 0.2722\n",
      "Epoch 2079/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1030 - class_loss: 0.0020 - l1_loss: 0.3101\n",
      "Epoch 2080/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.1890 - class_loss: 0.0034 - l1_loss: 0.3186\n",
      "Epoch 2081/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.3992 - class_loss: 0.0018 - l1_loss: 0.2397\n",
      "Epoch 2082/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2357 - class_loss: 5.4041e-04 - l1_loss: 0.3235\n",
      "Epoch 2083/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2915 - class_loss: 0.0014 - l1_loss: 0.2290\n",
      "Epoch 2084/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3408 - class_loss: 0.0070 - l1_loss: 0.3334\n",
      "Epoch 2085/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2050 - class_loss: 0.0015 - l1_loss: 0.2204\n",
      "Epoch 2086/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1367 - class_loss: 8.1635e-04 - l1_loss: 0.2136\n",
      "Epoch 2087/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4820 - class_loss: 0.0092 - l1_loss: 0.1473\n",
      "Epoch 2088/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6685 - class_loss: 0.0128 - l1_loss: 0.1656\n",
      "Epoch 2089/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5118 - class_loss: 0.0045 - l1_loss: 0.1507\n",
      "Epoch 2090/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3692 - class_loss: 0.0036 - l1_loss: 0.1366\n",
      "Epoch 2091/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6452 - class_loss: 0.0073 - l1_loss: 0.1638\n",
      "Epoch 2092/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1584 - class_loss: 0.0077 - l1_loss: 0.1151\n",
      "Epoch 2093/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3427 - class_loss: 0.0072 - l1_loss: 0.1335\n",
      "Epoch 2094/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1255 - class_loss: 0.0020 - l1_loss: 0.1123\n",
      "Epoch 2095/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.3539 - class_loss: 0.0023 - l1_loss: 0.1352\n",
      "Epoch 2096/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.9831 - class_loss: 0.0057 - l1_loss: 0.0977\n",
      "Epoch 2097/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.1764 - class_loss: 0.0039 - l1_loss: 0.1172\n",
      "Epoch 2098/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1928 - class_loss: 0.0027 - l1_loss: 0.1190\n",
      "Epoch 2099/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9809 - class_loss: 0.0022 - l1_loss: 0.0979\n",
      "Epoch 2100/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.2457 - class_loss: 0.0033 - l1_loss: 0.1242\n",
      "Epoch 2101/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4376 - class_loss: 0.0022 - l1_loss: 0.1435\n",
      "Epoch 2102/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.3952 - class_loss: 0.0025 - l1_loss: 0.1393\n",
      "Epoch 2103/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3723 - class_loss: 0.0076 - l1_loss: 0.1365\n",
      "Epoch 2104/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4026 - class_loss: 0.0109 - l1_loss: 0.1392\n",
      "Epoch 2105/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9088 - class_loss: 0.0023 - l1_loss: 0.1906\n",
      "Epoch 2106/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1685 - class_loss: 0.0035 - l1_loss: 0.3165\n",
      "Epoch 2107/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.4698 - class_loss: 0.0027 - l1_loss: 0.3467\n",
      "Epoch 2108/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.2632 - class_loss: 0.0060 - l1_loss: 0.4257\n",
      "Epoch 2109/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.9126 - class_loss: 0.0074 - l1_loss: 0.5905\n",
      "Epoch 2110/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.0282 - class_loss: 0.0138 - l1_loss: 1.0014\n",
      "Epoch 2111/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.4347 - class_loss: 0.2126 - l1_loss: 1.6222\n",
      "Epoch 2112/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 8.3766 - class_loss: 0.0718 - l1_loss: 0.8305\n",
      "Epoch 2113/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.7810 - class_loss: 0.0244 - l1_loss: 0.6757\n",
      "Epoch 2114/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.3375 - class_loss: 3.4592e-04 - l1_loss: 0.4337\n",
      "Epoch 2115/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.0243 - class_loss: 0.0016 - l1_loss: 0.5023\n",
      "Epoch 2116/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 5.2763 - class_loss: 5.5235e-04 - l1_loss: 0.5276\n",
      "Epoch 2117/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.2714 - class_loss: 0.0012 - l1_loss: 0.6270\n",
      "Epoch 2118/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 3.3259 - class_loss: 5.2441e-04 - l1_loss: 0.3325\n",
      "Epoch 2119/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 3.4892 - class_loss: 0.0040 - l1_loss: 0.3485\n",
      "Epoch 2120/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 4.2296 - class_loss: 0.0119 - l1_loss: 0.4218\n",
      "Epoch 2121/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.5584 - class_loss: 0.0055 - l1_loss: 0.3553\n",
      "Epoch 2122/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.1142 - class_loss: 0.0020 - l1_loss: 0.3112\n",
      "Epoch 2123/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.5805 - class_loss: 0.0066 - l1_loss: 0.2574\n",
      "Epoch 2124/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9369 - class_loss: 0.0042 - l1_loss: 0.2933\n",
      "Epoch 2125/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0091 - class_loss: 0.0048 - l1_loss: 0.3004\n",
      "Epoch 2126/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.3273 - class_loss: 0.0039 - l1_loss: 0.2323\n",
      "Epoch 2127/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.5336 - class_loss: 0.0042 - l1_loss: 0.4529\n",
      "Epoch 2128/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.4476 - class_loss: 0.0029 - l1_loss: 0.2445\n",
      "Epoch 2129/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.3047 - class_loss: 0.0043 - l1_loss: 0.3300\n",
      "Epoch 2130/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.1412 - class_loss: 0.0184 - l1_loss: 0.4123\n",
      "Epoch 2131/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 3.3377 - class_loss: 0.0037 - l1_loss: 0.3334\n",
      "Epoch 2132/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.0733 - class_loss: 0.0019 - l1_loss: 0.4071\n",
      "Epoch 2133/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 4.4730 - class_loss: 0.0073 - l1_loss: 0.4466\n",
      "Epoch 2134/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.6714 - class_loss: 0.0063 - l1_loss: 0.3665\n",
      "Epoch 2135/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.1139 - class_loss: 0.0052 - l1_loss: 0.5109\n",
      "Epoch 2136/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.0676 - class_loss: 0.0051 - l1_loss: 0.4063\n",
      "Epoch 2137/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 3.7445 - class_loss: 0.0020 - l1_loss: 0.3743\n",
      "Epoch 2138/5000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 4.2309 - class_loss: 0.0086 - l1_loss: 0.4222\n",
      "Epoch 2139/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 2.7364 - class_loss: 0.0084 - l1_loss: 0.2728\n",
      "Epoch 2140/5000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 3.2369 - class_loss: 0.0054 - l1_loss: 0.3231\n",
      "Epoch 2141/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.7777 - class_loss: 0.0036 - l1_loss: 0.1774\n",
      "Epoch 2142/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.1640 - class_loss: 0.0057 - l1_loss: 0.2158\n",
      "Epoch 2143/5000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.5605 - class_loss: 0.0029 - l1_loss: 0.1558\n",
      "Epoch 2144/5000\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.6613 - class_loss: 0.0039 - l1_loss: 0.1657\n",
      "Epoch 2145/5000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.9889 - class_loss: 0.0039 - l1_loss: 0.1985\n",
      "Epoch 2146/5000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.4930 - class_loss: 0.0019 - l1_loss: 0.1491\n",
      "Epoch 2147/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.9259 - class_loss: 0.0012 - l1_loss: 0.1925\n",
      "Epoch 2148/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 2.0076 - class_loss: 0.0014 - l1_loss: 0.2006\n",
      "Epoch 2149/5000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.6726 - class_loss: 0.0020 - l1_loss: 0.1671\n",
      "Epoch 2150/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.2808 - class_loss: 0.0086 - l1_loss: 0.1272\n",
      "Epoch 2151/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.0331 - class_loss: 0.0109 - l1_loss: 0.2022\n",
      "Epoch 2152/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.9952 - class_loss: 0.0049 - l1_loss: 0.1990\n",
      "Epoch 2153/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.9193 - class_loss: 0.0047 - l1_loss: 0.1915\n",
      "Epoch 2154/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.1006 - class_loss: 0.0127 - l1_loss: 0.2088\n",
      "Epoch 2155/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.7543 - class_loss: 0.0105 - l1_loss: 0.1744\n",
      "Epoch 2156/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.1838 - class_loss: 0.0044 - l1_loss: 0.2179\n",
      "Epoch 2157/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.5538 - class_loss: 0.0043 - l1_loss: 0.2549\n",
      "Epoch 2158/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.6221 - class_loss: 0.0062 - l1_loss: 0.2616\n",
      "Epoch 2159/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.0836 - class_loss: 0.0119 - l1_loss: 0.2072\n",
      "Epoch 2160/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.9238 - class_loss: 0.0026 - l1_loss: 0.1921\n",
      "Epoch 2161/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.8566 - class_loss: 0.0011 - l1_loss: 0.1856\n",
      "Epoch 2162/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.1171 - class_loss: 0.0028 - l1_loss: 0.2114\n",
      "Epoch 2163/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.1000 - class_loss: 0.0015 - l1_loss: 0.3099\n",
      "Epoch 2164/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 2.8729 - class_loss: 4.2087e-04 - l1_loss: 0.2872\n",
      "Epoch 2165/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.2924 - class_loss: 0.0077 - l1_loss: 0.3285\n",
      "Epoch 2166/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 6.5253 - class_loss: 0.0032 - l1_loss: 0.6522\n",
      "Epoch 2167/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 5.5802 - class_loss: 0.0026 - l1_loss: 0.5578\n",
      "Epoch 2168/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 5.3422 - class_loss: 7.6567e-04 - l1_loss: 0.5341\n",
      "Epoch 2169/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.1808 - class_loss: 0.0040 - l1_loss: 0.4177\n",
      "Epoch 2170/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 5.3881 - class_loss: 0.0042 - l1_loss: 0.5384\n",
      "Epoch 2171/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 4.2780 - class_loss: 0.0094 - l1_loss: 0.4269\n",
      "Epoch 2172/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 4.1646 - class_loss: 0.0030 - l1_loss: 0.4162\n",
      "Epoch 2173/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.0139 - class_loss: 0.0074 - l1_loss: 0.5007\n",
      "Epoch 2174/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.1397 - class_loss: 0.0173 - l1_loss: 0.5122\n",
      "Epoch 2175/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 6.6689 - class_loss: 0.0013 - l1_loss: 0.6668\n",
      "Epoch 2176/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 7.6479 - class_loss: 9.6449e-04 - l1_loss: 0.7647\n",
      "Epoch 2177/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 17.3544 - class_loss: 0.0251 - l1_loss: 1.7329\n",
      "Epoch 2178/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 20.0024 - class_loss: 6.6375e-04 - l1_loss: 2.0002\n",
      "Epoch 2179/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 28.7521 - class_loss: 5.5821e-04 - l1_loss: 2.8752\n",
      "Epoch 2180/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 25.4063 - class_loss: 0.0012 - l1_loss: 2.5405\n",
      "Epoch 2181/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 14.2588 - class_loss: 1.5487e-04 - l1_loss: 1.4259\n",
      "Epoch 2182/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 14.1357 - class_loss: 5.5634e-04 - l1_loss: 1.4135\n",
      "Epoch 2183/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 14.3594 - class_loss: 0.0030 - l1_loss: 1.4356\n",
      "Epoch 2184/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 15.2219 - class_loss: 9.0772e-04 - l1_loss: 1.5221\n",
      "Epoch 2185/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 15.7294 - class_loss: 0.0099 - l1_loss: 1.5720\n",
      "Epoch 2186/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 16.1077 - class_loss: 0.0185 - l1_loss: 1.6089\n",
      "Epoch 2187/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.4491 - class_loss: 0.0044 - l1_loss: 1.1445\n",
      "Epoch 2188/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 15.3200 - class_loss: 0.0163 - l1_loss: 1.5304\n",
      "Epoch 2189/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 18.1978 - class_loss: 0.0021 - l1_loss: 1.8196\n",
      "Epoch 2190/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.4735 - class_loss: 0.0036 - l1_loss: 1.4470\n",
      "Epoch 2191/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 13.1237 - class_loss: 0.0056 - l1_loss: 1.3118\n",
      "Epoch 2192/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 15.7598 - class_loss: 0.0133 - l1_loss: 1.5746\n",
      "Epoch 2193/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 14.9275 - class_loss: 0.0109 - l1_loss: 1.4917\n",
      "Epoch 2194/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.7458 - class_loss: 0.0507 - l1_loss: 1.5695\n",
      "Epoch 2195/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.8926 - class_loss: 0.0054 - l1_loss: 1.6887\n",
      "Epoch 2196/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 15.9959 - class_loss: 3.6405e-04 - l1_loss: 1.5996\n",
      "Epoch 2197/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 17.3050 - class_loss: 0.0028 - l1_loss: 1.7302\n",
      "Epoch 2198/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.5020 - class_loss: 0.0080 - l1_loss: 1.3494\n",
      "Epoch 2199/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 19.7486 - class_loss: 0.7709 - l1_loss: 1.8978\n",
      "Epoch 2200/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 31.6628 - class_loss: 0.6261 - l1_loss: 3.1037\n",
      "Epoch 2201/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 39.7229 - class_loss: 0.4665 - l1_loss: 3.9256\n",
      "Epoch 2202/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 31.5423 - class_loss: 0.1368 - l1_loss: 3.1406\n",
      "Epoch 2203/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 17.6125 - class_loss: 0.0822 - l1_loss: 1.7530\n",
      "Epoch 2204/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.7425 - class_loss: 0.0094 - l1_loss: 1.4733\n",
      "Epoch 2205/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 16.1299 - class_loss: 0.5991 - l1_loss: 1.5531\n",
      "Epoch 2206/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 22.8063 - class_loss: 0.3730 - l1_loss: 2.2433\n",
      "Epoch 2207/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 25.5644 - class_loss: 0.3198 - l1_loss: 2.5245\n",
      "Epoch 2208/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 29.5629 - class_loss: 0.0663 - l1_loss: 2.9497\n",
      "Epoch 2209/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 25.8643 - class_loss: 0.0373 - l1_loss: 2.5827\n",
      "Epoch 2210/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 29.2058 - class_loss: 0.1138 - l1_loss: 2.9092\n",
      "Epoch 2211/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 27.9517 - class_loss: 0.0860 - l1_loss: 2.7866\n",
      "Epoch 2212/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 24.9451 - class_loss: 0.0036 - l1_loss: 2.4941\n",
      "Epoch 2213/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 16.3356 - class_loss: 0.0211 - l1_loss: 1.6315\n",
      "Epoch 2214/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.1994 - class_loss: 0.0259 - l1_loss: 1.7173\n",
      "Epoch 2215/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 17.4243 - class_loss: 0.0076 - l1_loss: 1.7417\n",
      "Epoch 2216/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.0177 - class_loss: 0.0100 - l1_loss: 1.5008\n",
      "Epoch 2217/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 30ms/step - loss: 17.0319 - class_loss: 0.0089 - l1_loss: 1.7023\n",
      "Epoch 2218/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.4373 - class_loss: 0.0090 - l1_loss: 1.2428\n",
      "Epoch 2219/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.6837 - class_loss: 0.0048 - l1_loss: 1.2679\n",
      "Epoch 2220/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.5494 - class_loss: 0.0027 - l1_loss: 1.3547\n",
      "Epoch 2221/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.5471 - class_loss: 0.0072 - l1_loss: 1.0540\n",
      "Epoch 2222/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.6582 - class_loss: 0.0067 - l1_loss: 1.2651\n",
      "Epoch 2223/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.4879 - class_loss: 0.0022 - l1_loss: 1.0486\n",
      "Epoch 2224/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.3026 - class_loss: 0.0050 - l1_loss: 1.1298\n",
      "Epoch 2225/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.7849 - class_loss: 0.0083 - l1_loss: 0.7777\n",
      "Epoch 2226/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.3325 - class_loss: 0.0041 - l1_loss: 0.7328\n",
      "Epoch 2227/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.1860 - class_loss: 0.0041 - l1_loss: 0.7182\n",
      "Epoch 2228/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.0915 - class_loss: 0.0018 - l1_loss: 1.0090\n",
      "Epoch 2229/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.3125 - class_loss: 0.0099 - l1_loss: 0.9303\n",
      "Epoch 2230/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.4964 - class_loss: 0.0056 - l1_loss: 0.8491\n",
      "Epoch 2231/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.9711 - class_loss: 0.0031 - l1_loss: 0.6968\n",
      "Epoch 2232/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.5182 - class_loss: 0.0065 - l1_loss: 0.5512\n",
      "Epoch 2233/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.6979 - class_loss: 0.0036 - l1_loss: 0.5694\n",
      "Epoch 2234/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.1401 - class_loss: 0.0021 - l1_loss: 1.0138\n",
      "Epoch 2235/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.2124 - class_loss: 0.0038 - l1_loss: 0.8209\n",
      "Epoch 2236/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.6986 - class_loss: 0.0028 - l1_loss: 0.6696\n",
      "Epoch 2237/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.3368 - class_loss: 0.0072 - l1_loss: 0.8330\n",
      "Epoch 2238/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.1443 - class_loss: 0.0025 - l1_loss: 0.6142\n",
      "Epoch 2239/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.3951 - class_loss: 0.0047 - l1_loss: 0.6390\n",
      "Epoch 2240/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.0303 - class_loss: 0.0090 - l1_loss: 0.6021\n",
      "Epoch 2241/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.3164 - class_loss: 0.0064 - l1_loss: 0.5310\n",
      "Epoch 2242/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.3387 - class_loss: 0.0025 - l1_loss: 0.4336\n",
      "Epoch 2243/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.4322 - class_loss: 0.0017 - l1_loss: 0.4431\n",
      "Epoch 2244/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9804 - class_loss: 0.0050 - l1_loss: 0.3975\n",
      "Epoch 2245/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7384 - class_loss: 0.0161 - l1_loss: 0.3722\n",
      "Epoch 2246/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.7368 - class_loss: 0.0023 - l1_loss: 0.3735\n",
      "Epoch 2247/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.5951 - class_loss: 0.0027 - l1_loss: 0.3592\n",
      "Epoch 2248/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0856 - class_loss: 0.0029 - l1_loss: 0.3083\n",
      "Epoch 2249/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.9222 - class_loss: 0.0031 - l1_loss: 0.3919\n",
      "Epoch 2250/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.2498 - class_loss: 0.0048 - l1_loss: 0.3245\n",
      "Epoch 2251/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.3890 - class_loss: 0.0039 - l1_loss: 0.3385\n",
      "Epoch 2252/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9253 - class_loss: 0.0027 - l1_loss: 0.2923\n",
      "Epoch 2253/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8155 - class_loss: 0.0038 - l1_loss: 0.1812 0s - loss: 1.8445 - class_loss: 0.0038 - l1_loss: 0.18\n",
      "Epoch 2254/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.1754 - class_loss: 0.0054 - l1_loss: 0.2170\n",
      "Epoch 2255/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.1641 - class_loss: 0.0040 - l1_loss: 0.2160\n",
      "Epoch 2256/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.0150 - class_loss: 0.0018 - l1_loss: 0.2013\n",
      "Epoch 2257/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.9140 - class_loss: 0.0019 - l1_loss: 0.1912\n",
      "Epoch 2258/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.9662 - class_loss: 0.0033 - l1_loss: 0.1963\n",
      "Epoch 2259/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5124 - class_loss: 0.0019 - l1_loss: 0.1510\n",
      "Epoch 2260/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6942 - class_loss: 8.7793e-04 - l1_loss: 0.1693\n",
      "Epoch 2261/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.7735 - class_loss: 0.0024 - l1_loss: 0.1771\n",
      "Epoch 2262/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.3709 - class_loss: 0.0069 - l1_loss: 0.2364\n",
      "Epoch 2263/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0633 - class_loss: 0.0030 - l1_loss: 0.2060\n",
      "Epoch 2264/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5849 - class_loss: 0.0027 - l1_loss: 0.2582\n",
      "Epoch 2265/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4588 - class_loss: 0.0068 - l1_loss: 0.3452\n",
      "Epoch 2266/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2823 - class_loss: 8.2876e-04 - l1_loss: 0.3281\n",
      "Epoch 2267/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1633 - class_loss: 0.0018 - l1_loss: 0.3161\n",
      "Epoch 2268/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.8608 - class_loss: 0.0030 - l1_loss: 0.3858\n",
      "Epoch 2269/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9733 - class_loss: 0.0016 - l1_loss: 0.2972\n",
      "Epoch 2270/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3096 - class_loss: 0.0036 - l1_loss: 0.3306\n",
      "Epoch 2271/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.1098 - class_loss: 0.0023 - l1_loss: 0.3108\n",
      "Epoch 2272/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.3944 - class_loss: 0.0039 - l1_loss: 0.2391 0s - loss: 2.6130 - class_loss: 0.0035 - l1_loss: 0.\n",
      "Epoch 2273/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.5903 - class_loss: 0.0101 - l1_loss: 0.3580\n",
      "Epoch 2274/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.7868 - class_loss: 0.0020 - l1_loss: 0.4785\n",
      "Epoch 2275/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.6796 - class_loss: 6.1586e-04 - l1_loss: 0.5679\n",
      "Epoch 2276/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.2358 - class_loss: 0.0010 - l1_loss: 0.4235\n",
      "Epoch 2277/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.4119 - class_loss: 6.9602e-04 - l1_loss: 0.5411\n",
      "Epoch 2278/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.9713 - class_loss: 0.0011 - l1_loss: 0.5970\n",
      "Epoch 2279/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.0012 - class_loss: 0.0039 - l1_loss: 0.5997\n",
      "Epoch 2280/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.0480 - class_loss: 0.0073 - l1_loss: 0.5041\n",
      "Epoch 2281/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.4499 - class_loss: 0.0052 - l1_loss: 0.4445\n",
      "Epoch 2282/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0732 - class_loss: 0.0037 - l1_loss: 0.3070\n",
      "Epoch 2283/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0404 - class_loss: 0.0056 - l1_loss: 0.2035\n",
      "Epoch 2284/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1434 - class_loss: 0.0022 - l1_loss: 0.2141 0s - loss: 2.4708 - class_loss: 0.0027 - l1_loss: \n",
      "Epoch 2285/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2624 - class_loss: 0.0032 - l1_loss: 0.2259\n",
      "Epoch 2286/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1615 - class_loss: 0.0029 - l1_loss: 0.2159\n",
      "Epoch 2287/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0728 - class_loss: 0.0013 - l1_loss: 0.2071\n",
      "Epoch 2288/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1093 - class_loss: 9.6159e-04 - l1_loss: 0.2108\n",
      "Epoch 2289/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.0427 - class_loss: 0.0012 - l1_loss: 0.4041\n",
      "Epoch 2290/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4256 - class_loss: 0.0055 - l1_loss: 0.3420\n",
      "Epoch 2291/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9976 - class_loss: 0.0092 - l1_loss: 0.2988\n",
      "Epoch 2292/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4913 - class_loss: 0.0093 - l1_loss: 0.2482\n",
      "Epoch 2293/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.3868 - class_loss: 0.0033 - l1_loss: 0.2383\n",
      "Epoch 2294/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9459 - class_loss: 0.0047 - l1_loss: 0.1941\n",
      "Epoch 2295/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2218 - class_loss: 0.0092 - l1_loss: 0.2213\n",
      "Epoch 2296/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0419 - class_loss: 0.0089 - l1_loss: 0.2033\n",
      "Epoch 2297/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0411 - class_loss: 0.0028 - l1_loss: 0.2038\n",
      "Epoch 2298/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.4283 - class_loss: 0.0029 - l1_loss: 0.2425\n",
      "Epoch 2299/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4810 - class_loss: 0.0019 - l1_loss: 0.2479\n",
      "Epoch 2300/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1212 - class_loss: 0.0023 - l1_loss: 0.2119\n",
      "Epoch 2301/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0994 - class_loss: 0.0033 - l1_loss: 0.2096\n",
      "Epoch 2302/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1787 - class_loss: 0.0044 - l1_loss: 0.2174\n",
      "Epoch 2303/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4605 - class_loss: 0.0029 - l1_loss: 0.2458\n",
      "Epoch 2304/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8310 - class_loss: 0.0024 - l1_loss: 0.2829\n",
      "Epoch 2305/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.5120 - class_loss: 0.0018 - l1_loss: 0.2510\n",
      "Epoch 2306/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.2383 - class_loss: 0.0029 - l1_loss: 0.2235\n",
      "Epoch 2307/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.0549 - class_loss: 0.0015 - l1_loss: 0.2053\n",
      "Epoch 2308/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2095 - class_loss: 0.0020 - l1_loss: 0.2208\n",
      "Epoch 2309/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8066 - class_loss: 0.0043 - l1_loss: 0.1802\n",
      "Epoch 2310/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7883 - class_loss: 0.0079 - l1_loss: 0.1780\n",
      "Epoch 2311/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.3191 - class_loss: 0.0054 - l1_loss: 0.2314\n",
      "Epoch 2312/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8131 - class_loss: 0.0042 - l1_loss: 0.1809\n",
      "Epoch 2313/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1430 - class_loss: 0.0022 - l1_loss: 0.2141\n",
      "Epoch 2314/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4705 - class_loss: 0.0020 - l1_loss: 0.2469\n",
      "Epoch 2315/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6552 - class_loss: 0.0050 - l1_loss: 0.2650\n",
      "Epoch 2316/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2452 - class_loss: 0.0121 - l1_loss: 0.3233\n",
      "Epoch 2317/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0574 - class_loss: 0.0012 - l1_loss: 0.2056\n",
      "Epoch 2318/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8079 - class_loss: 0.0016 - l1_loss: 0.1806\n",
      "Epoch 2319/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5688 - class_loss: 0.0043 - l1_loss: 0.1564\n",
      "Epoch 2320/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0531 - class_loss: 0.0019 - l1_loss: 0.2051\n",
      "Epoch 2321/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4798 - class_loss: 0.0049 - l1_loss: 0.1475\n",
      "Epoch 2322/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0776 - class_loss: 0.0071 - l1_loss: 0.2071\n",
      "Epoch 2323/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8424 - class_loss: 0.0016 - l1_loss: 0.1841\n",
      "Epoch 2324/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8943 - class_loss: 0.0028 - l1_loss: 0.1892\n",
      "Epoch 2325/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9531 - class_loss: 0.0023 - l1_loss: 0.1951\n",
      "Epoch 2326/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6249 - class_loss: 9.7037e-04 - l1_loss: 0.1624\n",
      "Epoch 2327/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5896 - class_loss: 0.0012 - l1_loss: 0.1588ETA: 0s - loss: 1.6108 - class_loss: 0.0014 - l1_loss: 0.\n",
      "Epoch 2328/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.0401 - class_loss: 0.0033 - l1_loss: 0.2037 0s - loss: 1.9800 - class_loss: 0.0016 - l1_loss: \n",
      "Epoch 2329/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9824 - class_loss: 0.0151 - l1_loss: 0.2967\n",
      "Epoch 2330/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.2503 - class_loss: 0.0027 - l1_loss: 0.2248\n",
      "Epoch 2331/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.5420 - class_loss: 0.0018 - l1_loss: 0.1540\n",
      "Epoch 2332/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2224 - class_loss: 0.0052 - l1_loss: 0.2217\n",
      "Epoch 2333/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8012 - class_loss: 0.0018 - l1_loss: 0.1799\n",
      "Epoch 2334/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6794 - class_loss: 0.0011 - l1_loss: 0.1678\n",
      "Epoch 2335/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9582 - class_loss: 0.0015 - l1_loss: 0.1957\n",
      "Epoch 2336/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8137 - class_loss: 0.0015 - l1_loss: 0.1812\n",
      "Epoch 2337/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6068 - class_loss: 7.0170e-04 - l1_loss: 0.1606\n",
      "Epoch 2338/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7554 - class_loss: 0.0025 - l1_loss: 0.1753\n",
      "Epoch 2339/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6022 - class_loss: 0.0094 - l1_loss: 0.1593\n",
      "Epoch 2340/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0129 - class_loss: 0.0015 - l1_loss: 0.2011\n",
      "Epoch 2341/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.6821 - class_loss: 0.0040 - l1_loss: 0.1678ETA: 0s - loss: 1.5992 - class_loss: 0.0034 - l1_loss: 0.\n",
      "Epoch 2342/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.0846 - class_loss: 0.0037 - l1_loss: 0.2081 0s - loss: 2.2585 - class_loss: 0.0040 - l1_loss: 0.\n",
      "Epoch 2343/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.5567 - class_loss: 0.0071 - l1_loss: 0.2550\n",
      "Epoch 2344/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.4941 - class_loss: 0.0036 - l1_loss: 0.2490\n",
      "Epoch 2345/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8137 - class_loss: 0.0024 - l1_loss: 0.2811\n",
      "Epoch 2346/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6442 - class_loss: 0.0108 - l1_loss: 0.3633\n",
      "Epoch 2347/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.2003 - class_loss: 0.0267 - l1_loss: 0.9174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2348/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.3694 - class_loss: 0.0014 - l1_loss: 1.2368\n",
      "Epoch 2349/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.4615 - class_loss: 0.0027 - l1_loss: 0.7459\n",
      "Epoch 2350/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.4275 - class_loss: 0.0024 - l1_loss: 0.8425ETA: 0s - loss: 7.6841 - class_loss: 0.0050 - l1_loss: 0.76\n",
      "Epoch 2351/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.3987 - class_loss: 0.0024 - l1_loss: 1.0396\n",
      "Epoch 2352/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 17.3644 - class_loss: 0.3862 - l1_loss: 1.6978\n",
      "Epoch 2353/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 19.0358 - class_loss: 0.2835 - l1_loss: 1.8752\n",
      "Epoch 2354/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 26.2030 - class_loss: 0.2445 - l1_loss: 2.5958\n",
      "Epoch 2355/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 28.2466 - class_loss: 0.0262 - l1_loss: 2.8220\n",
      "Epoch 2356/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.0047 - class_loss: 0.0976 - l1_loss: 1.4907\n",
      "Epoch 2357/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 18.7511 - class_loss: 0.0216 - l1_loss: 1.8729\n",
      "Epoch 2358/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 14.7966 - class_loss: 0.0141 - l1_loss: 1.4783\n",
      "Epoch 2359/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.4390 - class_loss: 0.0077 - l1_loss: 1.3431\n",
      "Epoch 2360/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 17.2547 - class_loss: 0.0111 - l1_loss: 1.7244\n",
      "Epoch 2361/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.3851 - class_loss: 0.0092 - l1_loss: 1.4376\n",
      "Epoch 2362/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.0812 - class_loss: 0.0067 - l1_loss: 1.4075\n",
      "Epoch 2363/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.5213 - class_loss: 0.0052 - l1_loss: 1.3516\n",
      "Epoch 2364/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.7899 - class_loss: 0.1113 - l1_loss: 1.8679\n",
      "Epoch 2365/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 10.9983 - class_loss: 0.0020 - l1_loss: 1.0996\n",
      "Epoch 2366/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.6005 - class_loss: 0.0014 - l1_loss: 1.2599\n",
      "Epoch 2367/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.3090 - class_loss: 0.0012 - l1_loss: 0.9308\n",
      "Epoch 2368/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.3031 - class_loss: 0.0041 - l1_loss: 1.1299\n",
      "Epoch 2369/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.8292 - class_loss: 0.0294 - l1_loss: 1.0800\n",
      "Epoch 2370/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.3814 - class_loss: 0.0129 - l1_loss: 1.4369\n",
      "Epoch 2371/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.4700 - class_loss: 0.0174 - l1_loss: 1.4453\n",
      "Epoch 2372/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.1115 - class_loss: 0.0432 - l1_loss: 1.4068\n",
      "Epoch 2373/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.6784 - class_loss: 0.0024 - l1_loss: 1.4676\n",
      "Epoch 2374/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 17.2480 - class_loss: 7.8918e-04 - l1_loss: 1.7247\n",
      "Epoch 2375/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.8462 - class_loss: 0.0026 - l1_loss: 1.3844\n",
      "Epoch 2376/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.9720 - class_loss: 0.0035 - l1_loss: 1.2968\n",
      "Epoch 2377/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 11.4218 - class_loss: 0.0022 - l1_loss: 1.1420\n",
      "Epoch 2378/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.6077 - class_loss: 0.0047 - l1_loss: 1.0603\n",
      "Epoch 2379/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.4530 - class_loss: 0.0168 - l1_loss: 0.8436\n",
      "Epoch 2380/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 11.9621 - class_loss: 0.0197 - l1_loss: 1.1942\n",
      "Epoch 2381/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 11.5933 - class_loss: 3.6821e-04 - l1_loss: 1.1593\n",
      "Epoch 2382/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.6286 - class_loss: 4.3383e-04 - l1_loss: 0.9628\n",
      "Epoch 2383/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.3177 - class_loss: 6.2840e-04 - l1_loss: 1.0317\n",
      "Epoch 2384/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.8290 - class_loss: 4.1851e-04 - l1_loss: 0.9829\n",
      "Epoch 2385/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.2505 - class_loss: 7.0376e-04 - l1_loss: 0.9250\n",
      "Epoch 2386/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.7814 - class_loss: 9.1938e-04 - l1_loss: 0.8780\n",
      "Epoch 2387/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 8.2711 - class_loss: 0.0053 - l1_loss: 0.8266\n",
      "Epoch 2388/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.4071 - class_loss: 0.0167 - l1_loss: 0.9390\n",
      "Epoch 2389/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.7532 - class_loss: 1.6137e-04 - l1_loss: 0.7753\n",
      "Epoch 2390/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.3203 - class_loss: 1.6705e-04 - l1_loss: 1.0320\n",
      "Epoch 2391/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.3359 - class_loss: 7.2544e-04 - l1_loss: 0.9335\n",
      "Epoch 2392/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.1365 - class_loss: 2.5336e-04 - l1_loss: 0.8136\n",
      "Epoch 2393/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.0794 - class_loss: 2.2080e-04 - l1_loss: 1.0079\n",
      "Epoch 2394/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.0027 - class_loss: 3.0843e-04 - l1_loss: 0.7002\n",
      "Epoch 2395/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.4900 - class_loss: 0.0163 - l1_loss: 0.7474\n",
      "Epoch 2396/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.0767 - class_loss: 0.0222 - l1_loss: 0.7055\n",
      "Epoch 2397/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.4575 - class_loss: 0.0018 - l1_loss: 0.6456\n",
      "Epoch 2398/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9605 - class_loss: 7.1826e-04 - l1_loss: 0.3960\n",
      "Epoch 2399/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.8442 - class_loss: 0.0151 - l1_loss: 0.4829\n",
      "Epoch 2400/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2324 - class_loss: 2.9114e-04 - l1_loss: 0.4232\n",
      "Epoch 2401/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.4673 - class_loss: 4.4352e-04 - l1_loss: 0.4467\n",
      "Epoch 2402/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.8949 - class_loss: 0.0037 - l1_loss: 0.3891\n",
      "Epoch 2403/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.3619 - class_loss: 0.0073 - l1_loss: 0.3355\n",
      "Epoch 2404/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0101 - class_loss: 0.0103 - l1_loss: 0.3000\n",
      "Epoch 2405/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.1051 - class_loss: 8.3511e-04 - l1_loss: 0.4104\n",
      "Epoch 2406/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.2469 - class_loss: 3.8705e-04 - l1_loss: 0.3247 0s - loss: 3.5029 - class_loss: 4.8854e-04 - l1_loss: 0.\n",
      "Epoch 2407/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9446 - class_loss: 0.0064 - l1_loss: 0.2938\n",
      "Epoch 2408/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.6558 - class_loss: 0.0110 - l1_loss: 0.4645\n",
      "Epoch 2409/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.0484 - class_loss: 6.2669e-04 - l1_loss: 0.4048\n",
      "Epoch 2410/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.9917 - class_loss: 0.0271 - l1_loss: 0.3965\n",
      "Epoch 2411/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8486 - class_loss: 0.0046 - l1_loss: 0.2844\n",
      "Epoch 2412/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.1101 - class_loss: 1.6368e-04 - l1_loss: 0.3110\n",
      "Epoch 2413/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 6.3256 - class_loss: 8.1172e-04 - l1_loss: 0.6325\n",
      "Epoch 2414/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.9799 - class_loss: 0.0040 - l1_loss: 0.3976\n",
      "Epoch 2415/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.7561 - class_loss: 0.0014 - l1_loss: 0.5755\n",
      "Epoch 2416/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9926 - class_loss: 0.0065 - l1_loss: 0.4986\n",
      "Epoch 2417/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.9553 - class_loss: 0.0060 - l1_loss: 0.4949\n",
      "Epoch 2418/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.3349 - class_loss: 0.0010 - l1_loss: 0.5334\n",
      "Epoch 2419/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.5257 - class_loss: 0.0039 - l1_loss: 0.6522\n",
      "Epoch 2420/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.9733 - class_loss: 0.0069 - l1_loss: 0.7966\n",
      "Epoch 2421/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 7.1496 - class_loss: 0.0040 - l1_loss: 0.7146\n",
      "Epoch 2422/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.4238 - class_loss: 0.0017 - l1_loss: 0.5422\n",
      "Epoch 2423/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.4735 - class_loss: 0.0095 - l1_loss: 0.3464\n",
      "Epoch 2424/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 5.5890 - class_loss: 0.0117 - l1_loss: 0.5577\n",
      "Epoch 2425/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 7.8969 - class_loss: 0.0031 - l1_loss: 0.7894\n",
      "Epoch 2426/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.7315 - class_loss: 1.5280e-04 - l1_loss: 0.4731\n",
      "Epoch 2427/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 6.0134 - class_loss: 3.9185e-04 - l1_loss: 0.6013\n",
      "Epoch 2428/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.2701 - class_loss: 2.9193e-04 - l1_loss: 0.5270\n",
      "Epoch 2429/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.1769 - class_loss: 0.0011 - l1_loss: 0.4176\n",
      "Epoch 2430/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.1143 - class_loss: 0.0062 - l1_loss: 0.4108\n",
      "Epoch 2431/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.4930 - class_loss: 0.0074 - l1_loss: 0.3486\n",
      "Epoch 2432/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.4769 - class_loss: 0.0213 - l1_loss: 0.3456\n",
      "Epoch 2433/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7587 - class_loss: 0.0038 - l1_loss: 0.2755\n",
      "Epoch 2434/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8281 - class_loss: 0.0032 - l1_loss: 0.2825\n",
      "Epoch 2435/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.0185 - class_loss: 6.6071e-04 - l1_loss: 0.3018\n",
      "Epoch 2436/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7724 - class_loss: 0.0031 - l1_loss: 0.2769\n",
      "Epoch 2437/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.1281 - class_loss: 0.0149 - l1_loss: 0.4113\n",
      "Epoch 2438/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.2467 - class_loss: 0.0026 - l1_loss: 0.4244\n",
      "Epoch 2439/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.2462 - class_loss: 5.6509e-04 - l1_loss: 0.4246\n",
      "Epoch 2440/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.1277 - class_loss: 8.3221e-04 - l1_loss: 0.3127\n",
      "Epoch 2441/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2081 - class_loss: 0.0102 - l1_loss: 0.3198\n",
      "Epoch 2442/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6896 - class_loss: 0.0023 - l1_loss: 0.3687\n",
      "Epoch 2443/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9172 - class_loss: 5.8730e-04 - l1_loss: 0.3917\n",
      "Epoch 2444/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.2077 - class_loss: 0.0036 - l1_loss: 0.4204\n",
      "Epoch 2445/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.6855 - class_loss: 0.0075 - l1_loss: 0.5678\n",
      "Epoch 2446/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.2288 - class_loss: 9.9322e-05 - l1_loss: 0.6229\n",
      "Epoch 2447/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.0333 - class_loss: 7.0093e-05 - l1_loss: 0.7033\n",
      "Epoch 2448/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.0200 - class_loss: 0.0011 - l1_loss: 0.6019\n",
      "Epoch 2449/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.5283 - class_loss: 0.0013 - l1_loss: 1.1527 ETA: 0s - loss: 12.2418 - class_loss: 0.0012 - l1_loss: 1.2241   \n",
      "Epoch 2450/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.0610 - class_loss: 0.0274 - l1_loss: 1.4034\n",
      "Epoch 2451/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 24.6859 - class_loss: 9.8479e-04 - l1_loss: 2.4685\n",
      "Epoch 2452/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 17.7830 - class_loss: 0.0018 - l1_loss: 1.7781\n",
      "Epoch 2453/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 43.8760 - class_loss: 0.0014 - l1_loss: 4.3875\n",
      "Epoch 2454/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 38.7234 - class_loss: 0.0026 - l1_loss: 3.8721\n",
      "Epoch 2455/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.7396 - class_loss: 0.0059 - l1_loss: 1.4734\n",
      "Epoch 2456/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 18.4234 - class_loss: 0.0021 - l1_loss: 1.8421\n",
      "Epoch 2457/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 45.4762 - class_loss: 0.0025 - l1_loss: 4.5474\n",
      "Epoch 2458/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 20.5639 - class_loss: 0.0020 - l1_loss: 2.0562\n",
      "Epoch 2459/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.6366 - class_loss: 0.0428 - l1_loss: 1.3594\n",
      "Epoch 2460/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 20.2480 - class_loss: 0.0016 - l1_loss: 2.0246\n",
      "Epoch 2461/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 21.4174 - class_loss: 0.0010 - l1_loss: 2.1416\n",
      "Epoch 2462/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 20.9275 - class_loss: 0.0103 - l1_loss: 2.0917\n",
      "Epoch 2463/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.3908 - class_loss: 0.0037 - l1_loss: 1.2387: 0s - loss: 11.6150 - class_loss: 0.0041 - l1_loss: 1.161\n",
      "Epoch 2464/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.3781 - class_loss: 0.0193 - l1_loss: 1.3359\n",
      "Epoch 2465/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.4697 - class_loss: 0.0044 - l1_loss: 0.8465\n",
      "Epoch 2466/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.7471 - class_loss: 0.0027 - l1_loss: 1.1744\n",
      "Epoch 2467/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.8439 - class_loss: 0.0097 - l1_loss: 0.9834\n",
      "Epoch 2468/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 12.5700 - class_loss: 0.0032 - l1_loss: 1.2567\n",
      "Epoch 2469/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.3122 - class_loss: 8.5779e-04 - l1_loss: 0.9311\n",
      "Epoch 2470/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.3568 - class_loss: 0.0016 - l1_loss: 0.9355\n",
      "Epoch 2471/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.1469 - class_loss: 0.0039 - l1_loss: 1.0143\n",
      "Epoch 2472/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.8596 - class_loss: 0.0013 - l1_loss: 1.0858\n",
      "Epoch 2473/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.4748 - class_loss: 0.0012 - l1_loss: 1.4474\n",
      "Epoch 2474/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.1014 - class_loss: 0.0010 - l1_loss: 0.9100\n",
      "Epoch 2475/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 14.4176 - class_loss: 0.0020 - l1_loss: 1.4416\n",
      "Epoch 2476/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.7163 - class_loss: 0.0015 - l1_loss: 0.9715\n",
      "Epoch 2477/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.9873 - class_loss: 0.0022 - l1_loss: 0.7985\n",
      "Epoch 2478/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 32ms/step - loss: 9.1305 - class_loss: 0.0348 - l1_loss: 0.9096\n",
      "Epoch 2479/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.9681 - class_loss: 0.0010 - l1_loss: 0.5967\n",
      "Epoch 2480/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.9750 - class_loss: 0.0136 - l1_loss: 0.6961\n",
      "Epoch 2481/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.1048 - class_loss: 0.0166 - l1_loss: 0.6088\n",
      "Epoch 2482/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.2486 - class_loss: 0.0011 - l1_loss: 0.6248\n",
      "Epoch 2483/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.6003 - class_loss: 3.1693e-04 - l1_loss: 0.4600\n",
      "Epoch 2484/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4805 - class_loss: 0.0030 - l1_loss: 0.3477\n",
      "Epoch 2485/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.6835 - class_loss: 0.0019 - l1_loss: 0.4682\n",
      "Epoch 2486/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.0282 - class_loss: 0.0028 - l1_loss: 0.4025\n",
      "Epoch 2487/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.7232 - class_loss: 0.0012 - l1_loss: 0.5722\n",
      "Epoch 2488/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 5.5199 - class_loss: 0.0069 - l1_loss: 0.5513\n",
      "Epoch 2489/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.2777 - class_loss: 0.0101 - l1_loss: 0.6268\n",
      "Epoch 2490/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0435 - class_loss: 1.7134e-04 - l1_loss: 0.4043\n",
      "Epoch 2491/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.0291 - class_loss: 4.9999e-04 - l1_loss: 0.4029\n",
      "Epoch 2492/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.7004 - class_loss: 0.0012 - l1_loss: 0.4699\n",
      "Epoch 2493/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.7987 - class_loss: 0.0010 - l1_loss: 0.5798\n",
      "Epoch 2494/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.6651 - class_loss: 8.4039e-04 - l1_loss: 0.3664\n",
      "Epoch 2495/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.2460 - class_loss: 0.0018 - l1_loss: 0.4244ETA: 0s - loss: 4.2091 - class_loss: 0.0011 - l1_loss: 0.4208\n",
      "Epoch 2496/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.6092 - class_loss: 0.3303 - l1_loss: 0.5279\n",
      "Epoch 2497/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.1195 - class_loss: 0.1481 - l1_loss: 0.5971\n",
      "Epoch 2498/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.4571 - class_loss: 0.0986 - l1_loss: 1.1358\n",
      "Epoch 2499/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.3730 - class_loss: 0.0178 - l1_loss: 1.2355\n",
      "Epoch 2500/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.9529 - class_loss: 0.0014 - l1_loss: 0.9952\n",
      "Epoch 2501/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.2035 - class_loss: 3.1124e-04 - l1_loss: 0.4203\n",
      "Epoch 2502/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.5509 - class_loss: 7.7016e-04 - l1_loss: 0.6550\n",
      "Epoch 2503/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.3607 - class_loss: 0.0079 - l1_loss: 0.6353\n",
      "Epoch 2504/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.8183 - class_loss: 9.5294e-04 - l1_loss: 0.5817\n",
      "Epoch 2505/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.7742 - class_loss: 0.0076 - l1_loss: 0.6767\n",
      "Epoch 2506/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.9119 - class_loss: 0.0055 - l1_loss: 0.5906\n",
      "Epoch 2507/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.1599 - class_loss: 4.7998e-04 - l1_loss: 0.5159\n",
      "Epoch 2508/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2509 - class_loss: 3.4997e-04 - l1_loss: 0.3251\n",
      "Epoch 2509/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.0403 - class_loss: 6.4071e-04 - l1_loss: 0.4040\n",
      "Epoch 2510/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.9274 - class_loss: 7.5257e-04 - l1_loss: 0.3927\n",
      "Epoch 2511/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3188 - class_loss: 0.0054 - l1_loss: 0.3313\n",
      "Epoch 2512/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5260 - class_loss: 0.0301 - l1_loss: 0.2496\n",
      "Epoch 2513/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1546 - class_loss: 0.0090 - l1_loss: 0.2146\n",
      "Epoch 2514/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9919 - class_loss: 0.0010 - l1_loss: 0.1991\n",
      "Epoch 2515/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6763 - class_loss: 8.8572e-04 - l1_loss: 0.1675\n",
      "Epoch 2516/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.6408 - class_loss: 0.0042 - l1_loss: 0.1637\n",
      "Epoch 2517/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.9434 - class_loss: 0.0012 - l1_loss: 0.1942\n",
      "Epoch 2518/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.1993 - class_loss: 0.0019 - l1_loss: 0.1197\n",
      "Epoch 2519/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.4685 - class_loss: 0.0048 - l1_loss: 0.1464\n",
      "Epoch 2520/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.3659 - class_loss: 0.0043 - l1_loss: 0.1362\n",
      "Epoch 2521/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.4581 - class_loss: 0.0036 - l1_loss: 0.1454\n",
      "Epoch 2522/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.1868 - class_loss: 0.0013 - l1_loss: 0.1186\n",
      "Epoch 2523/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.3584 - class_loss: 0.0015 - l1_loss: 0.1357\n",
      "Epoch 2524/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.9874 - class_loss: 0.0052 - l1_loss: 0.1982\n",
      "Epoch 2525/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.3240 - class_loss: 0.0044 - l1_loss: 0.2320\n",
      "Epoch 2526/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.5488 - class_loss: 0.0199 - l1_loss: 0.2529\n",
      "Epoch 2527/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.3990 - class_loss: 8.8496e-04 - l1_loss: 0.1398\n",
      "Epoch 2528/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.2203 - class_loss: 4.9318e-04 - l1_loss: 0.2220\n",
      "Epoch 2529/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.6308 - class_loss: 4.8600e-04 - l1_loss: 0.1630\n",
      "Epoch 2530/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.6788 - class_loss: 0.0017 - l1_loss: 0.1677\n",
      "Epoch 2531/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.3562 - class_loss: 0.0011 - l1_loss: 0.2355\n",
      "Epoch 2532/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.8086 - class_loss: 0.0019 - l1_loss: 0.1807\n",
      "Epoch 2533/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.7142 - class_loss: 0.0010 - l1_loss: 0.1713\n",
      "Epoch 2534/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.1064 - class_loss: 0.0016 - l1_loss: 0.2105\n",
      "Epoch 2535/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.5103 - class_loss: 0.0034 - l1_loss: 0.1507\n",
      "Epoch 2536/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.7844 - class_loss: 0.0021 - l1_loss: 0.1782\n",
      "Epoch 2537/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.8759 - class_loss: 0.0033 - l1_loss: 0.1873\n",
      "Epoch 2538/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.2446 - class_loss: 9.6356e-04 - l1_loss: 0.2244\n",
      "Epoch 2539/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.0629 - class_loss: 0.0056 - l1_loss: 0.2057\n",
      "Epoch 2540/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.1655 - class_loss: 0.0070 - l1_loss: 0.2158\n",
      "Epoch 2541/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.1250 - class_loss: 0.0015 - l1_loss: 0.3123\n",
      "Epoch 2542/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.7042 - class_loss: 7.5580e-04 - l1_loss: 0.2703\n",
      "Epoch 2543/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9115 - class_loss: 0.0011 - l1_loss: 0.2910\n",
      "Epoch 2544/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.9014 - class_loss: 0.0015 - l1_loss: 0.4900\n",
      "Epoch 2545/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 3.9775 - class_loss: 7.0618e-04 - l1_loss: 0.3977\n",
      "Epoch 2546/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.8362 - class_loss: 0.0086 - l1_loss: 0.5828\n",
      "Epoch 2547/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.9136 - class_loss: 0.0137 - l1_loss: 0.4900\n",
      "Epoch 2548/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.1008 - class_loss: 0.0025 - l1_loss: 0.4098\n",
      "Epoch 2549/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.3276 - class_loss: 0.0026 - l1_loss: 0.4325\n",
      "Epoch 2550/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.4160 - class_loss: 0.0053 - l1_loss: 0.3411\n",
      "Epoch 2551/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.3977 - class_loss: 0.0015 - l1_loss: 0.3396\n",
      "Epoch 2552/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.0752 - class_loss: 0.0045 - l1_loss: 0.3071\n",
      "Epoch 2553/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.7317 - class_loss: 0.0170 - l1_loss: 0.2715\n",
      "Epoch 2554/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2420 - class_loss: 0.0031 - l1_loss: 0.3239\n",
      "Epoch 2555/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.6140 - class_loss: 4.4300e-04 - l1_loss: 0.3614\n",
      "Epoch 2556/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8610 - class_loss: 0.0019 - l1_loss: 0.2859\n",
      "Epoch 2557/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.4357 - class_loss: 6.5223e-04 - l1_loss: 0.4435\n",
      "Epoch 2558/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.5087 - class_loss: 0.0085 - l1_loss: 0.3500\n",
      "Epoch 2559/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3375 - class_loss: 0.0041 - l1_loss: 0.3333\n",
      "Epoch 2560/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8377 - class_loss: 6.4278e-04 - l1_loss: 0.2837\n",
      "Epoch 2561/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.0739 - class_loss: 0.0025 - l1_loss: 0.3071\n",
      "Epoch 2562/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9400 - class_loss: 0.0134 - l1_loss: 0.2927\n",
      "Epoch 2563/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.4701 - class_loss: 0.0011 - l1_loss: 0.2469\n",
      "Epoch 2564/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9840 - class_loss: 0.0015 - l1_loss: 0.2982\n",
      "Epoch 2565/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9472 - class_loss: 0.0108 - l1_loss: 0.2936\n",
      "Epoch 2566/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.0885 - class_loss: 0.0020 - l1_loss: 0.3087\n",
      "Epoch 2567/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5915 - class_loss: 0.0010 - l1_loss: 0.2591\n",
      "Epoch 2568/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7040 - class_loss: 0.0020 - l1_loss: 0.2702\n",
      "Epoch 2569/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7978 - class_loss: 0.0019 - l1_loss: 0.1796\n",
      "Epoch 2570/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.3042 - class_loss: 0.0018 - l1_loss: 0.2302\n",
      "Epoch 2571/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1422 - class_loss: 0.0023 - l1_loss: 0.2140\n",
      "Epoch 2572/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.4196 - class_loss: 0.0076 - l1_loss: 0.3412\n",
      "Epoch 2573/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.8014 - class_loss: 0.0028 - l1_loss: 0.3799\n",
      "Epoch 2574/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7102 - class_loss: 0.0164 - l1_loss: 0.2694\n",
      "Epoch 2575/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8765 - class_loss: 0.0015 - l1_loss: 0.1875\n",
      "Epoch 2576/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.1150 - class_loss: 6.9693e-04 - l1_loss: 0.3114\n",
      "Epoch 2577/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.6014 - class_loss: 9.9222e-04 - l1_loss: 0.2600\n",
      "Epoch 2578/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7188 - class_loss: 0.0032 - l1_loss: 0.2716\n",
      "Epoch 2579/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2291 - class_loss: 0.0034 - l1_loss: 0.2226\n",
      "Epoch 2580/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.6634 - class_loss: 8.6312e-04 - l1_loss: 0.2663\n",
      "Epoch 2581/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0953 - class_loss: 0.0017 - l1_loss: 0.2094\n",
      "Epoch 2582/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1567 - class_loss: 0.0014 - l1_loss: 0.2155\n",
      "Epoch 2583/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2420 - class_loss: 0.0027 - l1_loss: 0.2239\n",
      "Epoch 2584/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.3053 - class_loss: 0.0031 - l1_loss: 0.2302\n",
      "Epoch 2585/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3662 - class_loss: 0.0049 - l1_loss: 0.2361\n",
      "Epoch 2586/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9418 - class_loss: 0.0025 - l1_loss: 0.3939\n",
      "Epoch 2587/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.3708 - class_loss: 0.0011 - l1_loss: 0.3370\n",
      "Epoch 2588/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2132 - class_loss: 0.0017 - l1_loss: 0.3211\n",
      "Epoch 2589/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.8346 - class_loss: 0.0017 - l1_loss: 0.3833\n",
      "Epoch 2590/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.1538 - class_loss: 0.0055 - l1_loss: 0.4148\n",
      "Epoch 2591/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 8.9308 - class_loss: 0.0030 - l1_loss: 0.8928\n",
      "Epoch 2592/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.2172 - class_loss: 0.0039 - l1_loss: 0.6213\n",
      "Epoch 2593/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.4076 - class_loss: 0.0073 - l1_loss: 0.5400\n",
      "Epoch 2594/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.3176 - class_loss: 0.0021 - l1_loss: 0.4316\n",
      "Epoch 2595/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.3861 - class_loss: 9.1980e-04 - l1_loss: 0.3385\n",
      "Epoch 2596/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9864 - class_loss: 0.0061 - l1_loss: 0.4980\n",
      "Epoch 2597/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.0458 - class_loss: 0.0018 - l1_loss: 0.5044\n",
      "Epoch 2598/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.2959 - class_loss: 0.0015 - l1_loss: 0.7294\n",
      "Epoch 2599/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.9249 - class_loss: 0.0017 - l1_loss: 0.4923\n",
      "Epoch 2600/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.5547 - class_loss: 0.0026 - l1_loss: 0.4552\n",
      "Epoch 2601/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1800 - class_loss: 0.0042 - l1_loss: 0.3176\n",
      "Epoch 2602/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.9899 - class_loss: 0.0262 - l1_loss: 0.3964 0s - loss: 4.0251 - class_loss: 0.0350 - l1_loss: 0.\n",
      "Epoch 2603/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.6246 - class_loss: 0.0014 - l1_loss: 0.4623\n",
      "Epoch 2604/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.6439 - class_loss: 0.0010 - l1_loss: 0.3643\n",
      "Epoch 2605/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7912 - class_loss: 8.5202e-04 - l1_loss: 0.4790\n",
      "Epoch 2606/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.8038 - class_loss: 0.0059 - l1_loss: 0.5798\n",
      "Epoch 2607/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.4344 - class_loss: 0.0107 - l1_loss: 0.7424\n",
      "Epoch 2608/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.7724 - class_loss: 0.0017 - l1_loss: 0.7771\n",
      "Epoch 2609/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.3609 - class_loss: 0.0221 - l1_loss: 1.0339\n",
      "Epoch 2610/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 31ms/step - loss: 9.1862 - class_loss: 0.0032 - l1_loss: 0.9183\n",
      "Epoch 2611/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 10.4644 - class_loss: 0.0050 - l1_loss: 1.0459\n",
      "Epoch 2612/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.5773 - class_loss: 0.0023 - l1_loss: 0.6575\n",
      "Epoch 2613/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.7385 - class_loss: 0.0015 - l1_loss: 0.6737 0s - loss: 6.8591 - class_loss: 0.0017 - l1_loss: 0.\n",
      "Epoch 2614/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.7669 - class_loss: 0.0121 - l1_loss: 0.9755\n",
      "Epoch 2615/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.6331 - class_loss: 0.0035 - l1_loss: 0.6630\n",
      "Epoch 2616/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.0249 - class_loss: 0.0029 - l1_loss: 0.9022\n",
      "Epoch 2617/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.5386 - class_loss: 0.0012 - l1_loss: 0.6537\n",
      "Epoch 2618/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 7.1153 - class_loss: 0.0014 - l1_loss: 0.7114\n",
      "Epoch 2619/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.2698 - class_loss: 0.0248 - l1_loss: 0.6245\n",
      "Epoch 2620/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.5867 - class_loss: 0.0020 - l1_loss: 0.6585 0s - loss: 5.6580 - class_loss: 0.0029 - l1_loss: 0.\n",
      "Epoch 2621/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.1864 - class_loss: 9.4121e-04 - l1_loss: 0.5185\n",
      "Epoch 2622/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.8281 - class_loss: 3.7889e-04 - l1_loss: 0.6828\n",
      "Epoch 2623/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.7333 - class_loss: 2.3033e-04 - l1_loss: 0.8733\n",
      "Epoch 2624/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.4080 - class_loss: 0.0013 - l1_loss: 1.1407\n",
      "Epoch 2625/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.2297 - class_loss: 0.0129 - l1_loss: 1.3217\n",
      "Epoch 2626/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.7499 - class_loss: 0.0088 - l1_loss: 1.8741\n",
      "Epoch 2627/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.7180 - class_loss: 0.0107 - l1_loss: 1.0707\n",
      "Epoch 2628/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.3115 - class_loss: 8.2416e-04 - l1_loss: 1.7311\n",
      "Epoch 2629/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 20.1064 - class_loss: 5.0778e-04 - l1_loss: 2.0106\n",
      "Epoch 2630/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.1597 - class_loss: 8.1829e-04 - l1_loss: 1.5159\n",
      "Epoch 2631/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 20.7453 - class_loss: 0.0212 - l1_loss: 2.0724\n",
      "Epoch 2632/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 19.9117 - class_loss: 0.0049 - l1_loss: 1.9907\n",
      "Epoch 2633/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 28.1604 - class_loss: 0.0017 - l1_loss: 2.8159\n",
      "Epoch 2634/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 25.7633 - class_loss: 0.0037 - l1_loss: 2.5760\n",
      "Epoch 2635/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 28.4201 - class_loss: 6.9779e-04 - l1_loss: 2.8419\n",
      "Epoch 2636/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 29.7826 - class_loss: 0.0107 - l1_loss: 2.9772\n",
      "Epoch 2637/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 42.6830 - class_loss: 0.0160 - l1_loss: 4.2667\n",
      "Epoch 2638/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 63.1981 - class_loss: 0.0038 - l1_loss: 6.3194\n",
      "Epoch 2639/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 54.6630 - class_loss: 0.0103 - l1_loss: 5.4653\n",
      "Epoch 2640/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 38.3497 - class_loss: 0.0023 - l1_loss: 3.8347\n",
      "Epoch 2641/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 51.7128 - class_loss: 0.0013 - l1_loss: 5.1712\n",
      "Epoch 2642/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 26.6701 - class_loss: 0.0052 - l1_loss: 2.6665\n",
      "Epoch 2643/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 24.6489 - class_loss: 0.0079 - l1_loss: 2.4641\n",
      "Epoch 2644/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.7173 - class_loss: 0.0133 - l1_loss: 1.5704\n",
      "Epoch 2645/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.9600 - class_loss: 0.0016 - l1_loss: 1.2958\n",
      "Epoch 2646/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.5702 - class_loss: 0.0021 - l1_loss: 1.3568\n",
      "Epoch 2647/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 13.1832 - class_loss: 0.0123 - l1_loss: 1.3171\n",
      "Epoch 2648/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.4708 - class_loss: 0.0206 - l1_loss: 1.1450\n",
      "Epoch 2649/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.8195 - class_loss: 0.0312 - l1_loss: 1.0788\n",
      "Epoch 2650/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.2171 - class_loss: 4.9311e-04 - l1_loss: 0.8217\n",
      "Epoch 2651/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.0623 - class_loss: 0.0015 - l1_loss: 0.9061\n",
      "Epoch 2652/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 10.8121 - class_loss: 0.0045 - l1_loss: 1.0808\n",
      "Epoch 2653/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.2761 - class_loss: 0.0034 - l1_loss: 0.6273\n",
      "Epoch 2654/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.9379 - class_loss: 0.0086 - l1_loss: 0.5929\n",
      "Epoch 2655/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.2100 - class_loss: 0.0070 - l1_loss: 0.6203\n",
      "Epoch 2656/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.4226 - class_loss: 0.0080 - l1_loss: 0.5415\n",
      "Epoch 2657/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.9308 - class_loss: 8.4044e-04 - l1_loss: 0.3930\n",
      "Epoch 2658/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 3.7509 - class_loss: 0.0030 - l1_loss: 0.3748\n",
      "Epoch 2659/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.6119 - class_loss: 0.0062 - l1_loss: 0.3606\n",
      "Epoch 2660/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1399 - class_loss: 0.0036 - l1_loss: 0.3136\n",
      "Epoch 2661/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.5886 - class_loss: 0.0250 - l1_loss: 0.2564\n",
      "Epoch 2662/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5608 - class_loss: 0.0127 - l1_loss: 0.2548\n",
      "Epoch 2663/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9575 - class_loss: 5.9564e-04 - l1_loss: 0.2957\n",
      "Epoch 2664/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7572 - class_loss: 2.8821e-04 - l1_loss: 0.2757\n",
      "Epoch 2665/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3773 - class_loss: 0.0012 - l1_loss: 0.2376\n",
      "Epoch 2666/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0678 - class_loss: 0.0083 - l1_loss: 0.2059\n",
      "Epoch 2667/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8450 - class_loss: 0.0037 - l1_loss: 0.1841\n",
      "Epoch 2668/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.3892 - class_loss: 0.0016 - l1_loss: 0.1388\n",
      "Epoch 2669/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5660 - class_loss: 0.0023 - l1_loss: 0.1564\n",
      "Epoch 2670/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2102 - class_loss: 0.0043 - l1_loss: 0.1206\n",
      "Epoch 2671/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4644 - class_loss: 0.0037 - l1_loss: 0.1461 0s - loss: 1.5802 - class_loss: 0.0015 - l1_loss: \n",
      "Epoch 2672/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4824 - class_loss: 0.0058 - l1_loss: 0.1477\n",
      "Epoch 2673/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2851 - class_loss: 0.0024 - l1_loss: 0.1283\n",
      "Epoch 2674/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9350 - class_loss: 0.0028 - l1_loss: 0.0932\n",
      "Epoch 2675/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7254 - class_loss: 0.0017 - l1_loss: 0.1724\n",
      "Epoch 2676/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7142 - class_loss: 0.0020 - l1_loss: 0.1712\n",
      "Epoch 2677/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2708 - class_loss: 0.0022 - l1_loss: 0.2269\n",
      "Epoch 2678/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6753 - class_loss: 0.0031 - l1_loss: 0.1672\n",
      "Epoch 2679/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.2414 - class_loss: 0.0031 - l1_loss: 0.2238\n",
      "Epoch 2680/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1093 - class_loss: 0.0067 - l1_loss: 0.3103\n",
      "Epoch 2681/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1842 - class_loss: 0.0013 - l1_loss: 0.3183\n",
      "Epoch 2682/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1098 - class_loss: 0.0011 - l1_loss: 0.2109\n",
      "Epoch 2683/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8030 - class_loss: 0.0060 - l1_loss: 0.3797\n",
      "Epoch 2684/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4034 - class_loss: 0.0029 - l1_loss: 0.2401\n",
      "Epoch 2685/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.8565 - class_loss: 0.0025 - l1_loss: 0.3854\n",
      "Epoch 2686/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.6759 - class_loss: 0.0057 - l1_loss: 0.3670\n",
      "Epoch 2687/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.3530 - class_loss: 0.0012 - l1_loss: 0.4352\n",
      "Epoch 2688/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.5320 - class_loss: 0.0049 - l1_loss: 0.4527\n",
      "Epoch 2689/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3678 - class_loss: 0.0054 - l1_loss: 0.3362\n",
      "Epoch 2690/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6902 - class_loss: 0.0028 - l1_loss: 0.2687\n",
      "Epoch 2691/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6744 - class_loss: 0.0044 - l1_loss: 0.2670\n",
      "Epoch 2692/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2296 - class_loss: 0.0054 - l1_loss: 0.2224\n",
      "Epoch 2693/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1813 - class_loss: 0.0228 - l1_loss: 0.2159\n",
      "Epoch 2694/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.6159 - class_loss: 0.0027 - l1_loss: 0.3613\n",
      "Epoch 2695/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1135 - class_loss: 5.5592e-04 - l1_loss: 0.3113\n",
      "Epoch 2696/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5858 - class_loss: 5.4207e-04 - l1_loss: 0.2585\n",
      "Epoch 2697/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.1096 - class_loss: 0.0014 - l1_loss: 0.2108\n",
      "Epoch 2698/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6247 - class_loss: 9.5781e-04 - l1_loss: 0.1624\n",
      "Epoch 2699/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6475 - class_loss: 8.5890e-04 - l1_loss: 0.1647\n",
      "Epoch 2700/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.4866 - class_loss: 0.0015 - l1_loss: 0.1485\n",
      "Epoch 2701/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1177 - class_loss: 0.0029 - l1_loss: 0.2115\n",
      "Epoch 2702/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6807 - class_loss: 0.0022 - l1_loss: 0.1679\n",
      "Epoch 2703/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5283 - class_loss: 0.0022 - l1_loss: 0.1526\n",
      "Epoch 2704/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9524 - class_loss: 0.0104 - l1_loss: 0.1942\n",
      "Epoch 2705/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.4790 - class_loss: 0.0014 - l1_loss: 0.1478\n",
      "Epoch 2706/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4305 - class_loss: 7.6936e-04 - l1_loss: 0.1430\n",
      "Epoch 2707/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5534 - class_loss: 3.1725e-04 - l1_loss: 0.1553\n",
      "Epoch 2708/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3925 - class_loss: 0.0010 - l1_loss: 0.1392\n",
      "Epoch 2709/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2908 - class_loss: 0.0038 - l1_loss: 0.1287\n",
      "Epoch 2710/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0171 - class_loss: 0.0028 - l1_loss: 0.1014\n",
      "Epoch 2711/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4492 - class_loss: 0.0069 - l1_loss: 0.1442\n",
      "Epoch 2712/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.0079 - class_loss: 0.0062 - l1_loss: 0.2002\n",
      "Epoch 2713/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5324 - class_loss: 0.0014 - l1_loss: 0.1531\n",
      "Epoch 2714/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6967 - class_loss: 0.0012 - l1_loss: 0.2696\n",
      "Epoch 2715/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1702 - class_loss: 0.0012 - l1_loss: 0.2169\n",
      "Epoch 2716/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.6015 - class_loss: 0.0024 - l1_loss: 0.3599\n",
      "Epoch 2717/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.6261 - class_loss: 0.0021 - l1_loss: 0.3624\n",
      "Epoch 2718/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6898 - class_loss: 0.0037 - l1_loss: 0.2686\n",
      "Epoch 2719/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5978 - class_loss: 0.0046 - l1_loss: 0.3593\n",
      "Epoch 2720/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1952 - class_loss: 0.0011 - l1_loss: 0.2194\n",
      "Epoch 2721/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6647 - class_loss: 0.0057 - l1_loss: 0.2659\n",
      "Epoch 2722/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6502 - class_loss: 0.0210 - l1_loss: 0.2629\n",
      "Epoch 2723/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9413 - class_loss: 0.0045 - l1_loss: 0.2937\n",
      "Epoch 2724/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1499 - class_loss: 0.0134 - l1_loss: 0.4137\n",
      "Epoch 2725/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6244 - class_loss: 0.0013 - l1_loss: 0.2623\n",
      "Epoch 2726/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1573 - class_loss: 7.5606e-04 - l1_loss: 0.2157\n",
      "Epoch 2727/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7471 - class_loss: 0.0014 - l1_loss: 0.2746\n",
      "Epoch 2728/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.1808 - class_loss: 0.0039 - l1_loss: 0.5177\n",
      "Epoch 2729/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7353 - class_loss: 1.3693e-04 - l1_loss: 0.3735\n",
      "Epoch 2730/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.7368 - class_loss: 0.0011 - l1_loss: 0.4736\n",
      "Epoch 2731/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.3579 - class_loss: 0.0201 - l1_loss: 0.8338\n",
      "Epoch 2732/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.7471 - class_loss: 0.0033 - l1_loss: 0.7744\n",
      "Epoch 2733/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.5651 - class_loss: 0.0016 - l1_loss: 0.7564\n",
      "Epoch 2734/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.8719 - class_loss: 0.0141 - l1_loss: 0.7858\n",
      "Epoch 2735/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.8324 - class_loss: 0.0015 - l1_loss: 0.8831\n",
      "Epoch 2736/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.6324 - class_loss: 0.0014 - l1_loss: 0.9631\n",
      "Epoch 2737/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.8985 - class_loss: 0.0054 - l1_loss: 0.8893\n",
      "Epoch 2738/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.7103 - class_loss: 7.6722e-04 - l1_loss: 0.8710\n",
      "Epoch 2739/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.2610 - class_loss: 0.0015 - l1_loss: 1.0259\n",
      "Epoch 2740/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.4135 - class_loss: 0.0023 - l1_loss: 0.9411\n",
      "Epoch 2741/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.2544 - class_loss: 0.0029 - l1_loss: 0.8251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2742/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.6133 - class_loss: 0.0161 - l1_loss: 0.6597\n",
      "Epoch 2743/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.4416 - class_loss: 0.0046 - l1_loss: 0.6437\n",
      "Epoch 2744/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.5571 - class_loss: 0.0101 - l1_loss: 0.7547\n",
      "Epoch 2745/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.4481 - class_loss: 4.5098e-04 - l1_loss: 0.6448\n",
      "Epoch 2746/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.2285 - class_loss: 0.0015 - l1_loss: 0.5227\n",
      "Epoch 2747/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.1704 - class_loss: 0.0061 - l1_loss: 0.5164\n",
      "Epoch 2748/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3265 - class_loss: 0.0016 - l1_loss: 0.4325\n",
      "Epoch 2749/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.9606 - class_loss: 9.2755e-04 - l1_loss: 0.3960\n",
      "Epoch 2750/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5949 - class_loss: 0.0028 - l1_loss: 0.3592\n",
      "Epoch 2751/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.9699 - class_loss: 0.0335 - l1_loss: 0.3936\n",
      "Epoch 2752/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0626 - class_loss: 3.6187e-04 - l1_loss: 0.3062\n",
      "Epoch 2753/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.7279 - class_loss: 6.9680e-04 - l1_loss: 0.3727\n",
      "Epoch 2754/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.2194 - class_loss: 0.0050 - l1_loss: 0.5214\n",
      "Epoch 2755/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.4137 - class_loss: 0.0014 - l1_loss: 0.5412\n",
      "Epoch 2756/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.5795 - class_loss: 0.0021 - l1_loss: 0.3577\n",
      "Epoch 2757/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1795 - class_loss: 0.0082 - l1_loss: 0.3171\n",
      "Epoch 2758/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4959 - class_loss: 0.0031 - l1_loss: 0.2493\n",
      "Epoch 2759/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0330 - class_loss: 0.0014 - l1_loss: 0.3032\n",
      "Epoch 2760/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9198 - class_loss: 0.0115 - l1_loss: 0.2908\n",
      "Epoch 2761/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8107 - class_loss: 9.8380e-04 - l1_loss: 0.2810\n",
      "Epoch 2762/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9376 - class_loss: 0.0042 - l1_loss: 0.3933\n",
      "Epoch 2763/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2585 - class_loss: 0.0062 - l1_loss: 0.4252\n",
      "Epoch 2764/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8602 - class_loss: 6.9714e-04 - l1_loss: 0.4859\n",
      "Epoch 2765/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.0902 - class_loss: 0.0021 - l1_loss: 0.6088\n",
      "Epoch 2766/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.8763 - class_loss: 0.0144 - l1_loss: 0.7862\n",
      "Epoch 2767/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.8124 - class_loss: 0.0116 - l1_loss: 0.5801\n",
      "Epoch 2768/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.9099 - class_loss: 5.7117e-04 - l1_loss: 0.7909\n",
      "Epoch 2769/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.2607 - class_loss: 0.0025 - l1_loss: 0.5258\n",
      "Epoch 2770/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.1086 - class_loss: 0.0028 - l1_loss: 0.4106\n",
      "Epoch 2771/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.4835 - class_loss: 0.0026 - l1_loss: 0.5481\n",
      "Epoch 2772/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 10.3003 - class_loss: 0.0087 - l1_loss: 1.0292\n",
      "Epoch 2773/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.1902 - class_loss: 0.0194 - l1_loss: 0.8171\n",
      "Epoch 2774/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 7.5429 - class_loss: 0.0024 - l1_loss: 0.75 - 0s 30ms/step - loss: 8.1740 - class_loss: 0.0023 - l1_loss: 0.8172\n",
      "Epoch 2775/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.8677 - class_loss: 0.0015 - l1_loss: 1.0866\n",
      "Epoch 2776/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.6212 - class_loss: 0.0044 - l1_loss: 1.0617\n",
      "Epoch 2777/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.7391 - class_loss: 0.0054 - l1_loss: 0.7734ETA: 0s - loss: 9.4136 - class_loss: 0.0086 - l1_loss: 0.\n",
      "Epoch 2778/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 18.5183 - class_loss: 0.0022 - l1_loss: 1.8516\n",
      "Epoch 2779/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 32.0975 - class_loss: 3.5245e-04 - l1_loss: 3.2097\n",
      "Epoch 2780/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 40.3058 - class_loss: 0.0030 - l1_loss: 4.0303\n",
      "Epoch 2781/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 20.1681 - class_loss: 0.0090 - l1_loss: 2.0159\n",
      "Epoch 2782/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 22.1780 - class_loss: 0.0049 - l1_loss: 2.2173\n",
      "Epoch 2783/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 18.0379 - class_loss: 3.9253e-04 - l1_loss: 1.8038\n",
      "Epoch 2784/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 28.0424 - class_loss: 0.0086 - l1_loss: 2.8034\n",
      "Epoch 2785/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 39.9831 - class_loss: 0.0154 - l1_loss: 3.9968\n",
      "Epoch 2786/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 30.7488 - class_loss: 0.0134 - l1_loss: 3.0735ETA: 0s - loss: 30.1420 - class_loss: 0.0152 - l1_loss: 3.012\n",
      "Epoch 2787/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 44.8780 - class_loss: 7.8507e-04 - l1_loss: 4.4877\n",
      "Epoch 2788/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 22.3951 - class_loss: 0.0016 - l1_loss: 2.2394\n",
      "Epoch 2789/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.7463 - class_loss: 0.0013 - l1_loss: 1.6745\n",
      "Epoch 2790/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.6700 - class_loss: 0.0563 - l1_loss: 1.5614\n",
      "Epoch 2791/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.8261 - class_loss: 4.9140e-04 - l1_loss: 1.5826\n",
      "Epoch 2792/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.7934 - class_loss: 0.0012 - l1_loss: 1.3792\n",
      "Epoch 2793/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.1041 - class_loss: 0.0712 - l1_loss: 1.2033\n",
      "Epoch 2794/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.7776 - class_loss: 7.6865e-04 - l1_loss: 1.1777\n",
      "Epoch 2795/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.6107 - class_loss: 7.9842e-04 - l1_loss: 1.6610\n",
      "Epoch 2796/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.6975 - class_loss: 0.0015 - l1_loss: 1.3696\n",
      "Epoch 2797/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.5730 - class_loss: 7.9793e-04 - l1_loss: 1.2572\n",
      "Epoch 2798/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 19.3805 - class_loss: 0.0045 - l1_loss: 1.9376\n",
      "Epoch 2799/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 17.4849 - class_loss: 0.1852 - l1_loss: 1.7300\n",
      "Epoch 2800/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.8639 - class_loss: 0.0718 - l1_loss: 1.2792\n",
      "Epoch 2801/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 10.4777 - class_loss: 0.0033 - l1_loss: 1.0474\n",
      "Epoch 2802/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 25.5959 - class_loss: 0.0018 - l1_loss: 2.5594\n",
      "Epoch 2803/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.7521 - class_loss: 6.3884e-04 - l1_loss: 1.4751\n",
      "Epoch 2804/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.3042 - class_loss: 0.0013 - l1_loss: 1.0303\n",
      "Epoch 2805/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.9155 - class_loss: 0.2113 - l1_loss: 1.1704\n",
      "Epoch 2806/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.0024 - class_loss: 0.0442 - l1_loss: 1.3958\n",
      "Epoch 2807/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.0036 - class_loss: 0.0191 - l1_loss: 0.9985\n",
      "Epoch 2808/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.4162 - class_loss: 0.0014 - l1_loss: 0.8415\n",
      "Epoch 2809/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.5483 - class_loss: 8.3369e-04 - l1_loss: 0.8547\n",
      "Epoch 2810/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.1826 - class_loss: 4.7566e-04 - l1_loss: 0.8182\n",
      "Epoch 2811/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 9.1454 - class_loss: 0.0043 - l1_loss: 0.9141\n",
      "Epoch 2812/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.3823 - class_loss: 0.0095 - l1_loss: 0.5373\n",
      "Epoch 2813/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.9565 - class_loss: 0.2515 - l1_loss: 0.4705\n",
      "Epoch 2814/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.9799 - class_loss: 0.0016 - l1_loss: 0.6978\n",
      "Epoch 2815/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.5447 - class_loss: 0.0100 - l1_loss: 0.5535A: 0s - loss: 5.3455 - class_loss: 0.0114 - l1_loss: 0.53\n",
      "Epoch 2816/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.7049 - class_loss: 0.0024 - l1_loss: 0.4703\n",
      "Epoch 2817/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.1537 - class_loss: 0.0013 - l1_loss: 0.6152\n",
      "Epoch 2818/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.2227 - class_loss: 7.5425e-04 - l1_loss: 0.4222\n",
      "Epoch 2819/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.2682 - class_loss: 2.6307e-04 - l1_loss: 0.8268\n",
      "Epoch 2820/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.8137 - class_loss: 0.0031 - l1_loss: 0.5811\n",
      "Epoch 2821/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 5.7587 - class_loss: 0.0028 - l1_loss: 0.5756\n",
      "Epoch 2822/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.6087 - class_loss: 0.0024 - l1_loss: 0.6606\n",
      "Epoch 2823/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.5942 - class_loss: 0.0017 - l1_loss: 0.8593\n",
      "Epoch 2824/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.5211 - class_loss: 0.0451 - l1_loss: 0.6476\n",
      "Epoch 2825/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.3721 - class_loss: 0.2055 - l1_loss: 0.6167\n",
      "Epoch 2826/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.5730 - class_loss: 0.0092 - l1_loss: 0.5564ETA: 0s - loss: 5.0473 - class_loss: 0.0044 - l1_loss: 0.50\n",
      "Epoch 2827/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.5480 - class_loss: 0.0304 - l1_loss: 1.0518\n",
      "Epoch 2828/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.8722 - class_loss: 0.0060 - l1_loss: 0.9866 ETA: 0s - loss: 9.7605 - class_loss: 0.0091 - l1_loss: 0.\n",
      "Epoch 2829/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.0297 - class_loss: 0.0032 - l1_loss: 0.8026\n",
      "Epoch 2830/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.2674 - class_loss: 0.0036 - l1_loss: 0.7264\n",
      "Epoch 2831/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.1161 - class_loss: 0.0014 - l1_loss: 0.6115\n",
      "Epoch 2832/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.4294 - class_loss: 0.0039 - l1_loss: 0.7426\n",
      "Epoch 2833/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.6312 - class_loss: 0.0016 - l1_loss: 0.5630\n",
      "Epoch 2834/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.8630 - class_loss: 2.9543e-04 - l1_loss: 0.4863\n",
      "Epoch 2835/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.6096 - class_loss: 0.0050 - l1_loss: 0.4605\n",
      "Epoch 2836/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.3689 - class_loss: 0.0068 - l1_loss: 0.5362\n",
      "Epoch 2837/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.3831 - class_loss: 0.0080 - l1_loss: 0.3375\n",
      "Epoch 2838/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.9708 - class_loss: 0.0205 - l1_loss: 0.5950\n",
      "Epoch 2839/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.3039 - class_loss: 0.0012 - l1_loss: 0.4303\n",
      "Epoch 2840/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.0549 - class_loss: 0.0013 - l1_loss: 0.4054\n",
      "Epoch 2841/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.2573 - class_loss: 0.0066 - l1_loss: 0.4251\n",
      "Epoch 2842/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.7389 - class_loss: 0.0073 - l1_loss: 0.3732\n",
      "Epoch 2843/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2565 - class_loss: 0.0122 - l1_loss: 0.3244\n",
      "Epoch 2844/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.7596 - class_loss: 0.0020 - l1_loss: 0.3758\n",
      "Epoch 2845/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7857 - class_loss: 0.0135 - l1_loss: 0.2772\n",
      "Epoch 2846/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.2053 - class_loss: 7.7661e-04 - l1_loss: 0.3205\n",
      "Epoch 2847/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.4163 - class_loss: 0.0013 - l1_loss: 0.2415\n",
      "Epoch 2848/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2534 - class_loss: 0.0085 - l1_loss: 0.2245\n",
      "Epoch 2849/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.0769 - class_loss: 0.0031 - l1_loss: 0.2074\n",
      "Epoch 2850/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.8347 - class_loss: 9.8204e-04 - l1_loss: 0.1834\n",
      "Epoch 2851/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7377 - class_loss: 0.0028 - l1_loss: 0.1735\n",
      "Epoch 2852/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.5948 - class_loss: 0.0026 - l1_loss: 0.1592\n",
      "Epoch 2853/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.5943 - class_loss: 0.0041 - l1_loss: 0.1590\n",
      "Epoch 2854/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.3937 - class_loss: 0.0052 - l1_loss: 0.1388\n",
      "Epoch 2855/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.2297 - class_loss: 0.0029 - l1_loss: 0.1227\n",
      "Epoch 2856/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7010 - class_loss: 0.0019 - l1_loss: 0.1699\n",
      "Epoch 2857/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.3934 - class_loss: 0.0025 - l1_loss: 0.1391\n",
      "Epoch 2858/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.6055 - class_loss: 0.0048 - l1_loss: 0.1601\n",
      "Epoch 2859/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.5711 - class_loss: 0.0020 - l1_loss: 0.1569\n",
      "Epoch 2860/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7932 - class_loss: 0.0012 - l1_loss: 0.1792\n",
      "Epoch 2861/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.4523 - class_loss: 0.0016 - l1_loss: 0.1451\n",
      "Epoch 2862/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.4231 - class_loss: 0.0024 - l1_loss: 0.1421\n",
      "Epoch 2863/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5824 - class_loss: 0.0027 - l1_loss: 0.1580\n",
      "Epoch 2864/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5757 - class_loss: 0.0131 - l1_loss: 0.1563\n",
      "Epoch 2865/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.9475 - class_loss: 0.0022 - l1_loss: 0.1945\n",
      "Epoch 2866/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.7771 - class_loss: 0.0014 - l1_loss: 0.1776\n",
      "Epoch 2867/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.6720 - class_loss: 0.0013 - l1_loss: 0.2671\n",
      "Epoch 2868/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.2476 - class_loss: 0.0042 - l1_loss: 0.2243\n",
      "Epoch 2869/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.0291 - class_loss: 0.0058 - l1_loss: 0.2023\n",
      "Epoch 2870/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.9155 - class_loss: 8.5053e-04 - l1_loss: 0.1915\n",
      "Epoch 2871/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7217 - class_loss: 0.0013 - l1_loss: 0.1720\n",
      "Epoch 2872/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.8810 - class_loss: 0.0035 - l1_loss: 0.1878\n",
      "Epoch 2873/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6561 - class_loss: 0.0018 - l1_loss: 0.1654\n",
      "Epoch 2874/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.6754 - class_loss: 6.2247e-04 - l1_loss: 0.1675\n",
      "Epoch 2875/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2860 - class_loss: 0.0032 - l1_loss: 0.1283\n",
      "Epoch 2876/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1141 - class_loss: 0.0026 - l1_loss: 0.1112\n",
      "Epoch 2877/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2949 - class_loss: 5.0215e-04 - l1_loss: 0.1294\n",
      "Epoch 2878/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2280 - class_loss: 0.0021 - l1_loss: 0.1226\n",
      "Epoch 2879/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3340 - class_loss: 0.0027 - l1_loss: 0.1331\n",
      "Epoch 2880/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1820 - class_loss: 0.0018 - l1_loss: 0.1180\n",
      "Epoch 2881/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1154 - class_loss: 9.6222e-04 - l1_loss: 0.1114\n",
      "Epoch 2882/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.2315 - class_loss: 0.0012 - l1_loss: 0.1230\n",
      "Epoch 2883/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2638 - class_loss: 0.0041 - l1_loss: 0.1260\n",
      "Epoch 2884/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3717 - class_loss: 0.0017 - l1_loss: 0.1370\n",
      "Epoch 2885/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5629 - class_loss: 0.0066 - l1_loss: 0.1556\n",
      "Epoch 2886/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2421 - class_loss: 0.0073 - l1_loss: 0.1235\n",
      "Epoch 2887/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.9667 - class_loss: 0.0018 - l1_loss: 0.0965\n",
      "Epoch 2888/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4842 - class_loss: 0.0031 - l1_loss: 0.1481\n",
      "Epoch 2889/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3280 - class_loss: 0.0028 - l1_loss: 0.1325\n",
      "Epoch 2890/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1290 - class_loss: 0.0091 - l1_loss: 0.1120\n",
      "Epoch 2891/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.6240 - class_loss: 0.0050 - l1_loss: 0.1619\n",
      "Epoch 2892/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.3560 - class_loss: 7.5878e-04 - l1_loss: 0.1355 0s - loss: 1.3411 - class_loss: 4.1343e-04 - l1_loss: 0.\n",
      "Epoch 2893/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7991 - class_loss: 6.9501e-04 - l1_loss: 0.1798\n",
      "Epoch 2894/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2334 - class_loss: 6.9103e-04 - l1_loss: 0.2233\n",
      "Epoch 2895/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9321 - class_loss: 0.0010 - l1_loss: 0.1931\n",
      "Epoch 2896/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6014 - class_loss: 0.0058 - l1_loss: 0.1596\n",
      "Epoch 2897/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4355 - class_loss: 0.0022 - l1_loss: 0.2433\n",
      "Epoch 2898/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4867 - class_loss: 5.8905e-04 - l1_loss: 0.2486\n",
      "Epoch 2899/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5267 - class_loss: 0.0027 - l1_loss: 0.1524\n",
      "Epoch 2900/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9870 - class_loss: 0.0017 - l1_loss: 0.2985\n",
      "Epoch 2901/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0100 - class_loss: 0.0020 - l1_loss: 0.2008\n",
      "Epoch 2902/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5703 - class_loss: 0.0053 - l1_loss: 0.2565\n",
      "Epoch 2903/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7748 - class_loss: 0.0013 - l1_loss: 0.2774\n",
      "Epoch 2904/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.4835 - class_loss: 4.4144e-04 - l1_loss: 0.4483\n",
      "Epoch 2905/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.7153 - class_loss: 0.0027 - l1_loss: 0.3713\n",
      "Epoch 2906/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2890 - class_loss: 0.0028 - l1_loss: 0.3286\n",
      "Epoch 2907/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9877 - class_loss: 0.0025 - l1_loss: 0.3985\n",
      "Epoch 2908/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.9184 - class_loss: 0.0057 - l1_loss: 0.3913\n",
      "Epoch 2909/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8694 - class_loss: 0.0104 - l1_loss: 0.4859\n",
      "Epoch 2910/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.4628 - class_loss: 0.0052 - l1_loss: 0.8458\n",
      "Epoch 2911/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.6237 - class_loss: 0.0021 - l1_loss: 0.7622\n",
      "Epoch 2912/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 9.7463 - class_loss: 2.9326e-04 - l1_loss: 0.9746\n",
      "Epoch 2913/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.8839 - class_loss: 7.7985e-04 - l1_loss: 1.4883\n",
      "Epoch 2914/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.5464 - class_loss: 0.0013 - l1_loss: 0.6545\n",
      "Epoch 2915/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.3212 - class_loss: 0.0225 - l1_loss: 0.9299\n",
      "Epoch 2916/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.4228 - class_loss: 0.0047 - l1_loss: 1.1418\n",
      "Epoch 2917/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 19.1378 - class_loss: 0.0013 - l1_loss: 1.9137\n",
      "Epoch 2918/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 15.0835 - class_loss: 0.0115 - l1_loss: 1.5072\n",
      "Epoch 2919/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.6196 - class_loss: 0.0031 - l1_loss: 0.9616\n",
      "Epoch 2920/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.7561 - class_loss: 0.0011 - l1_loss: 1.2755A: 0s - loss: 13.6559 - class_loss: 0.0012 - l1_loss: 1.365\n",
      "Epoch 2921/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.3703 - class_loss: 0.0023 - l1_loss: 1.4368\n",
      "Epoch 2922/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.9323 - class_loss: 0.0018 - l1_loss: 1.6931: 0s - loss: 12.0865 - class_loss: 0.0020 - l1_loss: 1.2\n",
      "Epoch 2923/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 25.0978 - class_loss: 7.1286e-04 - l1_loss: 2.5097\n",
      "Epoch 2924/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.2249 - class_loss: 0.0011 - l1_loss: 1.4224\n",
      "Epoch 2925/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.4199 - class_loss: 6.5135e-04 - l1_loss: 1.4419\n",
      "Epoch 2926/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 20.4265 - class_loss: 0.0012 - l1_loss: 2.0425\n",
      "Epoch 2927/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 17.0532 - class_loss: 0.0393 - l1_loss: 1.7014\n",
      "Epoch 2928/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 13.9871 - class_loss: 0.0035 - l1_loss: 1.3984\n",
      "Epoch 2929/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 18.2211 - class_loss: 0.0185 - l1_loss: 1.8203\n",
      "Epoch 2930/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 14.1642 - class_loss: 7.7063e-04 - l1_loss: 1.4163\n",
      "Epoch 2931/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 12.0965 - class_loss: 4.2739e-04 - l1_loss: 1.2096\n",
      "Epoch 2932/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 11.0592 - class_loss: 1.8210e-04 - l1_loss: 1.1059\n",
      "Epoch 2933/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 12.8232 - class_loss: 0.0051 - l1_loss: 1.2818\n",
      "Epoch 2934/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 10.1012 - class_loss: 0.0057 - l1_loss: 1.0096\n",
      "Epoch 2935/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 8.3071 - class_loss: 0.0036 - l1_loss: 0.8303\n",
      "Epoch 2936/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 9.2260 - class_loss: 9.9632e-04 - l1_loss: 0.9225\n",
      "Epoch 2937/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 6.0991 - class_loss: 0.0017 - l1_loss: 0.6097\n",
      "Epoch 2938/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.7614 - class_loss: 0.0012 - l1_loss: 0.5760\n",
      "Epoch 2939/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 6.2797 - class_loss: 0.0043 - l1_loss: 0.6275\n",
      "Epoch 2940/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.8394 - class_loss: 0.0027 - l1_loss: 0.5837\n",
      "Epoch 2941/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.7406 - class_loss: 0.0014 - l1_loss: 0.3739\n",
      "Epoch 2942/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.5549 - class_loss: 0.0188 - l1_loss: 0.5536\n",
      "Epoch 2943/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.9324 - class_loss: 0.0085 - l1_loss: 0.5924\n",
      "Epoch 2944/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 7.0493 - class_loss: 9.5835e-04 - l1_loss: 0.7048\n",
      "Epoch 2945/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 6.0709 - class_loss: 0.0044 - l1_loss: 0.6067\n",
      "Epoch 2946/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.8506 - class_loss: 0.0220 - l1_loss: 0.5829\n",
      "Epoch 2947/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.9723 - class_loss: 0.0086 - l1_loss: 0.4964\n",
      "Epoch 2948/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 5.6140 - class_loss: 4.6395e-04 - l1_loss: 0.5614\n",
      "Epoch 2949/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.3706 - class_loss: 3.3456e-04 - l1_loss: 0.5370\n",
      "Epoch 2950/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 6.9393 - class_loss: 0.0028 - l1_loss: 0.6937\n",
      "Epoch 2951/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 6.2935 - class_loss: 0.0017 - l1_loss: 0.6292\n",
      "Epoch 2952/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.6857 - class_loss: 0.0017 - l1_loss: 0.4684\n",
      "Epoch 2953/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 9.6412 - class_loss: 0.0046 - l1_loss: 0.9637\n",
      "Epoch 2954/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 5.9196 - class_loss: 0.0221 - l1_loss: 0.5897\n",
      "Epoch 2955/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.3563 - class_loss: 0.0077 - l1_loss: 0.4349\n",
      "Epoch 2956/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.0904 - class_loss: 0.0010 - l1_loss: 0.5089\n",
      "Epoch 2957/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.2387 - class_loss: 2.6385e-04 - l1_loss: 0.5238\n",
      "Epoch 2958/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.9688 - class_loss: 0.0036 - l1_loss: 0.4965\n",
      "Epoch 2959/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 5.6394 - class_loss: 0.0029 - l1_loss: 0.5636\n",
      "Epoch 2960/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 7.2770 - class_loss: 0.0096 - l1_loss: 0.7267\n",
      "Epoch 2961/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.5665 - class_loss: 0.0027 - l1_loss: 0.6564\n",
      "Epoch 2962/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.7309 - class_loss: 0.0019 - l1_loss: 0.4729\n",
      "Epoch 2963/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.2111 - class_loss: 0.0016 - l1_loss: 0.5210\n",
      "Epoch 2964/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.2360 - class_loss: 0.0022 - l1_loss: 0.3234\n",
      "Epoch 2965/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.1482 - class_loss: 0.0054 - l1_loss: 0.4143\n",
      "Epoch 2966/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.6214 - class_loss: 0.0246 - l1_loss: 0.3597\n",
      "Epoch 2967/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.2103 - class_loss: 0.0023 - l1_loss: 0.4208\n",
      "Epoch 2968/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.8437 - class_loss: 0.0049 - l1_loss: 0.3839\n",
      "Epoch 2969/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.9309 - class_loss: 0.0044 - l1_loss: 0.2926\n",
      "Epoch 2970/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.2878 - class_loss: 0.0083 - l1_loss: 0.4279\n",
      "Epoch 2971/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.8323 - class_loss: 0.0012 - l1_loss: 0.3831\n",
      "Epoch 2972/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.3630 - class_loss: 0.0019 - l1_loss: 0.4361\n",
      "Epoch 2973/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.8048 - class_loss: 0.0065 - l1_loss: 0.5798\n",
      "Epoch 2974/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.4967 - class_loss: 0.0032 - l1_loss: 0.3494\n",
      "Epoch 2975/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.5020 - class_loss: 0.0089 - l1_loss: 0.3493\n",
      "Epoch 2976/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.2759 - class_loss: 8.5713e-04 - l1_loss: 0.4275\n",
      "Epoch 2977/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.7118 - class_loss: 4.6737e-04 - l1_loss: 0.6711\n",
      "Epoch 2978/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 8.8274 - class_loss: 0.0013 - l1_loss: 0.8826\n",
      "Epoch 2979/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 12.5399 - class_loss: 4.6123e-04 - l1_loss: 1.2539\n",
      "Epoch 2980/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.3493 - class_loss: 0.0020 - l1_loss: 0.9347\n",
      "Epoch 2981/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 8.4406 - class_loss: 7.5329e-04 - l1_loss: 0.8440\n",
      "Epoch 2982/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.8171 - class_loss: 0.0032 - l1_loss: 0.3814\n",
      "Epoch 2983/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.2109 - class_loss: 0.0064 - l1_loss: 0.4204\n",
      "Epoch 2984/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.4584 - class_loss: 0.0045 - l1_loss: 0.3454\n",
      "Epoch 2985/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.8972 - class_loss: 0.0013 - l1_loss: 0.4896\n",
      "Epoch 2986/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.1459 - class_loss: 0.0082 - l1_loss: 0.5138\n",
      "Epoch 2987/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.1253 - class_loss: 4.5642e-04 - l1_loss: 0.6125\n",
      "Epoch 2988/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9869 - class_loss: 6.1270e-04 - l1_loss: 0.2986\n",
      "Epoch 2989/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1537 - class_loss: 4.9625e-04 - l1_loss: 0.2153\n",
      "Epoch 2990/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.4651 - class_loss: 6.6842e-04 - l1_loss: 0.3464\n",
      "Epoch 2991/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.7407 - class_loss: 0.0027 - l1_loss: 0.5738\n",
      "Epoch 2992/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.6143 - class_loss: 0.0046 - l1_loss: 0.3610\n",
      "Epoch 2993/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5328 - class_loss: 0.0089 - l1_loss: 0.3524\n",
      "Epoch 2994/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.0808 - class_loss: 0.0029 - l1_loss: 0.4078\n",
      "Epoch 2995/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.2504 - class_loss: 0.0038 - l1_loss: 0.3247\n",
      "Epoch 2996/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.4521 - class_loss: 0.0144 - l1_loss: 0.3438\n",
      "Epoch 2997/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8977 - class_loss: 0.0013 - l1_loss: 0.2896\n",
      "Epoch 2998/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9486 - class_loss: 0.0012 - l1_loss: 0.2947\n",
      "Epoch 2999/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.2778 - class_loss: 0.0054 - l1_loss: 0.2272\n",
      "Epoch 3000/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9235 - class_loss: 0.0048 - l1_loss: 0.2919\n",
      "Epoch 3001/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8704 - class_loss: 0.0016 - l1_loss: 0.2869\n",
      "Epoch 3002/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 32ms/step - loss: 2.5329 - class_loss: 0.0061 - l1_loss: 0.2527\n",
      "Epoch 3003/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2596 - class_loss: 0.0031 - l1_loss: 0.2257\n",
      "Epoch 3004/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9170 - class_loss: 6.2964e-04 - l1_loss: 0.2916\n",
      "Epoch 3005/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8736 - class_loss: 0.0014 - l1_loss: 0.2872\n",
      "Epoch 3006/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5704 - class_loss: 0.0047 - l1_loss: 0.2566\n",
      "Epoch 3007/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6573 - class_loss: 0.0040 - l1_loss: 0.2653\n",
      "Epoch 3008/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6078 - class_loss: 0.0103 - l1_loss: 0.3597\n",
      "Epoch 3009/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.1023 - class_loss: 0.0040 - l1_loss: 0.3098\n",
      "Epoch 3010/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.2051 - class_loss: 0.0031 - l1_loss: 0.2202\n",
      "Epoch 3011/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9768 - class_loss: 0.0017 - l1_loss: 0.2975\n",
      "Epoch 3012/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.2429 - class_loss: 5.2859e-04 - l1_loss: 0.3242\n",
      "Epoch 3013/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8977 - class_loss: 0.0024 - l1_loss: 0.2895\n",
      "Epoch 3014/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.3441 - class_loss: 0.0026 - l1_loss: 0.2342\n",
      "Epoch 3015/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.3597 - class_loss: 4.0091e-04 - l1_loss: 0.2359\n",
      "Epoch 3016/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.0956 - class_loss: 0.0010 - l1_loss: 0.2095\n",
      "Epoch 3017/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.5160 - class_loss: 0.0013 - l1_loss: 0.2515\n",
      "Epoch 3018/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6756 - class_loss: 0.0019 - l1_loss: 0.2674\n",
      "Epoch 3019/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0679 - class_loss: 0.0086 - l1_loss: 0.3059\n",
      "Epoch 3020/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0333 - class_loss: 0.0086 - l1_loss: 0.3025\n",
      "Epoch 3021/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4534 - class_loss: 0.0039 - l1_loss: 0.3450\n",
      "Epoch 3022/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9309 - class_loss: 0.0107 - l1_loss: 0.3920\n",
      "Epoch 3023/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9520 - class_loss: 0.0017 - l1_loss: 0.3950\n",
      "Epoch 3024/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 4.7207 - class_loss: 9.6646e-04 - l1_loss: 0.47 - 0s 31ms/step - loss: 4.5260 - class_loss: 0.0016 - l1_loss: 0.4524\n",
      "Epoch 3025/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.3043 - class_loss: 7.6460e-04 - l1_loss: 0.4304\n",
      "Epoch 3026/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.2961 - class_loss: 4.4717e-04 - l1_loss: 0.5296\n",
      "Epoch 3027/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.9438 - class_loss: 3.0084e-04 - l1_loss: 0.4944\n",
      "Epoch 3028/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.0109 - class_loss: 0.0010 - l1_loss: 0.4010\n",
      "Epoch 3029/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.5374 - class_loss: 0.0205 - l1_loss: 0.5517\n",
      "Epoch 3030/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.9652 - class_loss: 0.0015 - l1_loss: 0.2964\n",
      "Epoch 3031/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.7709 - class_loss: 4.3904e-04 - l1_loss: 0.3770\n",
      "Epoch 3032/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5953 - class_loss: 6.0143e-04 - l1_loss: 0.3595\n",
      "Epoch 3033/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5841 - class_loss: 0.0012 - l1_loss: 0.2583\n",
      "Epoch 3034/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1844 - class_loss: 0.0031 - l1_loss: 0.3181\n",
      "Epoch 3035/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6648 - class_loss: 0.0056 - l1_loss: 0.3659\n",
      "Epoch 3036/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1206 - class_loss: 0.0084 - l1_loss: 0.4112\n",
      "Epoch 3037/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.8704 - class_loss: 0.0011 - l1_loss: 0.3869\n",
      "Epoch 3038/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.5747 - class_loss: 0.0023 - l1_loss: 0.3572\n",
      "Epoch 3039/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.4939 - class_loss: 0.0029 - l1_loss: 0.4491\n",
      "Epoch 3040/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.5576 - class_loss: 0.0018 - l1_loss: 0.2556\n",
      "Epoch 3041/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.8518 - class_loss: 0.0012 - l1_loss: 0.3851\n",
      "Epoch 3042/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.3611 - class_loss: 0.0023 - l1_loss: 0.4359\n",
      "Epoch 3043/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.2940 - class_loss: 0.0091 - l1_loss: 0.4285\n",
      "Epoch 3044/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5444 - class_loss: 0.0051 - l1_loss: 0.3539\n",
      "Epoch 3045/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.6453 - class_loss: 0.0025 - l1_loss: 0.3643\n",
      "Epoch 3046/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2547 - class_loss: 0.0027 - l1_loss: 0.3252\n",
      "Epoch 3047/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.6168 - class_loss: 0.0052 - l1_loss: 0.2612\n",
      "Epoch 3048/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2902 - class_loss: 0.0013 - l1_loss: 0.3289\n",
      "Epoch 3049/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.4624 - class_loss: 0.0021 - l1_loss: 0.2460\n",
      "Epoch 3050/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4003 - class_loss: 0.0054 - l1_loss: 0.2395\n",
      "Epoch 3051/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8522 - class_loss: 0.0033 - l1_loss: 0.2849\n",
      "Epoch 3052/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1195 - class_loss: 0.0012 - l1_loss: 0.2118\n",
      "Epoch 3053/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.3496 - class_loss: 0.0039 - l1_loss: 0.3346\n",
      "Epoch 3054/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7192 - class_loss: 0.0016 - l1_loss: 0.2718 0s - loss: 2.6223 - class_loss: 0.0010 - l1_loss: 0.\n",
      "Epoch 3055/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5787 - class_loss: 0.0015 - l1_loss: 0.2577\n",
      "Epoch 3056/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2018 - class_loss: 0.0016 - l1_loss: 0.3200\n",
      "Epoch 3057/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.8858 - class_loss: 0.0014 - l1_loss: 0.3884\n",
      "Epoch 3058/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1799 - class_loss: 0.0044 - l1_loss: 0.4175\n",
      "Epoch 3059/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.8680 - class_loss: 0.0044 - l1_loss: 0.3864\n",
      "Epoch 3060/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.7127 - class_loss: 0.0111 - l1_loss: 0.3702\n",
      "Epoch 3061/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5491 - class_loss: 0.0041 - l1_loss: 0.3545 0s - loss: 3.4472 - class_loss: 0.0041 - l1_loss: 0.\n",
      "Epoch 3062/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.3696 - class_loss: 0.0012 - l1_loss: 0.3368\n",
      "Epoch 3063/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.3708 - class_loss: 0.0020 - l1_loss: 0.3369\n",
      "Epoch 3064/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4470 - class_loss: 0.0104 - l1_loss: 0.3437\n",
      "Epoch 3065/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3109 - class_loss: 0.0028 - l1_loss: 0.3308\n",
      "Epoch 3066/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0342 - class_loss: 4.6200e-04 - l1_loss: 0.4034\n",
      "Epoch 3067/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.8892 - class_loss: 0.0025 - l1_loss: 0.2887\n",
      "Epoch 3068/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5012 - class_loss: 0.0014 - l1_loss: 0.3500\n",
      "Epoch 3069/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5399 - class_loss: 0.0107 - l1_loss: 0.4529\n",
      "Epoch 3070/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.1362 - class_loss: 0.0054 - l1_loss: 0.4131\n",
      "Epoch 3071/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.4453 - class_loss: 0.0016 - l1_loss: 0.4444\n",
      "Epoch 3072/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7004 - class_loss: 0.0024 - l1_loss: 0.4698\n",
      "Epoch 3073/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.2837 - class_loss: 0.0020 - l1_loss: 0.4282\n",
      "Epoch 3074/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.1840 - class_loss: 0.0116 - l1_loss: 0.5172\n",
      "Epoch 3075/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9359 - class_loss: 9.2525e-04 - l1_loss: 0.3935\n",
      "Epoch 3076/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.9658 - class_loss: 8.3637e-04 - l1_loss: 0.5965\n",
      "Epoch 3077/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7235 - class_loss: 0.0012 - l1_loss: 0.4722\n",
      "Epoch 3078/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.5774 - class_loss: 0.0017 - l1_loss: 0.6576\n",
      "Epoch 3079/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.5765 - class_loss: 0.0059 - l1_loss: 0.7571 0s - loss: 4.6891 - class_loss: 0.0060 - l1_loss: \n",
      "Epoch 3080/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.5425 - class_loss: 0.0030 - l1_loss: 0.5540\n",
      "Epoch 3081/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.7497 - class_loss: 0.0027 - l1_loss: 0.5747\n",
      "Epoch 3082/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.1927 - class_loss: 5.5825e-04 - l1_loss: 0.5192\n",
      "Epoch 3083/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.9373 - class_loss: 9.0555e-04 - l1_loss: 0.7936\n",
      "Epoch 3084/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.7731 - class_loss: 0.0011 - l1_loss: 0.5772\n",
      "Epoch 3085/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.1685 - class_loss: 0.0013 - l1_loss: 0.5167\n",
      "Epoch 3086/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.4187 - class_loss: 0.0203 - l1_loss: 0.4398\n",
      "Epoch 3087/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.3162 - class_loss: 0.0081 - l1_loss: 0.5308\n",
      "Epoch 3088/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.5021 - class_loss: 9.0074e-04 - l1_loss: 0.5501\n",
      "Epoch 3089/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.1965 - class_loss: 4.9993e-04 - l1_loss: 0.4196\n",
      "Epoch 3090/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.9823 - class_loss: 9.2313e-04 - l1_loss: 0.3981\n",
      "Epoch 3091/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.1983 - class_loss: 0.0019 - l1_loss: 0.6196\n",
      "Epoch 3092/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.7631 - class_loss: 0.0047 - l1_loss: 0.5758\n",
      "Epoch 3093/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.6813 - class_loss: 0.0024 - l1_loss: 0.5679\n",
      "Epoch 3094/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7915 - class_loss: 0.0099 - l1_loss: 0.4782\n",
      "Epoch 3095/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.4651 - class_loss: 0.0053 - l1_loss: 0.5460\n",
      "Epoch 3096/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.6545 - class_loss: 0.0028 - l1_loss: 0.4652\n",
      "Epoch 3097/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.7100 - class_loss: 0.0030 - l1_loss: 0.4707\n",
      "Epoch 3098/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.8552 - class_loss: 0.0139 - l1_loss: 0.4841\n",
      "Epoch 3099/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.1086 - class_loss: 0.0041 - l1_loss: 0.6104 0s - loss: 6.0785 - class_loss: 0.0058 - l1_loss: 0.\n",
      "Epoch 3100/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.4903 - class_loss: 0.0017 - l1_loss: 0.4489\n",
      "Epoch 3101/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.1770 - class_loss: 0.0154 - l1_loss: 0.5162\n",
      "Epoch 3102/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7489 - class_loss: 3.6594e-04 - l1_loss: 0.4749\n",
      "Epoch 3103/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8816 - class_loss: 0.0024 - l1_loss: 0.3879\n",
      "Epoch 3104/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.5687 - class_loss: 0.0055 - l1_loss: 0.6563\n",
      "Epoch 3105/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.4791 - class_loss: 0.0407 - l1_loss: 0.9438\n",
      "Epoch 3106/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.9150 - class_loss: 1.6710e-04 - l1_loss: 0.7915\n",
      "Epoch 3107/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.1243 - class_loss: 6.4093e-04 - l1_loss: 1.5124\n",
      "Epoch 3108/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.5569 - class_loss: 6.3105e-04 - l1_loss: 0.7556\n",
      "Epoch 3109/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.2386 - class_loss: 4.6037e-04 - l1_loss: 0.6238\n",
      "Epoch 3110/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.4125 - class_loss: 4.8066e-04 - l1_loss: 0.9412\n",
      "Epoch 3111/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.2993 - class_loss: 0.0789 - l1_loss: 1.0220\n",
      "Epoch 3112/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.9161 - class_loss: 4.0942e-04 - l1_loss: 0.8916 0s - loss: 8.7493 - class_loss: 4.6560e-04 - l1_loss: 0.87\n",
      "Epoch 3113/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.2007 - class_loss: 4.1810e-04 - l1_loss: 1.0200\n",
      "Epoch 3114/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.7984 - class_loss: 0.1452 - l1_loss: 0.7653ETA: 0s - loss: 6.8183 - class_loss: 0.2318 - l1_loss: 0.6586\n",
      "Epoch 3115/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.4125 - class_loss: 0.0131 - l1_loss: 0.7399\n",
      "Epoch 3116/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.1578 - class_loss: 0.0035 - l1_loss: 0.8154\n",
      "Epoch 3117/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.9499 - class_loss: 3.4445e-04 - l1_loss: 0.9950\n",
      "Epoch 3118/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.3555 - class_loss: 0.0038 - l1_loss: 0.9352\n",
      "Epoch 3119/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.8574 - class_loss: 0.0024 - l1_loss: 1.0855 ETA: 0s - loss: 13.3814 - class_loss: 0.0062 - l1_loss: 1.3375\n",
      "Epoch 3120/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.9448 - class_loss: 6.9135e-04 - l1_loss: 0.8944\n",
      "Epoch 3121/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 22.6107 - class_loss: 0.8463 - l1_loss: 2.1764\n",
      "Epoch 3122/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 20.6862 - class_loss: 0.2218 - l1_loss: 2.0464\n",
      "Epoch 3123/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 32.6073 - class_loss: 0.4144 - l1_loss: 3.2193\n",
      "Epoch 3124/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 46.2632 - class_loss: 0.1756 - l1_loss: 4.6088\n",
      "Epoch 3125/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 50.9935 - class_loss: 0.0701 - l1_loss: 5.0923\n",
      "Epoch 3126/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 43.0324 - class_loss: 0.0049 - l1_loss: 4.3027\n",
      "Epoch 3127/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 55.1475 - class_loss: 0.0025 - l1_loss: 5.5145\n",
      "Epoch 3128/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 40.8741 - class_loss: 1.1780 - l1_loss: 3.9696\n",
      "Epoch 3129/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 33.2281 - class_loss: 0.1284 - l1_loss: 3.3100\n",
      "Epoch 3130/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 69.8969 - class_loss: 0.5439 - l1_loss: 6.9353\n",
      "Epoch 3131/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 28ms/step - loss: 56.4003 - class_loss: 0.0053 - l1_loss: 5.6395 ETA: 0s - loss: 60.0574 - class_loss: 0.0060 - l1_loss: 6.005\n",
      "Epoch 3132/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 72.5650 - class_loss: 0.0161 - l1_loss: 7.2549\n",
      "Epoch 3133/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 106.5863 - class_loss: 0.2987 - l1_loss: 10.6288\n",
      "Epoch 3134/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 68.9177 - class_loss: 0.0745 - l1_loss: 6.8843\n",
      "Epoch 3135/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 60.3780 - class_loss: 0.0019 - l1_loss: 6.0376\n",
      "Epoch 3136/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 43.4218 - class_loss: 0.0015 - l1_loss: 4.3420\n",
      "Epoch 3137/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 29.8681 - class_loss: 1.2943e-04 - l1_loss: 2.9868: 0s - loss: 29.8681 - class_loss: 1.2943e-04 - l1_loss: 2.986\n",
      "Epoch 3138/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 25.4830 - class_loss: 0.0010 - l1_loss: 2.5482\n",
      "Epoch 3139/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 22.1723 - class_loss: 0.3977 - l1_loss: 2.1775\n",
      "Epoch 3140/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 27.1058 - class_loss: 0.0333 - l1_loss: 2.7072\n",
      "Epoch 3141/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 19.3377 - class_loss: 0.0433 - l1_loss: 1.9294\n",
      "Epoch 3142/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 17.7185 - class_loss: 0.0090 - l1_loss: 1.7710\n",
      "Epoch 3143/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 13.5124 - class_loss: 0.0020 - l1_loss: 1.3510\n",
      "Epoch 3144/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.1047 - class_loss: 0.0029 - l1_loss: 1.3102\n",
      "Epoch 3145/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.7742 - class_loss: 0.0010 - l1_loss: 1.2773\n",
      "Epoch 3146/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.3390 - class_loss: 0.0041 - l1_loss: 1.1335\n",
      "Epoch 3147/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.5575 - class_loss: 0.0014 - l1_loss: 1.2556\n",
      "Epoch 3148/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.2028 - class_loss: 0.0355 - l1_loss: 1.3167\n",
      "Epoch 3149/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.0574 - class_loss: 0.0060 - l1_loss: 1.0051: 0s - loss: 8.7930 - class_loss: 0.0095 - l1_loss: 0.87\n",
      "Epoch 3150/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.2809 - class_loss: 0.0255 - l1_loss: 0.7255\n",
      "Epoch 3151/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.6997 - class_loss: 2.2172e-04 - l1_loss: 0.9699\n",
      "Epoch 3152/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.7230 - class_loss: 0.0152 - l1_loss: 0.4708\n",
      "Epoch 3153/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.9072 - class_loss: 0.0048 - l1_loss: 0.5902\n",
      "Epoch 3154/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.3189 - class_loss: 0.0041 - l1_loss: 0.4315\n",
      "Epoch 3155/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.6487 - class_loss: 6.8845e-04 - l1_loss: 0.5648\n",
      "Epoch 3156/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.4234 - class_loss: 0.0026 - l1_loss: 0.5421\n",
      "Epoch 3157/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8867 - class_loss: 0.0255 - l1_loss: 0.2861\n",
      "Epoch 3158/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.6377 - class_loss: 0.0051 - l1_loss: 0.3633\n",
      "Epoch 3159/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4965 - class_loss: 6.3166e-04 - l1_loss: 0.3496\n",
      "Epoch 3160/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0396 - class_loss: 3.3614e-04 - l1_loss: 0.3039\n",
      "Epoch 3161/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.3435 - class_loss: 0.0019 - l1_loss: 0.3342\n",
      "Epoch 3162/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2099 - class_loss: 0.0011 - l1_loss: 0.2209\n",
      "Epoch 3163/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9891 - class_loss: 0.0012 - l1_loss: 0.2988\n",
      "Epoch 3164/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.5755 - class_loss: 0.0058 - l1_loss: 0.2570\n",
      "Epoch 3165/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.1439 - class_loss: 0.0063 - l1_loss: 0.2138\n",
      "Epoch 3166/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8573 - class_loss: 0.0030 - l1_loss: 0.1854\n",
      "Epoch 3167/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.7716 - class_loss: 0.0031 - l1_loss: 0.1769\n",
      "Epoch 3168/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9390 - class_loss: 0.0053 - l1_loss: 0.1934\n",
      "Epoch 3169/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4328 - class_loss: 0.0045 - l1_loss: 0.1428\n",
      "Epoch 3170/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.2332 - class_loss: 0.0031 - l1_loss: 0.1230\n",
      "Epoch 3171/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2063 - class_loss: 0.0010 - l1_loss: 0.1205\n",
      "Epoch 3172/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2581 - class_loss: 0.0026 - l1_loss: 0.1256\n",
      "Epoch 3173/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8825 - class_loss: 0.0056 - l1_loss: 0.0877\n",
      "Epoch 3174/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4154 - class_loss: 0.0092 - l1_loss: 0.1406\n",
      "Epoch 3175/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3448 - class_loss: 0.0012 - l1_loss: 0.1344\n",
      "Epoch 3176/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5899 - class_loss: 7.9047e-04 - l1_loss: 0.1589\n",
      "Epoch 3177/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.5500 - class_loss: 0.0017 - l1_loss: 0.1548\n",
      "Epoch 3178/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6602 - class_loss: 0.0043 - l1_loss: 0.1656\n",
      "Epoch 3179/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7826 - class_loss: 0.0092 - l1_loss: 0.1773\n",
      "Epoch 3180/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4249 - class_loss: 0.0033 - l1_loss: 0.1422\n",
      "Epoch 3181/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.3152 - class_loss: 0.0023 - l1_loss: 0.1313\n",
      "Epoch 3182/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1160 - class_loss: 7.0381e-04 - l1_loss: 0.2115\n",
      "Epoch 3183/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6547 - class_loss: 2.3097e-04 - l1_loss: 0.2654\n",
      "Epoch 3184/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1667 - class_loss: 5.6722e-04 - l1_loss: 0.2166\n",
      "Epoch 3185/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7306 - class_loss: 8.5632e-04 - l1_loss: 0.1730\n",
      "Epoch 3186/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9550 - class_loss: 0.0015 - l1_loss: 0.1953\n",
      "Epoch 3187/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0423 - class_loss: 0.0019 - l1_loss: 0.2040\n",
      "Epoch 3188/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0297 - class_loss: 0.0015 - l1_loss: 0.2028ETA: 0s - loss: 2.1296 - class_loss: 0.0015 - l1_loss: 0.\n",
      "Epoch 3189/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8952 - class_loss: 6.9710e-04 - l1_loss: 0.2895\n",
      "Epoch 3190/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.8149 - class_loss: 0.0019 - l1_loss: 0.1813 0s - loss: 1.8033 - class_loss: 0.0018 - l1_loss: 0.18\n",
      "Epoch 3191/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5719 - class_loss: 0.0047 - l1_loss: 0.1567\n",
      "Epoch 3192/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2506 - class_loss: 0.0034 - l1_loss: 0.1247\n",
      "Epoch 3193/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0581 - class_loss: 0.0027 - l1_loss: 0.1055\n",
      "Epoch 3194/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0078 - class_loss: 0.0018 - l1_loss: 0.1006\n",
      "Epoch 3195/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1906 - class_loss: 0.0049 - l1_loss: 0.1186\n",
      "Epoch 3196/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0112 - class_loss: 0.0024 - l1_loss: 0.1009\n",
      "Epoch 3197/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9748 - class_loss: 0.0017 - l1_loss: 0.0973\n",
      "Epoch 3198/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1113 - class_loss: 0.0066 - l1_loss: 0.1105\n",
      "Epoch 3199/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9122 - class_loss: 0.0055 - l1_loss: 0.0907\n",
      "Epoch 3200/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.7938 - class_loss: 0.0014 - l1_loss: 0.0792ETA: 0s - loss: 0.7962 - class_loss: 4.9630e-04 - l1_loss: 0.\n",
      "Epoch 3201/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9730 - class_loss: 7.9026e-04 - l1_loss: 0.0972\n",
      "Epoch 3202/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0110 - class_loss: 0.0018 - l1_loss: 0.1009\n",
      "Epoch 3203/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9489 - class_loss: 0.0010 - l1_loss: 0.0948\n",
      "Epoch 3204/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.8142 - class_loss: 8.2945e-04 - l1_loss: 0.0813\n",
      "Epoch 3205/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9605 - class_loss: 0.0014 - l1_loss: 0.0959\n",
      "Epoch 3206/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9093 - class_loss: 0.0013 - l1_loss: 0.0908\n",
      "Epoch 3207/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8425 - class_loss: 0.0016 - l1_loss: 0.0841\n",
      "Epoch 3208/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8967 - class_loss: 0.0018 - l1_loss: 0.0895\n",
      "Epoch 3209/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7352 - class_loss: 0.0020 - l1_loss: 0.0733 0s - loss: 0.6753 - class_loss: 0.0025 - l1_loss: 0.\n",
      "Epoch 3210/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8016 - class_loss: 0.0014 - l1_loss: 0.0800\n",
      "Epoch 3211/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9479 - class_loss: 0.0039 - l1_loss: 0.0944\n",
      "Epoch 3212/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.7993 - class_loss: 0.0024 - l1_loss: 0.0797\n",
      "Epoch 3213/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.6532 - class_loss: 7.9051e-04 - l1_loss: 0.0652\n",
      "Epoch 3214/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8521 - class_loss: 4.9586e-04 - l1_loss: 0.0852\n",
      "Epoch 3215/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8023 - class_loss: 4.8957e-04 - l1_loss: 0.0802\n",
      "Epoch 3216/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7373 - class_loss: 0.0023 - l1_loss: 0.0735\n",
      "Epoch 3217/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8822 - class_loss: 0.0029 - l1_loss: 0.0879\n",
      "Epoch 3218/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.6497 - class_loss: 0.0026 - l1_loss: 0.0647\n",
      "Epoch 3219/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8880 - class_loss: 0.0036 - l1_loss: 0.0884\n",
      "Epoch 3220/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.7464 - class_loss: 0.0025 - l1_loss: 0.0744\n",
      "Epoch 3221/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9620 - class_loss: 0.0022 - l1_loss: 0.0960\n",
      "Epoch 3222/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0016 - class_loss: 0.0026 - l1_loss: 0.0999\n",
      "Epoch 3223/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2839 - class_loss: 0.0059 - l1_loss: 0.1278\n",
      "Epoch 3224/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0905 - class_loss: 0.0033 - l1_loss: 0.1087\n",
      "Epoch 3225/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.2260 - class_loss: 0.0042 - l1_loss: 0.1222\n",
      "Epoch 3226/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.3451 - class_loss: 0.0027 - l1_loss: 0.1342\n",
      "Epoch 3227/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1335 - class_loss: 0.0025 - l1_loss: 0.1131\n",
      "Epoch 3228/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1752 - class_loss: 0.0048 - l1_loss: 0.1170\n",
      "Epoch 3229/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1491 - class_loss: 0.0022 - l1_loss: 0.1147 0s - loss: 0.9409 - class_loss: 0.0050 - l1_loss: \n",
      "Epoch 3230/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1246 - class_loss: 6.3342e-04 - l1_loss: 0.1124 0s - loss: 1.1065 - class_loss: 7.4062e-04 - l1_loss: 0.\n",
      "Epoch 3231/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0105 - class_loss: 5.5301e-04 - l1_loss: 0.1010\n",
      "Epoch 3232/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1554 - class_loss: 0.0013 - l1_loss: 0.1154\n",
      "Epoch 3233/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2411 - class_loss: 9.6034e-04 - l1_loss: 0.1240\n",
      "Epoch 3234/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.2417 - class_loss: 0.0014 - l1_loss: 0.1240\n",
      "Epoch 3235/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2621 - class_loss: 0.0020 - l1_loss: 0.1260\n",
      "Epoch 3236/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8343 - class_loss: 0.0015 - l1_loss: 0.0833\n",
      "Epoch 3237/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8033 - class_loss: 0.0019 - l1_loss: 0.0801\n",
      "Epoch 3238/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.6585 - class_loss: 0.0021 - l1_loss: 0.0656\n",
      "Epoch 3239/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7953 - class_loss: 0.0020 - l1_loss: 0.0793\n",
      "Epoch 3240/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9078 - class_loss: 0.0019 - l1_loss: 0.0906\n",
      "Epoch 3241/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0234 - class_loss: 0.0032 - l1_loss: 0.1020\n",
      "Epoch 3242/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.8928 - class_loss: 0.0046 - l1_loss: 0.0888\n",
      "Epoch 3243/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9674 - class_loss: 0.0052 - l1_loss: 0.0962\n",
      "Epoch 3244/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7993 - class_loss: 0.0048 - l1_loss: 0.0794\n",
      "Epoch 3245/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7577 - class_loss: 0.0021 - l1_loss: 0.0756\n",
      "Epoch 3246/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.7625 - class_loss: 8.8361e-04 - l1_loss: 0.0762\n",
      "Epoch 3247/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8833 - class_loss: 0.0011 - l1_loss: 0.0882\n",
      "Epoch 3248/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7934 - class_loss: 0.0017 - l1_loss: 0.0792\n",
      "Epoch 3249/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3226 - class_loss: 0.0010 - l1_loss: 0.1322\n",
      "Epoch 3250/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1773 - class_loss: 7.2754e-04 - l1_loss: 0.1177\n",
      "Epoch 3251/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3594 - class_loss: 0.0015 - l1_loss: 0.1358\n",
      "Epoch 3252/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4691 - class_loss: 0.0043 - l1_loss: 0.2465\n",
      "Epoch 3253/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.5530 - class_loss: 0.0049 - l1_loss: 0.3548\n",
      "Epoch 3254/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.7246 - class_loss: 0.0099 - l1_loss: 0.4715\n",
      "Epoch 3255/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.5958 - class_loss: 0.0063 - l1_loss: 0.3590\n",
      "Epoch 3256/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.3941 - class_loss: 0.0046 - l1_loss: 0.6390\n",
      "Epoch 3257/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 7.1586 - class_loss: 0.0102 - l1_loss: 0.7148\n",
      "Epoch 3258/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 6.5842 - class_loss: 6.8064e-04 - l1_loss: 0.6584\n",
      "Epoch 3259/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 7.1972 - class_loss: 3.0251e-04 - l1_loss: 0.7197\n",
      "Epoch 3260/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 32ms/step - loss: 7.5575 - class_loss: 7.5661e-04 - l1_loss: 0.7557\n",
      "Epoch 3261/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.9265 - class_loss: 6.6678e-04 - l1_loss: 0.5926\n",
      "Epoch 3262/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 9.1753 - class_loss: 0.0034 - l1_loss: 0.9172\n",
      "Epoch 3263/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 10.5504 - class_loss: 0.0104 - l1_loss: 1.0540\n",
      "Epoch 3264/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.4233 - class_loss: 0.0027 - l1_loss: 1.0421\n",
      "Epoch 3265/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.7794 - class_loss: 0.0036 - l1_loss: 0.9776\n",
      "Epoch 3266/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 10.3371 - class_loss: 0.0034 - l1_loss: 1.0334\n",
      "Epoch 3267/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.7888 - class_loss: 0.0127 - l1_loss: 0.9776\n",
      "Epoch 3268/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.8625 - class_loss: 0.0336 - l1_loss: 0.8829\n",
      "Epoch 3269/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 11.2992 - class_loss: 1.0999e-04 - l1_loss: 1.1299\n",
      "Epoch 3270/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 10.5913 - class_loss: 1.3654e-04 - l1_loss: 1.0591\n",
      "Epoch 3271/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.0517 - class_loss: 3.6505e-04 - l1_loss: 1.0051\n",
      "Epoch 3272/5000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 11.9826 - class_loss: 2.3664e-04 - l1_loss: 1.1982\n",
      "Epoch 3273/5000\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 6.2884 - class_loss: 0.0013 - l1_loss: 0.6287\n",
      "Epoch 3274/5000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 5.2561 - class_loss: 0.0020 - l1_loss: 0.5254\n",
      "Epoch 3275/5000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 4.8973 - class_loss: 5.1194e-04 - l1_loss: 0.4897\n",
      "Epoch 3276/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 5.6378 - class_loss: 0.0022 - l1_loss: 0.5636\n",
      "Epoch 3277/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 8.0367 - class_loss: 0.0056 - l1_loss: 0.8031\n",
      "Epoch 3278/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 8.0460 - class_loss: 5.3622e-04 - l1_loss: 0.8045\n",
      "Epoch 3279/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.5459 - class_loss: 3.2845e-04 - l1_loss: 0.4546\n",
      "Epoch 3280/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.4218 - class_loss: 0.0030 - l1_loss: 0.5419\n",
      "Epoch 3281/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 8.5284 - class_loss: 0.0154 - l1_loss: 0.8513\n",
      "Epoch 3282/5000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 10.2677 - class_loss: 0.0046 - l1_loss: 1.0263\n",
      "Epoch 3283/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 6.1837 - class_loss: 0.0042 - l1_loss: 0.6179\n",
      "Epoch 3284/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 7.8702 - class_loss: 0.0099 - l1_loss: 0.7860\n",
      "Epoch 3285/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 8.9233 - class_loss: 0.0011 - l1_loss: 0.8922\n",
      "Epoch 3286/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.2871 - class_loss: 0.0014 - l1_loss: 0.9286\n",
      "Epoch 3287/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 8.9231 - class_loss: 0.0058 - l1_loss: 0.8917\n",
      "Epoch 3288/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.6003 - class_loss: 0.0017 - l1_loss: 0.8599\n",
      "Epoch 3289/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 6.3069 - class_loss: 2.9746e-04 - l1_loss: 0.6307\n",
      "Epoch 3290/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 7.7212 - class_loss: 3.3770e-04 - l1_loss: 0.7721\n",
      "Epoch 3291/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.0162 - class_loss: 0.0055 - l1_loss: 0.7011\n",
      "Epoch 3292/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.3073 - class_loss: 0.0099 - l1_loss: 0.5297\n",
      "Epoch 3293/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.4469 - class_loss: 0.0133 - l1_loss: 0.7434\n",
      "Epoch 3294/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.2044 - class_loss: 0.0131 - l1_loss: 0.7191\n",
      "Epoch 3295/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 6.9921 - class_loss: 0.0026 - l1_loss: 0.6990\n",
      "Epoch 3296/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.8680 - class_loss: 6.4450e-04 - l1_loss: 0.5867\n",
      "Epoch 3297/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.7786 - class_loss: 0.0018 - l1_loss: 0.5777\n",
      "Epoch 3298/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.7881 - class_loss: 0.0013 - l1_loss: 0.7787- ETA: 0s - loss: 9.2916 - class_loss: 0.0012 - l1_loss: 0.9290 \n",
      "Epoch 3299/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.2308 - class_loss: 6.5006e-04 - l1_loss: 0.8230\n",
      "Epoch 3300/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.2917 - class_loss: 0.0168 - l1_loss: 0.6275\n",
      "Epoch 3301/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.5090 - class_loss: 0.0053 - l1_loss: 0.6504\n",
      "Epoch 3302/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.4796 - class_loss: 0.0042 - l1_loss: 0.5475\n",
      "Epoch 3303/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.8536 - class_loss: 0.0036 - l1_loss: 0.5850\n",
      "Epoch 3304/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.7897 - class_loss: 0.0039 - l1_loss: 0.5786\n",
      "Epoch 3305/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.1315 - class_loss: 0.0011 - l1_loss: 0.7130\n",
      "Epoch 3306/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.0276 - class_loss: 5.4671e-04 - l1_loss: 0.6027\n",
      "Epoch 3307/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.1948 - class_loss: 4.9662e-04 - l1_loss: 0.6194\n",
      "Epoch 3308/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.6339 - class_loss: 9.7712e-04 - l1_loss: 0.4633\n",
      "Epoch 3309/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.3935 - class_loss: 0.0168 - l1_loss: 0.3377\n",
      "Epoch 3310/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.8001 - class_loss: 0.0034 - l1_loss: 0.4797\n",
      "Epoch 3311/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.7854 - class_loss: 6.9462e-04 - l1_loss: 0.3785\n",
      "Epoch 3312/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7857 - class_loss: 0.0012 - l1_loss: 0.4785\n",
      "Epoch 3313/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.9673 - class_loss: 0.0048 - l1_loss: 0.4963: 0s - loss: 5.2365 - class_loss: 0.0047 - l1_loss: 0.\n",
      "Epoch 3314/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.2018 - class_loss: 9.5443e-04 - l1_loss: 0.6201\n",
      "Epoch 3315/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.6398 - class_loss: 5.0133e-04 - l1_loss: 0.7639\n",
      "Epoch 3316/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.3760 - class_loss: 0.0025 - l1_loss: 0.4373\n",
      "Epoch 3317/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.3549 - class_loss: 4.6593e-04 - l1_loss: 0.6354\n",
      "Epoch 3318/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.7301 - class_loss: 5.5458e-04 - l1_loss: 0.7730\n",
      "Epoch 3319/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.0599 - class_loss: 5.7355e-04 - l1_loss: 0.7059\n",
      "Epoch 3320/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.5438 - class_loss: 0.0210 - l1_loss: 0.5523\n",
      "Epoch 3321/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.2244 - class_loss: 0.0312 - l1_loss: 0.7193\n",
      "Epoch 3322/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 13.3202 - class_loss: 0.0118 - l1_loss: 1.3308\n",
      "Epoch 3323/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.6190 - class_loss: 2.1587e-04 - l1_loss: 0.7619\n",
      "Epoch 3324/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.2804 - class_loss: 3.7606e-04 - l1_loss: 0.6280\n",
      "Epoch 3325/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.7720 - class_loss: 0.0023 - l1_loss: 0.5770\n",
      "Epoch 3326/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.2944 - class_loss: 0.0067 - l1_loss: 0.7288\n",
      "Epoch 3327/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.1156 - class_loss: 0.0036 - l1_loss: 1.6112\n",
      "Epoch 3328/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.1598 - class_loss: 0.0027 - l1_loss: 0.8157 0s - loss: 10.2659 - class_loss: 8.6203e-04 - l1_loss: 1\n",
      "Epoch 3329/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.1484 - class_loss: 0.0124 - l1_loss: 0.8136\n",
      "Epoch 3330/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.3462 - class_loss: 0.0269 - l1_loss: 0.7319\n",
      "Epoch 3331/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.5800 - class_loss: 0.0033 - l1_loss: 0.9577\n",
      "Epoch 3332/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.7811 - class_loss: 0.0037 - l1_loss: 0.6777\n",
      "Epoch 3333/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.3580 - class_loss: 0.0031 - l1_loss: 0.7355\n",
      "Epoch 3334/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.5711 - class_loss: 0.0013 - l1_loss: 0.8570 0s - loss: 8.7588 - class_loss: 0.0012 - l1_loss: 0.\n",
      "Epoch 3335/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.3842 - class_loss: 7.5843e-04 - l1_loss: 1.03830s - loss: 10.8158 - class_loss: 4.2188e-04 - l1_loss: 1.081\n",
      "Epoch 3336/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.6434 - class_loss: 7.2598e-04 - l1_loss: 0.8643 0s - loss: 8.9852 - class_loss: 7.9247e-04 - l1_loss: 0.89\n",
      "Epoch 3337/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 14.3313 - class_loss: 0.0055 - l1_loss: 1.4326 ETA: 0s - loss: 12.2417 - class_loss: 0.0021 - l1_loss: 1.22\n",
      "Epoch 3338/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 14.6659 - class_loss: 0.0040 - l1_loss: 1.4662\n",
      "Epoch 3339/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 19.6269 - class_loss: 0.0280 - l1_loss: 1.9599\n",
      "Epoch 3340/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.6480 - class_loss: 0.0021 - l1_loss: 1.5646\n",
      "Epoch 3341/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.9021 - class_loss: 0.0053 - l1_loss: 1.3897\n",
      "Epoch 3342/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.5398 - class_loss: 4.4093e-05 - l1_loss: 1.2540\n",
      "Epoch 3343/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 13.0365 - class_loss: 0.0016 - l1_loss: 1.3035\n",
      "Epoch 3344/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 14.4845 - class_loss: 0.0298 - l1_loss: 1.4455\n",
      "Epoch 3345/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 23.3809 - class_loss: 0.0017 - l1_loss: 2.3379\n",
      "Epoch 3346/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 22.1749 - class_loss: 0.0026 - l1_loss: 2.2172\n",
      "Epoch 3347/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 16.7301 - class_loss: 0.0017 - l1_loss: 1.6728\n",
      "Epoch 3348/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 15.5896 - class_loss: 0.0048 - l1_loss: 1.5585\n",
      "Epoch 3349/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 16.5033 - class_loss: 0.0057 - l1_loss: 1.6498\n",
      "Epoch 3350/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 16.1983 - class_loss: 0.0169 - l1_loss: 1.6181\n",
      "Epoch 3351/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 21.0466 - class_loss: 0.0120 - l1_loss: 2.1035\n",
      "Epoch 3352/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 21.8914 - class_loss: 0.0141 - l1_loss: 2.1877\n",
      "Epoch 3353/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 32.9626 - class_loss: 7.3232e-04 - l1_loss: 3.2962\n",
      "Epoch 3354/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 21.2448 - class_loss: 1.3578e-04 - l1_loss: 2.1245\n",
      "Epoch 3355/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 33.9642 - class_loss: 0.0026 - l1_loss: 3.3962\n",
      "Epoch 3356/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 23.3238 - class_loss: 0.0022 - l1_loss: 2.3322\n",
      "Epoch 3357/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 21.2472 - class_loss: 0.0016 - l1_loss: 2.1246\n",
      "Epoch 3358/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 24.8722 - class_loss: 0.0012 - l1_loss: 2.4871\n",
      "Epoch 3359/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 21.1290 - class_loss: 0.0373 - l1_loss: 2.1092\n",
      "Epoch 3360/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 34.8891 - class_loss: 0.0077 - l1_loss: 3.4881\n",
      "Epoch 3361/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 18.1255 - class_loss: 0.0289 - l1_loss: 1.8097\n",
      "Epoch 3362/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 19.5921 - class_loss: 0.0012 - l1_loss: 1.9591\n",
      "Epoch 3363/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.8052 - class_loss: 0.0046 - l1_loss: 1.4801\n",
      "Epoch 3364/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.7471 - class_loss: 0.0081 - l1_loss: 1.3739\n",
      "Epoch 3365/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 10.9038 - class_loss: 0.0565 - l1_loss: 1.0847\n",
      "Epoch 3366/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 12.8539 - class_loss: 0.0017 - l1_loss: 1.2852\n",
      "Epoch 3367/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 7.6706 - class_loss: 4.6665e-04 - l1_loss: 0.7670\n",
      "Epoch 3368/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.8105 - class_loss: 1.7995e-04 - l1_loss: 0.4810\n",
      "Epoch 3369/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.3763 - class_loss: 0.0125 - l1_loss: 0.7364\n",
      "Epoch 3370/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.8615 - class_loss: 0.0171 - l1_loss: 0.5844\n",
      "Epoch 3371/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.7877 - class_loss: 0.0010 - l1_loss: 0.9787\n",
      "Epoch 3372/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 8.1971 - class_loss: 0.0012 - l1_loss: 0.8196\n",
      "Epoch 3373/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 9.5990 - class_loss: 8.4913e-04 - l1_loss: 0.9598\n",
      "Epoch 3374/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 10.3648 - class_loss: 0.0022 - l1_loss: 1.0363\n",
      "Epoch 3375/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 6.9382 - class_loss: 0.0038 - l1_loss: 0.6934\n",
      "Epoch 3376/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.6235 - class_loss: 0.0043 - l1_loss: 0.6619\n",
      "Epoch 3377/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 6.0379 - class_loss: 0.0017 - l1_loss: 0.6036\n",
      "Epoch 3378/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 4.9497 - class_loss: 0.0038 - l1_loss: 0.4946\n",
      "Epoch 3379/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 3.9165 - class_loss: 0.0022 - l1_loss: 0.3914\n",
      "Epoch 3380/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 4.5006 - class_loss: 0.0080 - l1_loss: 0.4493\n",
      "Epoch 3381/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 5.6032 - class_loss: 0.0191 - l1_loss: 0.5584\n",
      "Epoch 3382/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.1311 - class_loss: 6.6193e-04 - l1_loss: 0.4130\n",
      "Epoch 3383/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 4.9452 - class_loss: 0.0434 - l1_loss: 0.4902\n",
      "Epoch 3384/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.8239 - class_loss: 4.9538e-04 - l1_loss: 0.3823\n",
      "Epoch 3385/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.7183 - class_loss: 5.2854e-04 - l1_loss: 0.3718\n",
      "Epoch 3386/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 3.3334 - class_loss: 0.0015 - l1_loss: 0.3332\n",
      "Epoch 3387/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.3440 - class_loss: 0.0018 - l1_loss: 0.3342\n",
      "Epoch 3388/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 3.1187 - class_loss: 0.0076 - l1_loss: 0.3111\n",
      "Epoch 3389/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 33ms/step - loss: 3.3072 - class_loss: 0.0156 - l1_loss: 0.3292\n",
      "Epoch 3390/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.0210 - class_loss: 0.0031 - l1_loss: 0.3018\n",
      "Epoch 3391/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.4184 - class_loss: 0.0041 - l1_loss: 0.2414\n",
      "Epoch 3392/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.1019 - class_loss: 0.0220 - l1_loss: 0.2080\n",
      "Epoch 3393/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4414 - class_loss: 5.8206e-04 - l1_loss: 0.2441\n",
      "Epoch 3394/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5665 - class_loss: 2.0664e-04 - l1_loss: 0.2566\n",
      "Epoch 3395/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5333 - class_loss: 0.0031 - l1_loss: 0.2530\n",
      "Epoch 3396/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8123 - class_loss: 0.0016 - l1_loss: 0.2811\n",
      "Epoch 3397/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2884 - class_loss: 0.0042 - l1_loss: 0.2284\n",
      "Epoch 3398/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.8042 - class_loss: 0.0036 - l1_loss: 0.1801\n",
      "Epoch 3399/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.5845 - class_loss: 0.0037 - l1_loss: 0.1581\n",
      "Epoch 3400/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6293 - class_loss: 0.0025 - l1_loss: 0.1627\n",
      "Epoch 3401/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.4402 - class_loss: 0.0028 - l1_loss: 0.1437\n",
      "Epoch 3402/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.6576 - class_loss: 0.0043 - l1_loss: 0.1653\n",
      "Epoch 3403/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.7747 - class_loss: 0.0045 - l1_loss: 0.1770\n",
      "Epoch 3404/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.1696 - class_loss: 0.0016 - l1_loss: 0.1168\n",
      "Epoch 3405/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.3056 - class_loss: 0.0015 - l1_loss: 0.1304\n",
      "Epoch 3406/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.2564 - class_loss: 0.0015 - l1_loss: 0.1255\n",
      "Epoch 3407/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0590 - class_loss: 0.0013 - l1_loss: 0.1058\n",
      "Epoch 3408/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.9503 - class_loss: 0.0014 - l1_loss: 0.0949\n",
      "Epoch 3409/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.8862 - class_loss: 0.0015 - l1_loss: 0.0885\n",
      "Epoch 3410/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1912 - class_loss: 9.4901e-04 - l1_loss: 0.1190\n",
      "Epoch 3411/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0045 - class_loss: 0.0038 - l1_loss: 0.1001\n",
      "Epoch 3412/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1920 - class_loss: 0.0034 - l1_loss: 0.1189\n",
      "Epoch 3413/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0170 - class_loss: 0.0020 - l1_loss: 0.1015\n",
      "Epoch 3414/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.4870 - class_loss: 0.0026 - l1_loss: 0.1484\n",
      "Epoch 3415/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9669 - class_loss: 0.0075 - l1_loss: 0.1959\n",
      "Epoch 3416/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0351 - class_loss: 0.0039 - l1_loss: 0.3031\n",
      "Epoch 3417/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.2031 - class_loss: 0.0017 - l1_loss: 0.3201\n",
      "Epoch 3418/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0162 - class_loss: 0.0023 - l1_loss: 0.3014\n",
      "Epoch 3419/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6796 - class_loss: 0.0019 - l1_loss: 0.3678\n",
      "Epoch 3420/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4802 - class_loss: 0.0011 - l1_loss: 0.2479\n",
      "Epoch 3421/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.9865 - class_loss: 0.0050 - l1_loss: 0.1982\n",
      "Epoch 3422/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8133 - class_loss: 0.0024 - l1_loss: 0.1811\n",
      "Epoch 3423/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.9036 - class_loss: 0.0013 - l1_loss: 0.1902\n",
      "Epoch 3424/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6528 - class_loss: 0.0040 - l1_loss: 0.1649\n",
      "Epoch 3425/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.5184 - class_loss: 0.0060 - l1_loss: 0.2512\n",
      "Epoch 3426/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9291 - class_loss: 0.0023 - l1_loss: 0.2927\n",
      "Epoch 3427/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.2749 - class_loss: 0.0046 - l1_loss: 0.3270\n",
      "Epoch 3428/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2125 - class_loss: 0.0072 - l1_loss: 0.2205\n",
      "Epoch 3429/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.2136 - class_loss: 0.0031 - l1_loss: 0.4210\n",
      "Epoch 3430/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.8883 - class_loss: 0.0043 - l1_loss: 0.5884\n",
      "Epoch 3431/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2035 - class_loss: 0.0025 - l1_loss: 0.4201\n",
      "Epoch 3432/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.1671 - class_loss: 0.0024 - l1_loss: 0.5165\n",
      "Epoch 3433/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.8151 - class_loss: 0.0031 - l1_loss: 0.3812\n",
      "Epoch 3434/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.9233 - class_loss: 0.0032 - l1_loss: 0.4920\n",
      "Epoch 3435/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.4655 - class_loss: 0.0090 - l1_loss: 0.5457\n",
      "Epoch 3436/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2943 - class_loss: 0.0084 - l1_loss: 0.2286\n",
      "Epoch 3437/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.6808 - class_loss: 8.4413e-04 - l1_loss: 0.3680\n",
      "Epoch 3438/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7707 - class_loss: 0.0066 - l1_loss: 0.3764\n",
      "Epoch 3439/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.9964 - class_loss: 0.0146 - l1_loss: 0.3982\n",
      "Epoch 3440/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0807 - class_loss: 9.8138e-04 - l1_loss: 0.3080\n",
      "Epoch 3441/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.3744 - class_loss: 8.5849e-04 - l1_loss: 0.2373\n",
      "Epoch 3442/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3120 - class_loss: 0.0023 - l1_loss: 0.2310\n",
      "Epoch 3443/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.2705 - class_loss: 0.0021 - l1_loss: 0.2268\n",
      "Epoch 3444/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7050 - class_loss: 0.0040 - l1_loss: 0.2701\n",
      "Epoch 3445/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4841 - class_loss: 0.0041 - l1_loss: 0.2480\n",
      "Epoch 3446/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.8995 - class_loss: 0.0026 - l1_loss: 0.1897\n",
      "Epoch 3447/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7700 - class_loss: 7.6904e-04 - l1_loss: 0.1769\n",
      "Epoch 3448/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.0410 - class_loss: 3.6887e-04 - l1_loss: 0.2041\n",
      "Epoch 3449/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.6177 - class_loss: 0.0013 - l1_loss: 0.1616\n",
      "Epoch 3450/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5707 - class_loss: 0.0012 - l1_loss: 0.1569\n",
      "Epoch 3451/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.8251 - class_loss: 8.4892e-04 - l1_loss: 0.1824\n",
      "Epoch 3452/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.6568 - class_loss: 0.0033 - l1_loss: 0.1653\n",
      "Epoch 3453/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5688 - class_loss: 0.0020 - l1_loss: 0.1567\n",
      "Epoch 3454/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.9516 - class_loss: 0.0027 - l1_loss: 0.1949\n",
      "Epoch 3455/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.1718 - class_loss: 0.0052 - l1_loss: 0.2167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3456/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.0224 - class_loss: 6.9315e-04 - l1_loss: 0.2022\n",
      "Epoch 3457/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9483 - class_loss: 6.5601e-04 - l1_loss: 0.1948\n",
      "Epoch 3458/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.8730 - class_loss: 0.0040 - l1_loss: 0.1869\n",
      "Epoch 3459/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 3.1582 - class_loss: 0.0106 - l1_loss: 0.31 - 0s 34ms/step - loss: 3.0652 - class_loss: 0.0096 - l1_loss: 0.3056\n",
      "Epoch 3460/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9861 - class_loss: 8.0727e-04 - l1_loss: 0.2985\n",
      "Epoch 3461/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.2032 - class_loss: 0.0034 - l1_loss: 0.3200\n",
      "Epoch 3462/5000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 2.9884 - class_loss: 0.0011 - l1_loss: 0.2987\n",
      "Epoch 3463/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.2345 - class_loss: 6.1778e-04 - l1_loss: 0.3234\n",
      "Epoch 3464/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.9402 - class_loss: 3.2151e-04 - l1_loss: 0.4940\n",
      "Epoch 3465/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.3935 - class_loss: 0.0075 - l1_loss: 0.3386\n",
      "Epoch 3466/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1464 - class_loss: 0.0011 - l1_loss: 0.3145\n",
      "Epoch 3467/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5595 - class_loss: 0.0040 - l1_loss: 0.3555\n",
      "Epoch 3468/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4764 - class_loss: 0.0039 - l1_loss: 0.2473\n",
      "Epoch 3469/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.4053 - class_loss: 0.0015 - l1_loss: 0.3404\n",
      "Epoch 3470/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.1507 - class_loss: 0.0043 - l1_loss: 0.4146\n",
      "Epoch 3471/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.1362 - class_loss: 0.0135 - l1_loss: 0.4123\n",
      "Epoch 3472/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.9389 - class_loss: 0.0045 - l1_loss: 0.4934\n",
      "Epoch 3473/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.5670 - class_loss: 0.0052 - l1_loss: 0.4562\n",
      "Epoch 3474/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4451 - class_loss: 0.0011 - l1_loss: 0.3444\n",
      "Epoch 3475/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1822 - class_loss: 0.0037 - l1_loss: 0.3179\n",
      "Epoch 3476/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.3681 - class_loss: 0.0069 - l1_loss: 0.4361\n",
      "Epoch 3477/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4118 - class_loss: 5.8079e-04 - l1_loss: 0.2411\n",
      "Epoch 3478/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.1439 - class_loss: 8.8161e-04 - l1_loss: 0.4143\n",
      "Epoch 3479/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8221 - class_loss: 1.5252e-04 - l1_loss: 0.3822\n",
      "Epoch 3480/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2021 - class_loss: 2.3951e-04 - l1_loss: 0.3202\n",
      "Epoch 3481/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4416 - class_loss: 1.2795e-04 - l1_loss: 0.3441\n",
      "Epoch 3482/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4660 - class_loss: 0.0011 - l1_loss: 0.2465\n",
      "Epoch 3483/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2063 - class_loss: 0.0020 - l1_loss: 0.3204\n",
      "Epoch 3484/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.8543 - class_loss: 0.0045 - l1_loss: 0.2850\n",
      "Epoch 3485/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9316 - class_loss: 0.0013 - l1_loss: 0.2930\n",
      "Epoch 3486/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.8936 - class_loss: 0.0222 - l1_loss: 0.4871\n",
      "Epoch 3487/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6082 - class_loss: 0.0030 - l1_loss: 0.3605\n",
      "Epoch 3488/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8062 - class_loss: 0.0078 - l1_loss: 0.2798\n",
      "Epoch 3489/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9425 - class_loss: 0.0012 - l1_loss: 0.2941\n",
      "Epoch 3490/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0415 - class_loss: 0.0040 - l1_loss: 0.4038\n",
      "Epoch 3491/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4058 - class_loss: 0.0063 - l1_loss: 0.3399\n",
      "Epoch 3492/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5707 - class_loss: 0.0040 - l1_loss: 0.2567\n",
      "Epoch 3493/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6291 - class_loss: 0.0074 - l1_loss: 0.2622\n",
      "Epoch 3494/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.4025 - class_loss: 0.0143 - l1_loss: 0.3388\n",
      "Epoch 3495/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9222 - class_loss: 5.3411e-04 - l1_loss: 0.2922\n",
      "Epoch 3496/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7155 - class_loss: 7.0679e-04 - l1_loss: 0.2715\n",
      "Epoch 3497/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.9514 - class_loss: 0.0079 - l1_loss: 0.3944\n",
      "Epoch 3498/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.6754 - class_loss: 0.0014 - l1_loss: 0.4674\n",
      "Epoch 3499/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.1950 - class_loss: 3.8251e-04 - l1_loss: 0.5195\n",
      "Epoch 3500/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.4697 - class_loss: 0.0090 - l1_loss: 0.6461ETA: 0s - loss: 7.4494 - class_loss: 8.4806e-04 - l1_loss: 0.\n",
      "Epoch 3501/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.2284 - class_loss: 0.0026 - l1_loss: 0.7226\n",
      "Epoch 3502/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.0148 - class_loss: 8.7952e-04 - l1_loss: 0.5014\n",
      "Epoch 3503/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.6041 - class_loss: 4.5550e-04 - l1_loss: 0.7604 0s - loss: 7.8849 - class_loss: 4.7180e-04 - l1_loss: 0.\n",
      "Epoch 3504/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.8738 - class_loss: 0.0274 - l1_loss: 0.8846\n",
      "Epoch 3505/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.9597 - class_loss: 0.0052 - l1_loss: 0.8954\n",
      "Epoch 3506/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.8717 - class_loss: 4.9067e-05 - l1_loss: 1.2872\n",
      "Epoch 3507/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.8760 - class_loss: 6.6684e-04 - l1_loss: 1.3875\n",
      "Epoch 3508/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.9735 - class_loss: 0.0017 - l1_loss: 1.3972\n",
      "Epoch 3509/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 16.7189 - class_loss: 0.1444 - l1_loss: 1.6575\n",
      "Epoch 3510/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.9132 - class_loss: 0.0054 - l1_loss: 1.2908\n",
      "Epoch 3511/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.3443 - class_loss: 0.0052 - l1_loss: 1.6339: 0s - loss: 16.5481 - class_loss: 0.0057 - l1_loss: 1.65\n",
      "Epoch 3512/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 46.5378 - class_loss: 0.0058 - l1_loss: 4.6532\n",
      "Epoch 3513/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 19.5453 - class_loss: 0.0018 - l1_loss: 1.9543\n",
      "Epoch 3514/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 23.9239 - class_loss: 0.0026 - l1_loss: 2.3921\n",
      "Epoch 3515/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 18.2262 - class_loss: 0.0025 - l1_loss: 1.8224\n",
      "Epoch 3516/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 20.7752 - class_loss: 3.3702e-04 - l1_loss: 2.0775\n",
      "Epoch 3517/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 17.8700 - class_loss: 6.2243e-04 - l1_loss: 1.7869\n",
      "Epoch 3518/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.1043 - class_loss: 0.0447 - l1_loss: 1.8060\n",
      "Epoch 3519/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 19.4770 - class_loss: 0.0062 - l1_loss: 1.9471\n",
      "Epoch 3520/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.9664 - class_loss: 0.0778 - l1_loss: 1.7889\n",
      "Epoch 3521/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.1668 - class_loss: 7.3774e-05 - l1_loss: 1.3167\n",
      "Epoch 3522/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.9026 - class_loss: 4.8787e-04 - l1_loss: 1.3902\n",
      "Epoch 3523/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.9036 - class_loss: 8.7105e-04 - l1_loss: 1.2903\n",
      "Epoch 3524/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.3979 - class_loss: 0.1212 - l1_loss: 1.4277\n",
      "Epoch 3525/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.1377 - class_loss: 0.0012 - l1_loss: 1.4137\n",
      "Epoch 3526/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.0614 - class_loss: 0.0193 - l1_loss: 0.7042\n",
      "Epoch 3527/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.9199 - class_loss: 0.0049 - l1_loss: 0.7915\n",
      "Epoch 3528/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.3362 - class_loss: 0.0012 - l1_loss: 0.8335\n",
      "Epoch 3529/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.4962 - class_loss: 0.0040 - l1_loss: 0.9492\n",
      "Epoch 3530/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.4579 - class_loss: 0.0153 - l1_loss: 0.8443\n",
      "Epoch 3531/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.4426 - class_loss: 9.4569e-04 - l1_loss: 0.7442\n",
      "Epoch 3532/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.9537 - class_loss: 0.1967 - l1_loss: 0.6757\n",
      "Epoch 3533/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.4360 - class_loss: 0.0082 - l1_loss: 1.0428\n",
      "Epoch 3534/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.4261 - class_loss: 0.0064 - l1_loss: 1.1420\n",
      "Epoch 3535/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.6875 - class_loss: 0.0020 - l1_loss: 1.2685\n",
      "Epoch 3536/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.7932 - class_loss: 1.4591e-04 - l1_loss: 0.9793\n",
      "Epoch 3537/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.2059 - class_loss: 0.0082 - l1_loss: 0.8198\n",
      "Epoch 3538/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.1744 - class_loss: 0.0063 - l1_loss: 0.5168\n",
      "Epoch 3539/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.7113 - class_loss: 0.0021 - l1_loss: 0.6709\n",
      "Epoch 3540/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.6597 - class_loss: 0.0048 - l1_loss: 0.7655\n",
      "Epoch 3541/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.6946 - class_loss: 0.0156 - l1_loss: 0.9679 0s - loss: 10.2156 - class_loss: 0.0177 - l1_loss: 1.019\n",
      "Epoch 3542/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.5628 - class_loss: 9.8810e-04 - l1_loss: 0.6562\n",
      "Epoch 3543/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5389 - class_loss: 5.0180e-04 - l1_loss: 0.3538\n",
      "Epoch 3544/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.9087 - class_loss: 0.0087 - l1_loss: 0.5900\n",
      "Epoch 3545/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2135 - class_loss: 0.0049 - l1_loss: 0.4209\n",
      "Epoch 3546/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.2518 - class_loss: 0.0053 - l1_loss: 0.5246\n",
      "Epoch 3547/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.4151 - class_loss: 0.0051 - l1_loss: 0.4410 0s - loss: 5.1629 - class_loss: 0.0046 - l1_loss: 0.\n",
      "Epoch 3548/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.0507 - class_loss: 0.0035 - l1_loss: 0.4047\n",
      "Epoch 3549/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.0714 - class_loss: 0.0015 - l1_loss: 0.3070\n",
      "Epoch 3550/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6817 - class_loss: 0.0031 - l1_loss: 0.2679\n",
      "Epoch 3551/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.8311 - class_loss: 0.0154 - l1_loss: 0.2816\n",
      "Epoch 3552/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.8672 - class_loss: 9.6450e-04 - l1_loss: 0.2866\n",
      "Epoch 3553/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5632 - class_loss: 0.0021 - l1_loss: 0.3561\n",
      "Epoch 3554/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.7717 - class_loss: 0.0041 - l1_loss: 0.3768\n",
      "Epoch 3555/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.0598 - class_loss: 0.0014 - l1_loss: 0.5058\n",
      "Epoch 3556/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5646 - class_loss: 0.0253 - l1_loss: 0.4539ETA: 0s - loss: 4.9927 - class_loss: 0.0293 - l1_loss: 0.\n",
      "Epoch 3557/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.9099 - class_loss: 0.0072 - l1_loss: 0.7903\n",
      "Epoch 3558/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.1676 - class_loss: 0.0025 - l1_loss: 1.1165\n",
      "Epoch 3559/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.1289 - class_loss: 0.0040 - l1_loss: 0.9125\n",
      "Epoch 3560/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 17.9132 - class_loss: 3.2651e-04 - l1_loss: 1.7913\n",
      "Epoch 3561/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.9062 - class_loss: 0.0022 - l1_loss: 1.4904\n",
      "Epoch 3562/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.3682 - class_loss: 0.0019 - l1_loss: 1.2366\n",
      "Epoch 3563/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.0918 - class_loss: 0.0410 - l1_loss: 1.5051\n",
      "Epoch 3564/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.3601 - class_loss: 4.9201e-04 - l1_loss: 1.2360\n",
      "Epoch 3565/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.5772 - class_loss: 3.3129e-05 - l1_loss: 1.1577\n",
      "Epoch 3566/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.5988 - class_loss: 0.0019 - l1_loss: 0.9597\n",
      "Epoch 3567/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.0329 - class_loss: 0.0067 - l1_loss: 0.9026\n",
      "Epoch 3568/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.2258 - class_loss: 0.0225 - l1_loss: 0.7203\n",
      "Epoch 3569/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.4474 - class_loss: 5.8226e-04 - l1_loss: 0.6447\n",
      "Epoch 3570/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.3468 - class_loss: 3.6215e-04 - l1_loss: 0.6346\n",
      "Epoch 3571/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.6886 - class_loss: 0.0085 - l1_loss: 0.7680\n",
      "Epoch 3572/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.4235 - class_loss: 0.0080 - l1_loss: 0.7415\n",
      "Epoch 3573/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.4410 - class_loss: 0.0058 - l1_loss: 1.2435\n",
      "Epoch 3574/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.3037 - class_loss: 0.0054 - l1_loss: 1.3298\n",
      "Epoch 3575/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 12.2730 - class_loss: 0.0019 - l1_loss: 1.2271\n",
      "Epoch 3576/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.5871 - class_loss: 0.0189 - l1_loss: 1.15680s - loss: 12.0140 - class_loss: 0.0089 - l1_loss: 1.2\n",
      "Epoch 3577/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.4781 - class_loss: 0.0037 - l1_loss: 1.4474\n",
      "Epoch 3578/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.7113 - class_loss: 0.0063 - l1_loss: 1.6705\n",
      "Epoch 3579/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.2085 - class_loss: 0.0194 - l1_loss: 1.3189\n",
      "Epoch 3580/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.0223 - class_loss: 0.1483 - l1_loss: 1.2874\n",
      "Epoch 3581/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.0798 - class_loss: 0.0070 - l1_loss: 1.0073\n",
      "Epoch 3582/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 14.6173 - class_loss: 0.0147 - l1_loss: 1.4603: 0s - loss: 19.2560 - class_loss: 0.0232 - l1_loss: 1.92\n",
      "Epoch 3583/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.9508 - class_loss: 3.9573e-04 - l1_loss: 1.3950\n",
      "Epoch 3584/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 30ms/step - loss: 15.6154 - class_loss: 0.0052 - l1_loss: 1.5610: 0s - loss: 15.5600 - class_loss: 0.0017 - l1_loss: 1.555\n",
      "Epoch 3585/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 19.4509 - class_loss: 0.0033 - l1_loss: 1.9448\n",
      "Epoch 3586/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.6210 - class_loss: 0.0029 - l1_loss: 1.4618\n",
      "Epoch 3587/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.5073 - class_loss: 0.0128 - l1_loss: 1.1495\n",
      "Epoch 3588/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 19.7280 - class_loss: 2.2067e-04 - l1_loss: 1.9728\n",
      "Epoch 3589/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.7527 - class_loss: 4.1433e-04 - l1_loss: 1.0752\n",
      "Epoch 3590/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.0443 - class_loss: 8.6524e-04 - l1_loss: 0.9043\n",
      "Epoch 3591/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.5379 - class_loss: 0.0060 - l1_loss: 0.9532\n",
      "Epoch 3592/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.7077 - class_loss: 0.0033 - l1_loss: 0.8704\n",
      "Epoch 3593/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.5994 - class_loss: 0.0073 - l1_loss: 0.9592 0s - loss: 10.8479 - class_loss: 0.0114 - l1_loss: 1.0\n",
      "Epoch 3594/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.6930 - class_loss: 5.8030e-04 - l1_loss: 0.8692A: 0s - loss: 10.9595 - class_loss: 8.3711e-04 - l1_loss: 1.0\n",
      "Epoch 3595/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.7988 - class_loss: 0.0029 - l1_loss: 0.3796\n",
      "Epoch 3596/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.4520 - class_loss: 0.0082 - l1_loss: 0.5444\n",
      "Epoch 3597/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.4079 - class_loss: 0.0010 - l1_loss: 0.5407\n",
      "Epoch 3598/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.8172 - class_loss: 3.4139e-04 - l1_loss: 0.3817\n",
      "Epoch 3599/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.6860 - class_loss: 9.2287e-04 - l1_loss: 0.5685\n",
      "Epoch 3600/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.4826 - class_loss: 6.2163e-04 - l1_loss: 0.3482\n",
      "Epoch 3601/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0367 - class_loss: 0.0027 - l1_loss: 0.3034\n",
      "Epoch 3602/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0019 - class_loss: 4.9785e-04 - l1_loss: 0.3001\n",
      "Epoch 3603/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3842 - class_loss: 0.0044 - l1_loss: 0.3380\n",
      "Epoch 3604/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.3596 - class_loss: 0.0040 - l1_loss: 0.4356\n",
      "Epoch 3605/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9084 - class_loss: 0.0012 - l1_loss: 0.2907\n",
      "Epoch 3606/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6903 - class_loss: 0.0129 - l1_loss: 0.3677\n",
      "Epoch 3607/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6646 - class_loss: 0.0063 - l1_loss: 0.2658\n",
      "Epoch 3608/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.0222 - class_loss: 4.0693e-04 - l1_loss: 0.3022\n",
      "Epoch 3609/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6301 - class_loss: 0.0024 - l1_loss: 0.2628\n",
      "Epoch 3610/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.3310 - class_loss: 0.0043 - l1_loss: 0.2327\n",
      "Epoch 3611/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.8177 - class_loss: 0.0071 - l1_loss: 0.2811\n",
      "Epoch 3612/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1825 - class_loss: 0.0012 - l1_loss: 0.3181\n",
      "Epoch 3613/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.3324 - class_loss: 0.0063 - l1_loss: 0.3326\n",
      "Epoch 3614/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.5150 - class_loss: 0.0096 - l1_loss: 0.3505\n",
      "Epoch 3615/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0372 - class_loss: 0.0016 - l1_loss: 0.2036\n",
      "Epoch 3616/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0656 - class_loss: 7.0368e-04 - l1_loss: 0.3065 0s - loss: 2.4508 - class_loss: 4.3979e-04 - l1_loss: \n",
      "Epoch 3617/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.2938 - class_loss: 0.0034 - l1_loss: 0.2290\n",
      "Epoch 3618/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4510 - class_loss: 0.0042 - l1_loss: 0.2447\n",
      "Epoch 3619/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.1260 - class_loss: 0.0011 - l1_loss: 0.2125\n",
      "Epoch 3620/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3385 - class_loss: 0.0015 - l1_loss: 0.2337\n",
      "Epoch 3621/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2680 - class_loss: 0.0017 - l1_loss: 0.2266\n",
      "Epoch 3622/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.3046 - class_loss: 0.0016 - l1_loss: 0.2303\n",
      "Epoch 3623/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.3122 - class_loss: 0.0041 - l1_loss: 0.2308\n",
      "Epoch 3624/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.2657 - class_loss: 0.0052 - l1_loss: 0.2261\n",
      "Epoch 3625/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1844 - class_loss: 0.0016 - l1_loss: 0.2183\n",
      "Epoch 3626/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8916 - class_loss: 0.0039 - l1_loss: 0.1888\n",
      "Epoch 3627/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4592 - class_loss: 0.0026 - l1_loss: 0.2457\n",
      "Epoch 3628/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1521 - class_loss: 0.0020 - l1_loss: 0.2150 0s - loss: 2.4034 - class_loss: 0.0020 - l1_loss: 0.\n",
      "Epoch 3629/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7001 - class_loss: 0.0046 - l1_loss: 0.1696\n",
      "Epoch 3630/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6765 - class_loss: 0.0040 - l1_loss: 0.1673\n",
      "Epoch 3631/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7831 - class_loss: 0.0022 - l1_loss: 0.1781\n",
      "Epoch 3632/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7081 - class_loss: 0.0011 - l1_loss: 0.1707\n",
      "Epoch 3633/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6008 - class_loss: 0.0014 - l1_loss: 0.1599\n",
      "Epoch 3634/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7277 - class_loss: 0.0013 - l1_loss: 0.1726\n",
      "Epoch 3635/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3991 - class_loss: 5.9064e-04 - l1_loss: 0.1399\n",
      "Epoch 3636/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6413 - class_loss: 0.0020 - l1_loss: 0.1639\n",
      "Epoch 3637/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4940 - class_loss: 0.0028 - l1_loss: 0.1491\n",
      "Epoch 3638/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9472 - class_loss: 0.0042 - l1_loss: 0.1943\n",
      "Epoch 3639/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.0420 - class_loss: 0.0036 - l1_loss: 0.2038\n",
      "Epoch 3640/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5679 - class_loss: 0.0011 - l1_loss: 0.1567 0s - loss: 1.5242 - class_loss: 0.0015 - l1_loss: 0.\n",
      "Epoch 3641/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1398 - class_loss: 0.0013 - l1_loss: 0.1138\n",
      "Epoch 3642/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0097 - class_loss: 0.0035 - l1_loss: 0.1006\n",
      "Epoch 3643/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2375 - class_loss: 0.0042 - l1_loss: 0.1233 0s - loss: 0.9852 - class_loss: 0.0053 - l1_loss: 0.\n",
      "Epoch 3644/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3543 - class_loss: 0.0011 - l1_loss: 0.1353\n",
      "Epoch 3645/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3121 - class_loss: 0.0011 - l1_loss: 0.1311\n",
      "Epoch 3646/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.3292 - class_loss: 0.0025 - l1_loss: 0.1327\n",
      "Epoch 3647/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5362 - class_loss: 0.0074 - l1_loss: 0.1529\n",
      "Epoch 3648/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0510 - class_loss: 0.0050 - l1_loss: 0.2046\n",
      "Epoch 3649/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.3263 - class_loss: 0.0035 - l1_loss: 0.4323\n",
      "Epoch 3650/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4245 - class_loss: 2.4929e-04 - l1_loss: 0.3424\n",
      "Epoch 3651/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.5615 - class_loss: 7.6727e-04 - l1_loss: 0.6561\n",
      "Epoch 3652/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.5303 - class_loss: 0.0019 - l1_loss: 0.4528\n",
      "Epoch 3653/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.7465 - class_loss: 0.0012 - l1_loss: 0.3745\n",
      "Epoch 3654/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.3128 - class_loss: 9.8035e-04 - l1_loss: 0.6312\n",
      "Epoch 3655/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1809 - class_loss: 6.9463e-04 - l1_loss: 0.4180\n",
      "Epoch 3656/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.6991 - class_loss: 0.0133 - l1_loss: 0.6686\n",
      "Epoch 3657/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.4419 - class_loss: 0.0013 - l1_loss: 0.5441\n",
      "Epoch 3658/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.7034 - class_loss: 6.8114e-04 - l1_loss: 0.5703\n",
      "Epoch 3659/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.2397 - class_loss: 0.0051 - l1_loss: 0.5235\n",
      "Epoch 3660/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.2007 - class_loss: 0.0023 - l1_loss: 0.6198ETA: 0s - loss: 6.5890 - class_loss: 9.8930e-04 - l1_loss: 0.\n",
      "Epoch 3661/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.3252 - class_loss: 0.0020 - l1_loss: 0.5323\n",
      "Epoch 3662/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.5345 - class_loss: 0.0011 - l1_loss: 0.5533\n",
      "Epoch 3663/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.7079 - class_loss: 0.0016 - l1_loss: 0.6706\n",
      "Epoch 3664/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9606 - class_loss: 0.0043 - l1_loss: 0.4956\n",
      "Epoch 3665/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.4536 - class_loss: 0.0050 - l1_loss: 0.6449\n",
      "Epoch 3666/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.1314 - class_loss: 5.3318e-04 - l1_loss: 0.5131\n",
      "Epoch 3667/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.0577 - class_loss: 0.0086 - l1_loss: 0.4049\n",
      "Epoch 3668/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.9637 - class_loss: 0.0062 - l1_loss: 0.4958\n",
      "Epoch 3669/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.1400 - class_loss: 1.1035e-04 - l1_loss: 0.4140\n",
      "Epoch 3670/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9922 - class_loss: 0.0010 - l1_loss: 0.2991\n",
      "Epoch 3671/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.1398 - class_loss: 0.0015 - l1_loss: 0.4138\n",
      "Epoch 3672/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4301 - class_loss: 5.0822e-04 - l1_loss: 0.3430\n",
      "Epoch 3673/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.4979 - class_loss: 0.0061 - l1_loss: 0.3492\n",
      "Epoch 3674/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2508 - class_loss: 0.0285 - l1_loss: 0.3222\n",
      "Epoch 3675/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0604 - class_loss: 5.1479e-05 - l1_loss: 0.2060\n",
      "Epoch 3676/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2990 - class_loss: 0.0011 - l1_loss: 0.2298\n",
      "Epoch 3677/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.7147 - class_loss: 8.4003e-04 - l1_loss: 0.2714\n",
      "Epoch 3678/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.5489 - class_loss: 3.5402e-04 - l1_loss: 0.2549\n",
      "Epoch 3679/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7800 - class_loss: 0.0223 - l1_loss: 0.1758\n",
      "Epoch 3680/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.8522 - class_loss: 7.1105e-04 - l1_loss: 0.2851\n",
      "Epoch 3681/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1286 - class_loss: 6.9942e-05 - l1_loss: 0.3128\n",
      "Epoch 3682/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6378 - class_loss: 4.8045e-04 - l1_loss: 0.2637\n",
      "Epoch 3683/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.5150 - class_loss: 0.0047 - l1_loss: 0.3510\n",
      "Epoch 3684/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0403 - class_loss: 0.0010 - l1_loss: 0.4039\n",
      "Epoch 3685/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.6931 - class_loss: 0.0202 - l1_loss: 0.6673\n",
      "Epoch 3686/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.9769 - class_loss: 0.0021 - l1_loss: 0.5975\n",
      "Epoch 3687/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.5083 - class_loss: 5.0853e-04 - l1_loss: 0.7508\n",
      "Epoch 3688/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 8.5098 - class_loss: 4.7955e-04 - l1_loss: 0.8509 ETA: 0s - loss: 8.4589 - class_loss: 6.5829e-04 - l1_loss: 0. - 0s 31ms/step - loss: 7.8520 - class_loss: 4.2008e-04 - l1_loss: 0.7852\n",
      "Epoch 3689/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.3084 - class_loss: 5.3063e-04 - l1_loss: 0.8308\n",
      "Epoch 3690/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.0611 - class_loss: 7.2195e-04 - l1_loss: 0.7060\n",
      "Epoch 3691/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.6071 - class_loss: 3.5327e-04 - l1_loss: 0.4607\n",
      "Epoch 3692/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8328 - class_loss: 0.0130 - l1_loss: 0.4820\n",
      "Epoch 3693/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.0179 - class_loss: 0.0012 - l1_loss: 0.7017\n",
      "Epoch 3694/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.5894 - class_loss: 0.0053 - l1_loss: 1.1584\n",
      "Epoch 3695/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.4635 - class_loss: 0.6880 - l1_loss: 1.5775\n",
      "Epoch 3696/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 20.6400 - class_loss: 0.1802 - l1_loss: 2.0460\n",
      "Epoch 3697/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 25.8308 - class_loss: 0.5137 - l1_loss: 2.5317\n",
      "Epoch 3698/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 64.9582 - class_loss: 0.1590 - l1_loss: 6.4799: 0s - loss: 55.9867 - class_loss: 0.1515 - l1_loss: 5.58\n",
      "Epoch 3699/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 19.7106 - class_loss: 0.1367 - l1_loss: 1.9574\n",
      "Epoch 3700/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 29.3787 - class_loss: 0.0252 - l1_loss: 2.9353\n",
      "Epoch 3701/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 26.9528 - class_loss: 0.0091 - l1_loss: 2.6944\n",
      "Epoch 3702/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 32.8432 - class_loss: 0.0062 - l1_loss: 3.2837\n",
      "Epoch 3703/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 36.1664 - class_loss: 9.2554e-04 - l1_loss: 3.616 - 0s 30ms/step - loss: 35.0452 - class_loss: 0.0014 - l1_loss: 3.5044\n",
      "Epoch 3704/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 35.1844 - class_loss: 0.2706 - l1_loss: 3.4914\n",
      "Epoch 3705/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 41.3535 - class_loss: 0.0011 - l1_loss: 4.1352\n",
      "Epoch 3706/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.2327 - class_loss: 0.0023 - l1_loss: 1.7230\n",
      "Epoch 3707/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 25.3484 - class_loss: 0.0013 - l1_loss: 2.5347\n",
      "Epoch 3708/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 26.2115 - class_loss: 3.3668e-05 - l1_loss: 2.6211\n",
      "Epoch 3709/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 21.1889 - class_loss: 3.1438e-04 - l1_loss: 2.1189\n",
      "Epoch 3710/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 35ms/step - loss: 17.8029 - class_loss: 0.0093 - l1_loss: 1.7794\n",
      "Epoch 3711/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.3497 - class_loss: 1.4981e-04 - l1_loss: 1.2350\n",
      "Epoch 3712/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 16.3132 - class_loss: 8.6712e-04 - l1_loss: 1.6312\n",
      "Epoch 3713/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.0297 - class_loss: 7.3132e-04 - l1_loss: 1.6029\n",
      "Epoch 3714/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.3945 - class_loss: 0.0030 - l1_loss: 1.2392\n",
      "Epoch 3715/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.9555 - class_loss: 0.0645 - l1_loss: 0.8891\n",
      "Epoch 3716/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.9040 - class_loss: 0.0013 - l1_loss: 1.0903\n",
      "Epoch 3717/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.7244 - class_loss: 7.7381e-04 - l1_loss: 0.8724\n",
      "Epoch 3718/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.0757 - class_loss: 0.0054 - l1_loss: 0.8070\n",
      "Epoch 3719/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.4667 - class_loss: 2.8877e-04 - l1_loss: 0.6466\n",
      "Epoch 3720/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.9403 - class_loss: 0.0013 - l1_loss: 0.5939\n",
      "Epoch 3721/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.7926 - class_loss: 0.0018 - l1_loss: 0.5791\n",
      "Epoch 3722/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.5439 - class_loss: 0.0012 - l1_loss: 0.8543\n",
      "Epoch 3723/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 6.9794 - class_loss: 0.0122 - l1_loss: 0.6967\n",
      "Epoch 3724/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.7124 - class_loss: 0.0062 - l1_loss: 0.7706\n",
      "Epoch 3725/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.9857 - class_loss: 6.7207e-04 - l1_loss: 0.7985\n",
      "Epoch 3726/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.7809 - class_loss: 0.0075 - l1_loss: 0.5773\n",
      "Epoch 3727/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.1999 - class_loss: 0.0078 - l1_loss: 0.8192\n",
      "Epoch 3728/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.6384 - class_loss: 4.2375e-04 - l1_loss: 0.7638\n",
      "Epoch 3729/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 9.1040 - class_loss: 4.5760e-04 - l1_loss: 0.9104\n",
      "Epoch 3730/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.0106 - class_loss: 0.0120 - l1_loss: 0.9999\n",
      "Epoch 3731/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.6459 - class_loss: 0.0047 - l1_loss: 1.1641\n",
      "Epoch 3732/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.6837 - class_loss: 0.0052 - l1_loss: 1.2678\n",
      "Epoch 3733/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.4049 - class_loss: 0.0031 - l1_loss: 1.2402\n",
      "Epoch 3734/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.6627 - class_loss: 0.0026 - l1_loss: 0.6660\n",
      "Epoch 3735/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.6647 - class_loss: 0.0073 - l1_loss: 0.8657\n",
      "Epoch 3736/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.1234 - class_loss: 0.0019 - l1_loss: 0.7121\n",
      "Epoch 3737/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.2485 - class_loss: 0.0063 - l1_loss: 0.6242\n",
      "Epoch 3738/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.9106 - class_loss: 0.0230 - l1_loss: 0.5888\n",
      "Epoch 3739/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.2286 - class_loss: 6.4328e-04 - l1_loss: 0.6228\n",
      "Epoch 3740/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.5058 - class_loss: 0.0040 - l1_loss: 0.8502\n",
      "Epoch 3741/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.0876 - class_loss: 5.5092e-04 - l1_loss: 0.6087\n",
      "Epoch 3742/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5426 - class_loss: 6.1439e-04 - l1_loss: 0.4542\n",
      "Epoch 3743/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.9611 - class_loss: 0.0061 - l1_loss: 0.6955\n",
      "Epoch 3744/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.4834 - class_loss: 0.0188 - l1_loss: 0.7465\n",
      "Epoch 3745/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.4320 - class_loss: 0.0050 - l1_loss: 0.8427\n",
      "Epoch 3746/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.3494 - class_loss: 9.8154e-04 - l1_loss: 0.8348\n",
      "Epoch 3747/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.0087 - class_loss: 4.2335e-04 - l1_loss: 0.6008\n",
      "Epoch 3748/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.4350 - class_loss: 0.0049 - l1_loss: 0.6430\n",
      "Epoch 3749/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.7224 - class_loss: 0.0013 - l1_loss: 0.8721\n",
      "Epoch 3750/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.4339 - class_loss: 0.0025 - l1_loss: 0.7431\n",
      "Epoch 3751/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.2606 - class_loss: 0.0049 - l1_loss: 0.4256\n",
      "Epoch 3752/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 6.4499 - class_loss: 0.1444 - l1_loss: 0.6306\n",
      "Epoch 3753/5000\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 5.2095 - class_loss: 0.0016 - l1_loss: 0.5208\n",
      "Epoch 3754/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.9282 - class_loss: 6.8709e-04 - l1_loss: 0.5927\n",
      "Epoch 3755/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.7367 - class_loss: 2.1118e-04 - l1_loss: 0.5737\n",
      "Epoch 3756/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.6969 - class_loss: 1.2274e-05 - l1_loss: 0.6697\n",
      "Epoch 3757/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.1675 - class_loss: 5.4313e-04 - l1_loss: 0.6167\n",
      "Epoch 3758/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.5667 - class_loss: 1.0932e-04 - l1_loss: 0.4567\n",
      "Epoch 3759/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.6233 - class_loss: 2.2859e-05 - l1_loss: 0.3623\n",
      "Epoch 3760/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.2632 - class_loss: 4.7281e-04 - l1_loss: 0.5263\n",
      "Epoch 3761/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.8573 - class_loss: 4.4561e-04 - l1_loss: 0.2857\n",
      "Epoch 3762/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.2047 - class_loss: 0.0031 - l1_loss: 0.3202\n",
      "Epoch 3763/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.3956 - class_loss: 0.0027 - l1_loss: 0.3393\n",
      "Epoch 3764/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.7068 - class_loss: 0.0064 - l1_loss: 0.3700\n",
      "Epoch 3765/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.1528 - class_loss: 0.0599 - l1_loss: 0.4093\n",
      "Epoch 3766/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.1814 - class_loss: 2.4767e-05 - l1_loss: 0.4181\n",
      "Epoch 3767/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.5715 - class_loss: 5.3250e-05 - l1_loss: 0.3571\n",
      "Epoch 3768/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.0158 - class_loss: 7.3377e-05 - l1_loss: 0.4016\n",
      "Epoch 3769/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.2865 - class_loss: 2.1867e-04 - l1_loss: 0.4286\n",
      "Epoch 3770/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.8774 - class_loss: 1.1050e-04 - l1_loss: 0.2877\n",
      "Epoch 3771/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.2238 - class_loss: 9.1320e-04 - l1_loss: 0.3223\n",
      "Epoch 3772/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 3.5761 - class_loss: 0.0012 - l1_loss: 0.3575\n",
      "Epoch 3773/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.9633 - class_loss: 6.4127e-04 - l1_loss: 0.2963\n",
      "Epoch 3774/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.0208 - class_loss: 0.0026 - l1_loss: 0.3018\n",
      "Epoch 3775/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.0385 - class_loss: 0.0089 - l1_loss: 0.3030\n",
      "Epoch 3776/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.1436 - class_loss: 0.0011 - l1_loss: 0.2143\n",
      "Epoch 3777/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.0164 - class_loss: 4.9914e-04 - l1_loss: 0.2016\n",
      "Epoch 3778/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.1031 - class_loss: 7.3777e-04 - l1_loss: 0.2102 0s - loss: 2.0463 - class_loss: 5.1763e-04 - l1_loss: 0.\n",
      "Epoch 3779/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.0470 - class_loss: 6.1407e-04 - l1_loss: 0.2046\n",
      "Epoch 3780/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7042 - class_loss: 5.4606e-04 - l1_loss: 0.1704\n",
      "Epoch 3781/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.5859 - class_loss: 8.8653e-04 - l1_loss: 0.1585\n",
      "Epoch 3782/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.2625 - class_loss: 0.0018 - l1_loss: 0.1261\n",
      "Epoch 3783/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.4893 - class_loss: 0.0072 - l1_loss: 0.1482\n",
      "Epoch 3784/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8454 - class_loss: 0.0039 - l1_loss: 0.1842\n",
      "Epoch 3785/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.3041 - class_loss: 0.0011 - l1_loss: 0.1303\n",
      "Epoch 3786/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7476 - class_loss: 7.6924e-04 - l1_loss: 0.1747\n",
      "Epoch 3787/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0980 - class_loss: 7.2986e-04 - l1_loss: 0.2097\n",
      "Epoch 3788/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.1916 - class_loss: 3.9069e-04 - l1_loss: 0.2191\n",
      "Epoch 3789/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5898 - class_loss: 7.5659e-04 - l1_loss: 0.1589\n",
      "Epoch 3790/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7005 - class_loss: 0.0018 - l1_loss: 0.1699\n",
      "Epoch 3791/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6543 - class_loss: 0.0015 - l1_loss: 0.1653\n",
      "Epoch 3792/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6621 - class_loss: 0.0018 - l1_loss: 0.1660\n",
      "Epoch 3793/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.6689 - class_loss: 0.0024 - l1_loss: 0.1666\n",
      "Epoch 3794/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.8159 - class_loss: 0.0037 - l1_loss: 0.1812\n",
      "Epoch 3795/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.0439 - class_loss: 0.0024 - l1_loss: 0.2041\n",
      "Epoch 3796/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.2740 - class_loss: 0.0023 - l1_loss: 0.2272\n",
      "Epoch 3797/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.8671 - class_loss: 8.3048e-04 - l1_loss: 0.2866\n",
      "Epoch 3798/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.4007 - class_loss: 4.3090e-04 - l1_loss: 0.2400\n",
      "Epoch 3799/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1960 - class_loss: 9.1260e-04 - l1_loss: 0.2195\n",
      "Epoch 3800/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8275 - class_loss: 0.0010 - l1_loss: 0.1826\n",
      "Epoch 3801/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.4444 - class_loss: 0.0013 - l1_loss: 0.2443\n",
      "Epoch 3802/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.6473 - class_loss: 0.0018 - l1_loss: 0.1646\n",
      "Epoch 3803/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.3879 - class_loss: 0.0043 - l1_loss: 0.3384\n",
      "Epoch 3804/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.9733 - class_loss: 0.0043 - l1_loss: 0.1969\n",
      "Epoch 3805/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.2407 - class_loss: 0.0030 - l1_loss: 0.2238\n",
      "Epoch 3806/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.8175 - class_loss: 0.0029 - l1_loss: 0.1815\n",
      "Epoch 3807/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.3581 - class_loss: 0.0021 - l1_loss: 0.2356\n",
      "Epoch 3808/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.2108 - class_loss: 0.0019 - l1_loss: 0.2209\n",
      "Epoch 3809/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6321 - class_loss: 0.0020 - l1_loss: 0.1630\n",
      "Epoch 3810/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.9377 - class_loss: 0.0019 - l1_loss: 0.1936\n",
      "Epoch 3811/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9103 - class_loss: 0.0038 - l1_loss: 0.1907\n",
      "Epoch 3812/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0485 - class_loss: 0.0054 - l1_loss: 0.2043\n",
      "Epoch 3813/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2087 - class_loss: 0.0037 - l1_loss: 0.2205\n",
      "Epoch 3814/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.6987 - class_loss: 0.0012 - l1_loss: 0.1698\n",
      "Epoch 3815/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7566 - class_loss: 6.2264e-04 - l1_loss: 0.1756\n",
      "Epoch 3816/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.6053 - class_loss: 3.9651e-04 - l1_loss: 0.1605\n",
      "Epoch 3817/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3944 - class_loss: 0.0011 - l1_loss: 0.2393\n",
      "Epoch 3818/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0819 - class_loss: 9.7093e-04 - l1_loss: 0.2081\n",
      "Epoch 3819/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.1055 - class_loss: 7.2035e-04 - l1_loss: 0.2105\n",
      "Epoch 3820/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.9846 - class_loss: 0.0013 - l1_loss: 0.1983\n",
      "Epoch 3821/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4520 - class_loss: 0.0036 - l1_loss: 0.2448\n",
      "Epoch 3822/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.0981 - class_loss: 0.0020 - l1_loss: 0.3096\n",
      "Epoch 3823/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9897 - class_loss: 0.0042 - l1_loss: 0.2986\n",
      "Epoch 3824/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.9685 - class_loss: 0.0050 - l1_loss: 0.5964\n",
      "Epoch 3825/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.9873 - class_loss: 0.0074 - l1_loss: 0.4980\n",
      "Epoch 3826/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.6083 - class_loss: 0.0037 - l1_loss: 0.8605\n",
      "Epoch 3827/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 27.4571 - class_loss: 0.0084 - l1_loss: 2.7449\n",
      "Epoch 3828/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 17.4873 - class_loss: 4.6155e-04 - l1_loss: 1.7487\n",
      "Epoch 3829/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 26.5836 - class_loss: 0.0042 - l1_loss: 2.6579\n",
      "Epoch 3830/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 13.5953 - class_loss: 0.0017 - l1_loss: 1.3594\n",
      "Epoch 3831/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 22.7477 - class_loss: 0.0065 - l1_loss: 2.2741\n",
      "Epoch 3832/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 17.3943 - class_loss: 8.3665e-04 - l1_loss: 1.7393\n",
      "Epoch 3833/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 20.6868 - class_loss: 3.3795e-04 - l1_loss: 2.0687\n",
      "Epoch 3834/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.4089 - class_loss: 7.2537e-04 - l1_loss: 1.1408\n",
      "Epoch 3835/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 20.2157 - class_loss: 0.0016 - l1_loss: 2.0214\n",
      "Epoch 3836/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 22.7144 - class_loss: 0.0027 - l1_loss: 2.2712\n",
      "Epoch 3837/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.5904 - class_loss: 0.0019 - l1_loss: 1.2589\n",
      "Epoch 3838/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.9672 - class_loss: 2.7193e-04 - l1_loss: 1.3967\n",
      "Epoch 3839/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.1105 - class_loss: 0.0115 - l1_loss: 1.1099\n",
      "Epoch 3840/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.9523 - class_loss: 0.0010 - l1_loss: 1.2951\n",
      "Epoch 3841/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.7127 - class_loss: 0.0022 - l1_loss: 0.8711\n",
      "Epoch 3842/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 32ms/step - loss: 10.4347 - class_loss: 0.0044 - l1_loss: 1.0430\n",
      "Epoch 3843/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.2638 - class_loss: 5.9908e-04 - l1_loss: 1.3263\n",
      "Epoch 3844/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.5320 - class_loss: 0.0011 - l1_loss: 0.8531\n",
      "Epoch 3845/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.2506 - class_loss: 0.0252 - l1_loss: 0.7225\n",
      "Epoch 3846/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.0556 - class_loss: 0.0120 - l1_loss: 1.1044\n",
      "Epoch 3847/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.6363 - class_loss: 0.0032 - l1_loss: 1.1633\n",
      "Epoch 3848/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.8531 - class_loss: 0.0025 - l1_loss: 1.0851\n",
      "Epoch 3849/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.6684 - class_loss: 0.0010 - l1_loss: 1.0667\n",
      "Epoch 3850/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.5113 - class_loss: 0.0017 - l1_loss: 0.8510\n",
      "Epoch 3851/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 9.3855 - class_loss: 0.0050 - l1_loss: 0.9380 0s - loss: 11.6505 - class_loss: 0.0067 - l1_loss: 1.1\n",
      "Epoch 3852/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.9178 - class_loss: 0.0081 - l1_loss: 0.6910\n",
      "Epoch 3853/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.5358 - class_loss: 6.2815e-04 - l1_loss: 0.6535\n",
      "Epoch 3854/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.4018 - class_loss: 0.0021 - l1_loss: 0.7400\n",
      "Epoch 3855/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.7675 - class_loss: 9.0238e-04 - l1_loss: 0.6767\n",
      "Epoch 3856/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.6781 - class_loss: 0.0024 - l1_loss: 0.5676\n",
      "Epoch 3857/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3762 - class_loss: 0.0166 - l1_loss: 0.4360\n",
      "Epoch 3858/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.4818 - class_loss: 0.0042 - l1_loss: 0.5478\n",
      "Epoch 3859/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.9087 - class_loss: 1.4050e-04 - l1_loss: 0.4909\n",
      "Epoch 3860/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7226 - class_loss: 0.0014 - l1_loss: 0.4721\n",
      "Epoch 3861/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5573 - class_loss: 0.0034 - l1_loss: 0.3554\n",
      "Epoch 3862/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6521 - class_loss: 0.0011 - l1_loss: 0.3651ETA: 0s - loss: 4.2086 - class_loss: 0.0011 - l1_loss: 0.\n",
      "Epoch 3863/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2858 - class_loss: 0.0029 - l1_loss: 0.4283\n",
      "Epoch 3864/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7423 - class_loss: 0.0049 - l1_loss: 0.2737\n",
      "Epoch 3865/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2081 - class_loss: 0.0022 - l1_loss: 0.3206\n",
      "Epoch 3866/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0073 - class_loss: 0.0029 - l1_loss: 0.3004\n",
      "Epoch 3867/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5194 - class_loss: 0.0033 - l1_loss: 0.3516\n",
      "Epoch 3868/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0266 - class_loss: 0.0025 - l1_loss: 0.2024\n",
      "Epoch 3869/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2936 - class_loss: 0.0059 - l1_loss: 0.2288\n",
      "Epoch 3870/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.2809 - class_loss: 0.0021 - l1_loss: 0.2279\n",
      "Epoch 3871/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8272 - class_loss: 0.0019 - l1_loss: 0.1825\n",
      "Epoch 3872/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5896 - class_loss: 0.0028 - l1_loss: 0.1587\n",
      "Epoch 3873/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8503 - class_loss: 9.5890e-04 - l1_loss: 0.1849\n",
      "Epoch 3874/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.7770 - class_loss: 0.0030 - l1_loss: 0.1774\n",
      "Epoch 3875/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.8265 - class_loss: 0.0028 - l1_loss: 0.1824\n",
      "Epoch 3876/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.8768 - class_loss: 0.0112 - l1_loss: 0.1866\n",
      "Epoch 3877/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0688 - class_loss: 0.0038 - l1_loss: 0.2065\n",
      "Epoch 3878/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8691 - class_loss: 0.0014 - l1_loss: 0.1868\n",
      "Epoch 3879/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7829 - class_loss: 0.0011 - l1_loss: 0.1782\n",
      "Epoch 3880/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5353 - class_loss: 0.0017 - l1_loss: 0.1534ETA: 0s - loss: 1.5146 - class_loss: 0.0015 - l1_loss: 0.15\n",
      "Epoch 3881/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8508 - class_loss: 0.0021 - l1_loss: 0.1849\n",
      "Epoch 3882/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6850 - class_loss: 0.0031 - l1_loss: 0.1682\n",
      "Epoch 3883/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5340 - class_loss: 8.4026e-04 - l1_loss: 0.1533\n",
      "Epoch 3884/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6058 - class_loss: 0.0012 - l1_loss: 0.1605\n",
      "Epoch 3885/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5923 - class_loss: 0.0023 - l1_loss: 0.1590\n",
      "Epoch 3886/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.6131 - class_loss: 0.0023 - l1_loss: 0.1611\n",
      "Epoch 3887/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6135 - class_loss: 0.0042 - l1_loss: 0.1609\n",
      "Epoch 3888/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.4773 - class_loss: 0.0046 - l1_loss: 0.1473\n",
      "Epoch 3889/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6292 - class_loss: 0.0017 - l1_loss: 0.1627\n",
      "Epoch 3890/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4976 - class_loss: 0.0011 - l1_loss: 0.1497\n",
      "Epoch 3891/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1779 - class_loss: 0.0013 - l1_loss: 0.2177\n",
      "Epoch 3892/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6276 - class_loss: 0.0016 - l1_loss: 0.1626\n",
      "Epoch 3893/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6715 - class_loss: 0.0011 - l1_loss: 0.1670\n",
      "Epoch 3894/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.0787 - class_loss: 0.0024 - l1_loss: 0.2076\n",
      "Epoch 3895/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4673 - class_loss: 0.0019 - l1_loss: 0.1465\n",
      "Epoch 3896/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8199 - class_loss: 6.1648e-04 - l1_loss: 0.1819\n",
      "Epoch 3897/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6491 - class_loss: 0.0011 - l1_loss: 0.1648\n",
      "Epoch 3898/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7519 - class_loss: 4.8364e-04 - l1_loss: 0.1751\n",
      "Epoch 3899/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1638 - class_loss: 6.3358e-04 - l1_loss: 0.2163\n",
      "Epoch 3900/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1957 - class_loss: 0.0011 - l1_loss: 0.2195\n",
      "Epoch 3901/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4665 - class_loss: 0.0043 - l1_loss: 0.1462\n",
      "Epoch 3902/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8214 - class_loss: 0.0086 - l1_loss: 0.1813\n",
      "Epoch 3903/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4888 - class_loss: 0.0027 - l1_loss: 0.1486\n",
      "Epoch 3904/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2241 - class_loss: 0.0036 - l1_loss: 0.1221\n",
      "Epoch 3905/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3324 - class_loss: 0.0030 - l1_loss: 0.1329\n",
      "Epoch 3906/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1714 - class_loss: 0.0038 - l1_loss: 0.1168\n",
      "Epoch 3907/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0756 - class_loss: 0.0032 - l1_loss: 0.1072\n",
      "Epoch 3908/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9736 - class_loss: 0.0029 - l1_loss: 0.0971\n",
      "Epoch 3909/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1373 - class_loss: 0.0021 - l1_loss: 0.1135\n",
      "Epoch 3910/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4281 - class_loss: 0.0029 - l1_loss: 0.1425\n",
      "Epoch 3911/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9985 - class_loss: 0.0012 - l1_loss: 0.0997\n",
      "Epoch 3912/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1119 - class_loss: 0.0020 - l1_loss: 0.1110\n",
      "Epoch 3913/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0934 - class_loss: 9.8528e-04 - l1_loss: 0.1092\n",
      "Epoch 3914/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.5286 - class_loss: 0.0011 - l1_loss: 0.1528\n",
      "Epoch 3915/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.1423 - class_loss: 0.0019 - l1_loss: 0.1140\n",
      "Epoch 3916/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4198 - class_loss: 0.0050 - l1_loss: 0.1415\n",
      "Epoch 3917/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8894 - class_loss: 0.0032 - l1_loss: 0.1886\n",
      "Epoch 3918/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1378 - class_loss: 0.0030 - l1_loss: 0.1135\n",
      "Epoch 3919/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5347 - class_loss: 0.0016 - l1_loss: 0.1533\n",
      "Epoch 3920/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5829 - class_loss: 9.0458e-04 - l1_loss: 0.1582\n",
      "Epoch 3921/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.1642 - class_loss: 0.0010 - l1_loss: 0.3163\n",
      "Epoch 3922/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.7340 - class_loss: 0.0011 - l1_loss: 0.2733\n",
      "Epoch 3923/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.3719 - class_loss: 0.0018 - l1_loss: 0.2370\n",
      "Epoch 3924/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9138 - class_loss: 0.0025 - l1_loss: 0.2911\n",
      "Epoch 3925/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5016 - class_loss: 0.0041 - l1_loss: 0.4497\n",
      "Epoch 3926/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.3098 - class_loss: 0.0090 - l1_loss: 0.5301\n",
      "Epoch 3927/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9183 - class_loss: 0.0028 - l1_loss: 0.4915\n",
      "Epoch 3928/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.0679 - class_loss: 5.2731e-04 - l1_loss: 0.9067\n",
      "Epoch 3929/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.5988 - class_loss: 1.5244e-04 - l1_loss: 1.0599\n",
      "Epoch 3930/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.9430 - class_loss: 1.2987e-04 - l1_loss: 1.0943\n",
      "Epoch 3931/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.3473 - class_loss: 2.2967e-04 - l1_loss: 1.0347\n",
      "Epoch 3932/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.0142 - class_loss: 3.6981e-04 - l1_loss: 1.0014\n",
      "Epoch 3933/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.0403 - class_loss: 9.4271e-05 - l1_loss: 0.9040\n",
      "Epoch 3934/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.1524 - class_loss: 6.0649e-04 - l1_loss: 0.7152 0s - loss: 3.6758 - class_loss: 8.4434e-05 - l1_loss: \n",
      "Epoch 3935/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.1387 - class_loss: 0.0026 - l1_loss: 0.9136\n",
      "Epoch 3936/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.5040 - class_loss: 0.0054 - l1_loss: 0.7499\n",
      "Epoch 3937/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.2566 - class_loss: 0.0067 - l1_loss: 0.6250\n",
      "Epoch 3938/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.5230 - class_loss: 0.0032 - l1_loss: 0.5520\n",
      "Epoch 3939/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.2281 - class_loss: 3.2034e-04 - l1_loss: 0.7228\n",
      "Epoch 3940/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.7808 - class_loss: 1.6286e-04 - l1_loss: 0.7781\n",
      "Epoch 3941/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.5435 - class_loss: 1.9585e-04 - l1_loss: 0.6543\n",
      "Epoch 3942/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.6037 - class_loss: 3.3521e-04 - l1_loss: 0.7603\n",
      "Epoch 3943/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.1393 - class_loss: 0.0015 - l1_loss: 0.6138\n",
      "Epoch 3944/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.1788 - class_loss: 0.0133 - l1_loss: 0.5166\n",
      "Epoch 3945/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.1166 - class_loss: 0.0017 - l1_loss: 0.6115\n",
      "Epoch 3946/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.4560 - class_loss: 0.0092 - l1_loss: 0.7447\n",
      "Epoch 3947/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.4111 - class_loss: 0.0070 - l1_loss: 0.4404\n",
      "Epoch 3948/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.7945 - class_loss: 0.0021 - l1_loss: 0.5792\n",
      "Epoch 3949/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.9937 - class_loss: 0.0070 - l1_loss: 0.6987\n",
      "Epoch 3950/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.5163 - class_loss: 0.0018 - l1_loss: 0.7515\n",
      "Epoch 3951/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.0995 - class_loss: 0.0028 - l1_loss: 0.7097\n",
      "Epoch 3952/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.7257 - class_loss: 0.0020 - l1_loss: 0.4724\n",
      "Epoch 3953/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.7481 - class_loss: 0.0053 - l1_loss: 0.5743\n",
      "Epoch 3954/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.9035 - class_loss: 0.0094 - l1_loss: 0.4894\n",
      "Epoch 3955/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 6.9359 - class_loss: 0.0027 - l1_loss: 0.6933\n",
      "Epoch 3956/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.3100 - class_loss: 0.0015 - l1_loss: 0.6308\n",
      "Epoch 3957/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.2438 - class_loss: 5.4293e-04 - l1_loss: 0.9243\n",
      "Epoch 3958/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.7318 - class_loss: 0.0014 - l1_loss: 1.0730\n",
      "Epoch 3959/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.0460 - class_loss: 0.0053 - l1_loss: 1.0041\n",
      "Epoch 3960/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.3234 - class_loss: 0.0082 - l1_loss: 1.5315\n",
      "Epoch 3961/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 22.1234 - class_loss: 0.0025 - l1_loss: 2.2121\n",
      "Epoch 3962/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 27.5624 - class_loss: 0.2047 - l1_loss: 2.7358\n",
      "Epoch 3963/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 34.6987 - class_loss: 8.3608e-04 - l1_loss: 3.4698\n",
      "Epoch 3964/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 25.4918 - class_loss: 0.0086 - l1_loss: 2.5483\n",
      "Epoch 3965/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 27.6315 - class_loss: 0.0019 - l1_loss: 2.7630\n",
      "Epoch 3966/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 89.1710 - class_loss: 0.0153 - l1_loss: 8.9156\n",
      "Epoch 3967/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 53.9206 - class_loss: 0.0038 - l1_loss: 5.3917\n",
      "Epoch 3968/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 43.7274 - class_loss: 0.2394 - l1_loss: 4.3488\n",
      "Epoch 3969/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 34.8262 - class_loss: 0.0111 - l1_loss: 3.4815\n",
      "Epoch 3970/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 37.3347 - class_loss: 0.0023 - l1_loss: 3.7332\n",
      "Epoch 3971/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 39.7246 - class_loss: 9.1775e-04 - l1_loss: 3.9724: 0s - loss: 49.3711 - class_loss: 0.0015 - l1_loss: 4.93\n",
      "Epoch 3972/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 31ms/step - loss: 36.9585 - class_loss: 0.3370 - l1_loss: 3.6621\n",
      "Epoch 3973/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 36.7215 - class_loss: 0.0978 - l1_loss: 3.6624\n",
      "Epoch 3974/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 43.8344 - class_loss: 0.1219 - l1_loss: 4.3713: 0s - loss: 43.6464 - class_loss: 0.1363 - l1_loss: 4.351\n",
      "Epoch 3975/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 30.0014 - class_loss: 0.0092 - l1_loss: 2.9992: 0s - loss: 24.9700 - class_loss: 0.0216 - l1_loss: 2.4\n",
      "Epoch 3976/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 31.7267 - class_loss: 4.0301e-04 - l1_loss: 3.1726\n",
      "Epoch 3977/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 44.4803 - class_loss: 0.0105 - l1_loss: 4.4470\n",
      "Epoch 3978/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 36.9446 - class_loss: 0.0641 - l1_loss: 3.6880\n",
      "Epoch 3979/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 30.1742 - class_loss: 8.9528e-04 - l1_loss: 3.0173\n",
      "Epoch 3980/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 35.7774 - class_loss: 0.0019 - l1_loss: 3.5775\n",
      "Epoch 3981/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.5206 - class_loss: 7.7432e-05 - l1_loss: 1.6521\n",
      "Epoch 3982/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.9326 - class_loss: 0.0011 - l1_loss: 1.5931\n",
      "Epoch 3983/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 13.6450 - class_loss: 0.0126 - l1_loss: 1.3632\n",
      "Epoch 3984/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 25.0323 - class_loss: 0.0109 - l1_loss: 2.5021\n",
      "Epoch 3985/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 19.7766 - class_loss: 0.0023 - l1_loss: 1.9774\n",
      "Epoch 3986/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 20.9287 - class_loss: 4.2975e-04 - l1_loss: 2.0928\n",
      "Epoch 3987/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 23.5077 - class_loss: 0.0374 - l1_loss: 2.3470\n",
      "Epoch 3988/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 27.1128 - class_loss: 0.1633 - l1_loss: 2.6950\n",
      "Epoch 3989/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 17.9069 - class_loss: 0.0334 - l1_loss: 1.7874\n",
      "Epoch 3990/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.7250 - class_loss: 0.0242 - l1_loss: 1.7701\n",
      "Epoch 3991/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.9104 - class_loss: 0.0019 - l1_loss: 1.7908\n",
      "Epoch 3992/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.3668 - class_loss: 0.0460 - l1_loss: 1.4321\n",
      "Epoch 3993/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.6078 - class_loss: 0.0047 - l1_loss: 1.4603\n",
      "Epoch 3994/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 17.7502 - class_loss: 0.0066 - l1_loss: 1.7744: 0s - loss: 21.7505 - class_loss: 0.0104 - l1_loss: 2.17\n",
      "Epoch 3995/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.7343 - class_loss: 5.9985e-04 - l1_loss: 1.4734\n",
      "Epoch 3996/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.2265 - class_loss: 0.0071 - l1_loss: 1.2219\n",
      "Epoch 3997/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.2255 - class_loss: 0.0359 - l1_loss: 1.1190\n",
      "Epoch 3998/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.4438 - class_loss: 0.0159 - l1_loss: 1.1428\n",
      "Epoch 3999/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.8427 - class_loss: 6.1977e-04 - l1_loss: 0.7842\n",
      "Epoch 4000/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.9245 - class_loss: 2.0063e-04 - l1_loss: 0.6924\n",
      "Epoch 4001/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.3821 - class_loss: 0.0025 - l1_loss: 0.6380\n",
      "Epoch 4002/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8124 - class_loss: 0.0173 - l1_loss: 0.4795\n",
      "Epoch 4003/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1801 - class_loss: 0.0031 - l1_loss: 0.4177\n",
      "Epoch 4004/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0207 - class_loss: 1.9516e-04 - l1_loss: 0.4021\n",
      "Epoch 4005/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2536 - class_loss: 0.0020 - l1_loss: 0.3252\n",
      "Epoch 4006/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.8758 - class_loss: 0.0032 - l1_loss: 0.4873\n",
      "Epoch 4007/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4508 - class_loss: 0.0036 - l1_loss: 0.3447\n",
      "Epoch 4008/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.5190 - class_loss: 0.0015 - l1_loss: 0.4518\n",
      "Epoch 4009/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7074 - class_loss: 0.0025 - l1_loss: 0.2705\n",
      "Epoch 4010/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9611 - class_loss: 0.0048 - l1_loss: 0.2956\n",
      "Epoch 4011/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7946 - class_loss: 0.0026 - l1_loss: 0.1792\n",
      "Epoch 4012/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.0371 - class_loss: 0.0047 - l1_loss: 0.2032\n",
      "Epoch 4013/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.5403 - class_loss: 9.9266e-04 - l1_loss: 0.1539\n",
      "Epoch 4014/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.3510 - class_loss: 0.0040 - l1_loss: 0.1347\n",
      "Epoch 4015/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.2776 - class_loss: 0.0013 - l1_loss: 0.1276\n",
      "Epoch 4016/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.1496 - class_loss: 0.0021 - l1_loss: 0.1148\n",
      "Epoch 4017/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.1718 - class_loss: 0.0015 - l1_loss: 0.1170\n",
      "Epoch 4018/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6628 - class_loss: 0.0019 - l1_loss: 0.1661\n",
      "Epoch 4019/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2573 - class_loss: 0.0026 - l1_loss: 0.1255\n",
      "Epoch 4020/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5482 - class_loss: 0.0035 - l1_loss: 0.1545\n",
      "Epoch 4021/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.4181 - class_loss: 0.0014 - l1_loss: 0.1417\n",
      "Epoch 4022/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2050 - class_loss: 5.0925e-04 - l1_loss: 0.1205\n",
      "Epoch 4023/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0756 - class_loss: 4.8008e-04 - l1_loss: 0.1075\n",
      "Epoch 4024/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9063 - class_loss: 8.0559e-04 - l1_loss: 0.0906\n",
      "Epoch 4025/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.8971 - class_loss: 0.0022 - l1_loss: 0.0895\n",
      "Epoch 4026/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9422 - class_loss: 0.0013 - l1_loss: 0.0941\n",
      "Epoch 4027/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3491 - class_loss: 0.0014 - l1_loss: 0.1348\n",
      "Epoch 4028/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9031 - class_loss: 9.4761e-04 - l1_loss: 0.0902\n",
      "Epoch 4029/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.8918 - class_loss: 0.0022 - l1_loss: 0.0890\n",
      "Epoch 4030/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7573 - class_loss: 4.8274e-04 - l1_loss: 0.0757\n",
      "Epoch 4031/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.7199 - class_loss: 7.0041e-04 - l1_loss: 0.0719\n",
      "Epoch 4032/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.7494 - class_loss: 4.5505e-04 - l1_loss: 0.0749\n",
      "Epoch 4033/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.7264 - class_loss: 8.2369e-04 - l1_loss: 0.0726\n",
      "Epoch 4034/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.7942 - class_loss: 0.0011 - l1_loss: 0.0793\n",
      "Epoch 4035/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.6888 - class_loss: 0.0023 - l1_loss: 0.0687\n",
      "Epoch 4036/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8702 - class_loss: 0.0056 - l1_loss: 0.0865\n",
      "Epoch 4037/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 30ms/step - loss: 0.7729 - class_loss: 7.8407e-04 - l1_loss: 0.0772\n",
      "Epoch 4038/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9837 - class_loss: 9.0578e-04 - l1_loss: 0.0983\n",
      "Epoch 4039/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1496 - class_loss: 7.2031e-04 - l1_loss: 0.1149\n",
      "Epoch 4040/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1204 - class_loss: 4.9530e-04 - l1_loss: 0.1120\n",
      "Epoch 4041/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1007 - class_loss: 0.0012 - l1_loss: 0.1099\n",
      "Epoch 4042/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1010 - class_loss: 0.0022 - l1_loss: 0.1099\n",
      "Epoch 4043/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6759 - class_loss: 0.0020 - l1_loss: 0.1674\n",
      "Epoch 4044/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3754 - class_loss: 0.0010 - l1_loss: 0.1374\n",
      "Epoch 4045/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0155 - class_loss: 0.0017 - l1_loss: 0.2014\n",
      "Epoch 4046/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2891 - class_loss: 0.0022 - l1_loss: 0.1287\n",
      "Epoch 4047/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.4163 - class_loss: 0.0036 - l1_loss: 0.2413 0s - loss: 2.3676 - class_loss: 0.0032 - l1_loss: 0.\n",
      "Epoch 4048/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9100 - class_loss: 0.0108 - l1_loss: 0.1899\n",
      "Epoch 4049/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5796 - class_loss: 0.0041 - l1_loss: 0.2575\n",
      "Epoch 4050/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4422 - class_loss: 0.0034 - l1_loss: 0.2439\n",
      "Epoch 4051/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1274 - class_loss: 0.0012 - l1_loss: 0.3126\n",
      "Epoch 4052/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3213 - class_loss: 0.0034 - l1_loss: 0.2318\n",
      "Epoch 4053/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.8031 - class_loss: 0.0016 - l1_loss: 0.2801 0s - loss: 2.8126 - class_loss: 0.0017 - l1_loss: 0.28\n",
      "Epoch 4054/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1829 - class_loss: 7.9166e-04 - l1_loss: 0.2182\n",
      "Epoch 4055/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1888 - class_loss: 4.5289e-04 - l1_loss: 0.2188\n",
      "Epoch 4056/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6412 - class_loss: 0.0017 - l1_loss: 0.1639\n",
      "Epoch 4057/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.3548 - class_loss: 0.0018 - l1_loss: 0.2353\n",
      "Epoch 4058/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8026 - class_loss: 0.0012 - l1_loss: 0.1801\n",
      "Epoch 4059/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0734 - class_loss: 0.0058 - l1_loss: 0.2068\n",
      "Epoch 4060/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2470 - class_loss: 3.9771e-04 - l1_loss: 0.1247\n",
      "Epoch 4061/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.4490 - class_loss: 4.1685e-04 - l1_loss: 0.1449\n",
      "Epoch 4062/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.2258 - class_loss: 1.6120e-04 - l1_loss: 0.1226\n",
      "Epoch 4063/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3040 - class_loss: 5.9770e-04 - l1_loss: 0.1303\n",
      "Epoch 4064/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1222 - class_loss: 8.6074e-04 - l1_loss: 0.1121\n",
      "Epoch 4065/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.2112 - class_loss: 0.0032 - l1_loss: 0.1208\n",
      "Epoch 4066/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3554 - class_loss: 0.0019 - l1_loss: 0.1354\n",
      "Epoch 4067/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6558 - class_loss: 0.0024 - l1_loss: 0.1653\n",
      "Epoch 4068/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5353 - class_loss: 0.0074 - l1_loss: 0.2528\n",
      "Epoch 4069/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1252 - class_loss: 0.0052 - l1_loss: 0.2120\n",
      "Epoch 4070/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6162 - class_loss: 0.0022 - l1_loss: 0.2614\n",
      "Epoch 4071/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0329 - class_loss: 0.0083 - l1_loss: 0.2025\n",
      "Epoch 4072/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4713 - class_loss: 0.0072 - l1_loss: 0.2464\n",
      "Epoch 4073/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9277 - class_loss: 9.0999e-04 - l1_loss: 0.1927\n",
      "Epoch 4074/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5907 - class_loss: 4.8138e-04 - l1_loss: 0.1590\n",
      "Epoch 4075/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5527 - class_loss: 9.5634e-04 - l1_loss: 0.1552\n",
      "Epoch 4076/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8663 - class_loss: 2.3007e-04 - l1_loss: 0.1866\n",
      "Epoch 4077/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7279 - class_loss: 3.6025e-04 - l1_loss: 0.1728\n",
      "Epoch 4078/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3282 - class_loss: 0.0011 - l1_loss: 0.1327A: 0s - loss: 1.2874 - class_loss: 0.0013 - l1_loss: 0.12\n",
      "Epoch 4079/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9100 - class_loss: 0.0015 - l1_loss: 0.1909\n",
      "Epoch 4080/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4481 - class_loss: 0.0019 - l1_loss: 0.1446\n",
      "Epoch 4081/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0862 - class_loss: 0.0016 - l1_loss: 0.2085\n",
      "Epoch 4082/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6626 - class_loss: 0.0042 - l1_loss: 0.1658\n",
      "Epoch 4083/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4536 - class_loss: 0.0051 - l1_loss: 0.1449\n",
      "Epoch 4084/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.8288 - class_loss: 6.2474e-04 - l1_loss: 0.1828\n",
      "Epoch 4085/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6379 - class_loss: 6.4757e-04 - l1_loss: 0.1637\n",
      "Epoch 4086/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5546 - class_loss: 0.0025 - l1_loss: 0.1552 0s - loss: 1.5823 - class_loss: 0.0028 - l1_loss: 0.15\n",
      "Epoch 4087/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9336 - class_loss: 0.0018 - l1_loss: 0.1932\n",
      "Epoch 4088/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2467 - class_loss: 7.4318e-04 - l1_loss: 0.2246\n",
      "Epoch 4089/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.2210 - class_loss: 7.0226e-04 - l1_loss: 0.2220 0s - loss: 2.0789 - class_loss: 0.0010 - l1_loss: 0.\n",
      "Epoch 4090/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9714 - class_loss: 0.0016 - l1_loss: 0.2970\n",
      "Epoch 4091/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1897 - class_loss: 0.0019 - l1_loss: 0.2188\n",
      "Epoch 4092/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0766 - class_loss: 6.4097e-04 - l1_loss: 0.2076\n",
      "Epoch 4093/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9197 - class_loss: 8.9264e-04 - l1_loss: 0.1919 0s - loss: 1.7598 - class_loss: 2.9498e-04 - l1_loss: \n",
      "Epoch 4094/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6467 - class_loss: 0.0026 - l1_loss: 0.1644\n",
      "Epoch 4095/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4968 - class_loss: 0.0036 - l1_loss: 0.1493\n",
      "Epoch 4096/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2848 - class_loss: 0.0046 - l1_loss: 0.1280\n",
      "Epoch 4097/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4283 - class_loss: 0.0026 - l1_loss: 0.1426\n",
      "Epoch 4098/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.8092 - class_loss: 0.0013 - l1_loss: 0.1808\n",
      "Epoch 4099/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3715 - class_loss: 0.0010 - l1_loss: 0.1370\n",
      "Epoch 4100/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4500 - class_loss: 3.2474e-04 - l1_loss: 0.1450\n",
      "Epoch 4101/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1309 - class_loss: 7.3262e-04 - l1_loss: 0.1130\n",
      "Epoch 4102/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9765 - class_loss: 0.0017 - l1_loss: 0.1975\n",
      "Epoch 4103/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5067 - class_loss: 0.0033 - l1_loss: 0.1503 0s - loss: 1.3444 - class_loss: 4.9009e-04 - l1_loss: \n",
      "Epoch 4104/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1691 - class_loss: 0.0035 - l1_loss: 0.2166 0s - loss: 2.2688 - class_loss: 0.0039 - l1_loss: 0.22\n",
      "Epoch 4105/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4339 - class_loss: 0.0011 - l1_loss: 0.2433\n",
      "Epoch 4106/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.5701 - class_loss: 0.0018 - l1_loss: 0.2568\n",
      "Epoch 4107/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6094 - class_loss: 0.0080 - l1_loss: 0.2601\n",
      "Epoch 4108/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5168 - class_loss: 0.0012 - l1_loss: 0.2516\n",
      "Epoch 4109/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8289 - class_loss: 0.0012 - l1_loss: 0.1828\n",
      "Epoch 4110/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0315 - class_loss: 0.0090 - l1_loss: 0.2023\n",
      "Epoch 4111/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1366 - class_loss: 0.0028 - l1_loss: 0.2134\n",
      "Epoch 4112/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1763 - class_loss: 7.1968e-04 - l1_loss: 0.2176\n",
      "Epoch 4113/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7688 - class_loss: 0.0017 - l1_loss: 0.1767\n",
      "Epoch 4114/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9402 - class_loss: 0.0020 - l1_loss: 0.1938\n",
      "Epoch 4115/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8229 - class_loss: 2.8270e-04 - l1_loss: 0.1823\n",
      "Epoch 4116/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.2026 - class_loss: 4.3851e-04 - l1_loss: 0.3202 0s - loss: 3.6002 - class_loss: 2.4222e-04 - l1_loss: \n",
      "Epoch 4117/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6263 - class_loss: 0.0015 - l1_loss: 0.2625\n",
      "Epoch 4118/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4074 - class_loss: 0.0013 - l1_loss: 0.2406\n",
      "Epoch 4119/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4838 - class_loss: 0.0016 - l1_loss: 0.3482\n",
      "Epoch 4120/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.1498 - class_loss: 7.7081e-04 - l1_loss: 0.4149\n",
      "Epoch 4121/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9568 - class_loss: 7.0552e-04 - l1_loss: 0.2956\n",
      "Epoch 4122/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1990 - class_loss: 0.0030 - l1_loss: 0.4196\n",
      "Epoch 4123/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0545 - class_loss: 0.0029 - l1_loss: 0.3052\n",
      "Epoch 4124/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3250 - class_loss: 9.1427e-04 - l1_loss: 0.3324\n",
      "Epoch 4125/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.1056 - class_loss: 7.0414e-04 - l1_loss: 0.4105\n",
      "Epoch 4126/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.0133 - class_loss: 0.0015 - l1_loss: 0.5012\n",
      "Epoch 4127/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0596 - class_loss: 0.0092 - l1_loss: 0.4050\n",
      "Epoch 4128/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.6206 - class_loss: 8.0888e-04 - l1_loss: 0.4620\n",
      "Epoch 4129/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.0609 - class_loss: 2.9593e-04 - l1_loss: 0.5061\n",
      "Epoch 4130/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.8019 - class_loss: 0.0034 - l1_loss: 0.4799\n",
      "Epoch 4131/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.4119 - class_loss: 0.0017 - l1_loss: 0.4410\n",
      "Epoch 4132/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.3785 - class_loss: 0.0012 - l1_loss: 0.4377 0s - loss: 4.2562 - class_loss: 0.0011 - l1_loss: \n",
      "Epoch 4133/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.6956 - class_loss: 4.7498e-04 - l1_loss: 0.3695\n",
      "Epoch 4134/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3797 - class_loss: 5.0300e-04 - l1_loss: 0.3379\n",
      "Epoch 4135/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.5217 - class_loss: 0.0026 - l1_loss: 0.3519\n",
      "Epoch 4136/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.9302 - class_loss: 0.0108 - l1_loss: 0.4919\n",
      "Epoch 4137/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.7034 - class_loss: 0.0016 - l1_loss: 0.4702ETA: 0s - loss: 4.7939 - class_loss: 0.0018 - l1_loss: 0.4792  \n",
      "Epoch 4138/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5433 - class_loss: 9.3093e-04 - l1_loss: 0.3542\n",
      "Epoch 4139/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2461 - class_loss: 0.0015 - l1_loss: 0.4245\n",
      "Epoch 4140/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.0515 - class_loss: 0.0060 - l1_loss: 0.4045\n",
      "Epoch 4141/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1623 - class_loss: 0.0029 - l1_loss: 0.3159\n",
      "Epoch 4142/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.5794 - class_loss: 3.8872e-04 - l1_loss: 0.4579 0s - loss: 4.9784 - class_loss: 3.5579e-04 - l1_loss: 0.\n",
      "Epoch 4143/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.4570 - class_loss: 0.0016 - l1_loss: 0.5455\n",
      "Epoch 4144/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1771 - class_loss: 0.0022 - l1_loss: 0.4175\n",
      "Epoch 4145/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.6239 - class_loss: 6.3591e-04 - l1_loss: 0.4623\n",
      "Epoch 4146/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.3801 - class_loss: 0.0017 - l1_loss: 0.4378\n",
      "Epoch 4147/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.6582 - class_loss: 0.0024 - l1_loss: 0.5656\n",
      "Epoch 4148/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.6196 - class_loss: 5.6968e-04 - l1_loss: 0.7619\n",
      "Epoch 4149/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.4601 - class_loss: 2.7652e-04 - l1_loss: 0.7460 0s - loss: 5.7025 - class_loss: 2.6362e-04 - l1_loss: 0.\n",
      "Epoch 4150/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.9445 - class_loss: 0.0041 - l1_loss: 1.6940\n",
      "Epoch 4151/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 22.2307 - class_loss: 0.0016 - l1_loss: 2.2229\n",
      "Epoch 4152/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 30.8716 - class_loss: 0.0054 - l1_loss: 3.0866\n",
      "Epoch 4153/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 37.3947 - class_loss: 0.0188 - l1_loss: 3.7376TA: 0s - loss: 48.6111 - class_loss: 0.0297 - l1_loss: 4.85\n",
      "Epoch 4154/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.3392 - class_loss: 2.1523e-04 - l1_loss: 1.4339\n",
      "Epoch 4155/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 17.0968 - class_loss: 0.0012 - l1_loss: 1.7096\n",
      "Epoch 4156/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.8142 - class_loss: 0.0381 - l1_loss: 1.7776\n",
      "Epoch 4157/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.3428 - class_loss: 0.0031 - l1_loss: 1.5340\n",
      "Epoch 4158/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 12.6705 - class_loss: 0.0023 - l1_loss: 1.2668\n",
      "Epoch 4159/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 24.6091 - class_loss: 1.9462e-04 - l1_loss: 2.4609\n",
      "Epoch 4160/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.8509 - class_loss: 9.7220e-04 - l1_loss: 1.5850\n",
      "Epoch 4161/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.0482 - class_loss: 5.8875e-04 - l1_loss: 1.2048\n",
      "Epoch 4162/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 28ms/step - loss: 13.6046 - class_loss: 0.0040 - l1_loss: 1.3601\n",
      "Epoch 4163/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 13.7230 - class_loss: 0.0019 - l1_loss: 1.3721\n",
      "Epoch 4164/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.9110 - class_loss: 1.3748e-04 - l1_loss: 1.2911\n",
      "Epoch 4165/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 15.5062 - class_loss: 0.0018 - l1_loss: 1.5504\n",
      "Epoch 4166/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.5773 - class_loss: 1.9854e-04 - l1_loss: 1.1577: 0s - loss: 12.5155 - class_loss: 1.3225e-04 - l1_loss: 1.25\n",
      "Epoch 4167/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.5322 - class_loss: 0.0527 - l1_loss: 0.8479\n",
      "Epoch 4168/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.4365 - class_loss: 0.0187 - l1_loss: 1.1418\n",
      "Epoch 4169/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.2777 - class_loss: 0.0265 - l1_loss: 1.2251\n",
      "Epoch 4170/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 10.0414 - class_loss: 6.3532e-05 - l1_loss: 1.0041\n",
      "Epoch 4171/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.6128 - class_loss: 3.0564e-04 - l1_loss: 0.8613\n",
      "Epoch 4172/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.4467 - class_loss: 0.0014 - l1_loss: 0.8445\n",
      "Epoch 4173/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.0730 - class_loss: 0.0020 - l1_loss: 0.7071\n",
      "Epoch 4174/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.6517 - class_loss: 5.5580e-04 - l1_loss: 0.5651\n",
      "Epoch 4175/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.1860 - class_loss: 0.0127 - l1_loss: 0.5173\n",
      "Epoch 4176/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.3559 - class_loss: 0.0048 - l1_loss: 0.5351\n",
      "Epoch 4177/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.3205 - class_loss: 0.0057 - l1_loss: 0.5315\n",
      "Epoch 4178/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.1613 - class_loss: 0.0215 - l1_loss: 0.5140\n",
      "Epoch 4179/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 6.3202 - class_loss: 0.0051 - l1_loss: 0.6315\n",
      "Epoch 4180/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.8519 - class_loss: 3.6933e-04 - l1_loss: 0.3852\n",
      "Epoch 4181/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.7655 - class_loss: 4.0509e-04 - l1_loss: 0.4765\n",
      "Epoch 4182/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 4.9066 - class_loss: 0.0065 - l1_loss: 0.4900\n",
      "Epoch 4183/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.2972 - class_loss: 0.0020 - l1_loss: 0.3295\n",
      "Epoch 4184/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.3914 - class_loss: 0.0022 - l1_loss: 0.4389\n",
      "Epoch 4185/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.2816 - class_loss: 0.0022 - l1_loss: 0.4279\n",
      "Epoch 4186/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.1395 - class_loss: 7.1027e-04 - l1_loss: 0.4139\n",
      "Epoch 4187/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.6376 - class_loss: 0.0124 - l1_loss: 0.3625\n",
      "Epoch 4188/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 4.7779 - class_loss: 5.9683e-04 - l1_loss: 0.4777\n",
      "Epoch 4189/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.1480 - class_loss: 3.8017e-04 - l1_loss: 0.5148\n",
      "Epoch 4190/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.3028 - class_loss: 0.0013 - l1_loss: 0.5301\n",
      "Epoch 4191/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 5.4351 - class_loss: 0.0077 - l1_loss: 0.5427\n",
      "Epoch 4192/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.9423 - class_loss: 0.0050 - l1_loss: 0.5937\n",
      "Epoch 4193/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.8156 - class_loss: 0.0030 - l1_loss: 0.5813\n",
      "Epoch 4194/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 7.5952 - class_loss: 0.0035 - l1_loss: 0.7592\n",
      "Epoch 4195/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.9626 - class_loss: 0.0015 - l1_loss: 0.5961\n",
      "Epoch 4196/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.8716 - class_loss: 0.0018 - l1_loss: 0.5870\n",
      "Epoch 4197/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.2007 - class_loss: 2.9222e-04 - l1_loss: 0.6200\n",
      "Epoch 4198/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.4468 - class_loss: 0.0010 - l1_loss: 0.6446\n",
      "Epoch 4199/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 10.1104 - class_loss: 1.6929e-04 - l1_loss: 1.0110\n",
      "Epoch 4200/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.1690 - class_loss: 0.0059 - l1_loss: 0.8163\n",
      "Epoch 4201/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.2370 - class_loss: 0.0108 - l1_loss: 1.2226\n",
      "Epoch 4202/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 17.2216 - class_loss: 0.0068 - l1_loss: 1.7215\n",
      "Epoch 4203/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 12.9464 - class_loss: 0.0011 - l1_loss: 1.2945\n",
      "Epoch 4204/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 11.9371 - class_loss: 8.5270e-04 - l1_loss: 1.1936\n",
      "Epoch 4205/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 15.5675 - class_loss: 9.4542e-04 - l1_loss: 1.5567\n",
      "Epoch 4206/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 22.6099 - class_loss: 0.7553 - l1_loss: 2.1855\n",
      "Epoch 4207/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.8272 - class_loss: 0.3525 - l1_loss: 1.7475\n",
      "Epoch 4208/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 19.1723 - class_loss: 0.3135 - l1_loss: 1.8859\n",
      "Epoch 4209/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 15.7170 - class_loss: 0.0666 - l1_loss: 1.5650\n",
      "Epoch 4210/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.3602 - class_loss: 0.0441 - l1_loss: 1.2316\n",
      "Epoch 4211/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.1075 - class_loss: 0.0088 - l1_loss: 1.1099\n",
      "Epoch 4212/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.8588 - class_loss: 6.1231e-04 - l1_loss: 0.9858\n",
      "Epoch 4213/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.4586 - class_loss: 9.4584e-05 - l1_loss: 0.7459\n",
      "Epoch 4214/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.7143 - class_loss: 8.5013e-04 - l1_loss: 0.5713\n",
      "Epoch 4215/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.1925 - class_loss: 3.6121e-04 - l1_loss: 0.7192\n",
      "Epoch 4216/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 6.5176 - class_loss: 3.2927e-04 - l1_loss: 0.6517\n",
      "Epoch 4217/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.1438 - class_loss: 0.0015 - l1_loss: 0.5142\n",
      "Epoch 4218/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.6592 - class_loss: 0.0089 - l1_loss: 0.3650\n",
      "Epoch 4219/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.2719 - class_loss: 0.0202 - l1_loss: 0.4252\n",
      "Epoch 4220/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.9039 - class_loss: 0.0073 - l1_loss: 0.3897\n",
      "Epoch 4221/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.0704 - class_loss: 0.0909 - l1_loss: 0.3979\n",
      "Epoch 4222/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.8369 - class_loss: 2.2919e-04 - l1_loss: 0.3837\n",
      "Epoch 4223/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.3754 - class_loss: 0.0011 - l1_loss: 0.3374\n",
      "Epoch 4224/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.6468 - class_loss: 7.1055e-04 - l1_loss: 0.3646\n",
      "Epoch 4225/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7869 - class_loss: 7.9924e-04 - l1_loss: 0.2786\n",
      "Epoch 4226/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.3826 - class_loss: 1.0276e-05 - l1_loss: 0.3383\n",
      "Epoch 4227/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.9992 - class_loss: 2.2606e-04 - l1_loss: 0.2999\n",
      "Epoch 4228/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.9215 - class_loss: 7.5641e-05 - l1_loss: 0.3921\n",
      "Epoch 4229/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.5008 - class_loss: 4.0276e-04 - l1_loss: 0.3500\n",
      "Epoch 4230/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.0310 - class_loss: 5.3274e-04 - l1_loss: 0.4030\n",
      "Epoch 4231/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.5687 - class_loss: 4.0642e-04 - l1_loss: 0.3568\n",
      "Epoch 4232/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.6635 - class_loss: 0.0188 - l1_loss: 0.5645\n",
      "Epoch 4233/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.5670 - class_loss: 0.0041 - l1_loss: 0.3563\n",
      "Epoch 4234/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.1944 - class_loss: 0.0224 - l1_loss: 0.6172\n",
      "Epoch 4235/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.9430 - class_loss: 0.0090 - l1_loss: 0.3934\n",
      "Epoch 4236/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.1691 - class_loss: 6.0173e-04 - l1_loss: 0.6168\n",
      "Epoch 4237/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.0845 - class_loss: 2.8497e-04 - l1_loss: 0.6084\n",
      "Epoch 4238/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.7518 - class_loss: 0.0029 - l1_loss: 0.3749\n",
      "Epoch 4239/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.0481 - class_loss: 0.0034 - l1_loss: 0.5045\n",
      "Epoch 4240/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.4309 - class_loss: 9.2501e-04 - l1_loss: 0.5430\n",
      "Epoch 4241/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 8.6544 - class_loss: 0.0011 - l1_loss: 0.8653\n",
      "Epoch 4242/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.2085 - class_loss: 0.0031 - l1_loss: 0.6205\n",
      "Epoch 4243/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.8008 - class_loss: 0.0200 - l1_loss: 0.3781\n",
      "Epoch 4244/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.4844 - class_loss: 0.0015 - l1_loss: 0.5483\n",
      "Epoch 4245/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.1416 - class_loss: 0.0011 - l1_loss: 0.5140\n",
      "Epoch 4246/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.7179 - class_loss: 0.0031 - l1_loss: 0.4715\n",
      "Epoch 4247/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8205 - class_loss: 8.6822e-04 - l1_loss: 0.4820\n",
      "Epoch 4248/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.7453 - class_loss: 0.0034 - l1_loss: 0.6742\n",
      "Epoch 4249/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 8.9918 - class_loss: 0.0014 - l1_loss: 0.8990\n",
      "Epoch 4250/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.7635 - class_loss: 0.0013 - l1_loss: 0.5762\n",
      "Epoch 4251/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.1268 - class_loss: 0.0119 - l1_loss: 0.5115\n",
      "Epoch 4252/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 4.0981 - class_loss: 0.0012 - l1_loss: 0.4097\n",
      "Epoch 4253/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.9032 - class_loss: 3.4634e-04 - l1_loss: 0.4903\n",
      "Epoch 4254/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.0471 - class_loss: 0.0019 - l1_loss: 0.7045\n",
      "Epoch 4255/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.7655 - class_loss: 7.0739e-04 - l1_loss: 0.4765\n",
      "Epoch 4256/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.9143 - class_loss: 0.0013 - l1_loss: 0.3913\n",
      "Epoch 4257/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.4080 - class_loss: 2.7049e-04 - l1_loss: 0.3408\n",
      "Epoch 4258/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.2057 - class_loss: 0.0018 - l1_loss: 0.4204\n",
      "Epoch 4259/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.9694 - class_loss: 0.0030 - l1_loss: 0.2966\n",
      "Epoch 4260/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.0146 - class_loss: 0.0377 - l1_loss: 0.3977\n",
      "Epoch 4261/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.8257 - class_loss: 2.0890e-04 - l1_loss: 0.2826\n",
      "Epoch 4262/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.3435 - class_loss: 7.1567e-05 - l1_loss: 0.3343\n",
      "Epoch 4263/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4695 - class_loss: 2.6175e-04 - l1_loss: 0.2469\n",
      "Epoch 4264/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0851 - class_loss: 2.3290e-04 - l1_loss: 0.2085\n",
      "Epoch 4265/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0249 - class_loss: 6.4462e-04 - l1_loss: 0.2024\n",
      "Epoch 4266/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9506 - class_loss: 0.0020 - l1_loss: 0.1949\n",
      "Epoch 4267/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.1478 - class_loss: 0.0017 - l1_loss: 0.2146\n",
      "Epoch 4268/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5114 - class_loss: 0.0014 - l1_loss: 0.1510\n",
      "Epoch 4269/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9139 - class_loss: 0.0050 - l1_loss: 0.1909\n",
      "Epoch 4270/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9604 - class_loss: 0.0026 - l1_loss: 0.1958\n",
      "Epoch 4271/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9569 - class_loss: 0.0012 - l1_loss: 0.1956\n",
      "Epoch 4272/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9706 - class_loss: 0.0016 - l1_loss: 0.1969\n",
      "Epoch 4273/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8738 - class_loss: 7.1061e-04 - l1_loss: 0.1873\n",
      "Epoch 4274/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.6083 - class_loss: 3.5330e-04 - l1_loss: 0.2608\n",
      "Epoch 4275/5000\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.0144 - class_loss: 0.0027 - l1_loss: 0.2012 ETA: 0s - loss: 1.8744 - class_loss: 0.0030 - l1_loss: 0. - 0s 31ms/step - loss: 1.9040 - class_loss: 0.0025 - l1_loss: 0.1901\n",
      "Epoch 4276/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4147 - class_loss: 0.0025 - l1_loss: 0.1412\n",
      "Epoch 4277/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.4931 - class_loss: 0.0038 - l1_loss: 0.1489ETA: 0s - loss: 1.2408 - class_loss: 0.0017 - l1_loss: 0.1239\n",
      "Epoch 4278/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4675 - class_loss: 0.0042 - l1_loss: 0.1463\n",
      "Epoch 4279/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7556 - class_loss: 0.0018 - l1_loss: 0.1754\n",
      "Epoch 4280/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7606 - class_loss: 7.0596e-04 - l1_loss: 0.1760\n",
      "Epoch 4281/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0953 - class_loss: 0.0012 - l1_loss: 0.2094\n",
      "Epoch 4282/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0711 - class_loss: 0.0024 - l1_loss: 0.2069\n",
      "Epoch 4283/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.5523 - class_loss: 0.0021 - l1_loss: 0.3550\n",
      "Epoch 4284/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4045 - class_loss: 9.8692e-04 - l1_loss: 0.3403\n",
      "Epoch 4285/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.1738 - class_loss: 7.1043e-04 - l1_loss: 0.3173\n",
      "Epoch 4286/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.3725 - class_loss: 0.0025 - l1_loss: 0.3370\n",
      "Epoch 4287/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.8998 - class_loss: 0.0026 - l1_loss: 0.3897\n",
      "Epoch 4288/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.0695 - class_loss: 0.0026 - l1_loss: 0.4067\n",
      "Epoch 4289/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.8975 - class_loss: 0.0024 - l1_loss: 0.3895\n",
      "Epoch 4290/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.8105 - class_loss: 0.0031 - l1_loss: 0.5807\n",
      "Epoch 4291/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.8790 - class_loss: 4.3804e-04 - l1_loss: 0.4879\n",
      "Epoch 4292/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 33ms/step - loss: 4.8080 - class_loss: 0.0078 - l1_loss: 0.4800\n",
      "Epoch 4293/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.0979 - class_loss: 8.2079e-04 - l1_loss: 0.4097\n",
      "Epoch 4294/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.6362 - class_loss: 0.0011 - l1_loss: 0.5635\n",
      "Epoch 4295/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.9163 - class_loss: 9.8407e-04 - l1_loss: 0.4915\n",
      "Epoch 4296/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3374 - class_loss: 1.2023e-04 - l1_loss: 0.4337\n",
      "Epoch 4297/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.9569 - class_loss: 0.0027 - l1_loss: 0.4954\n",
      "Epoch 4298/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.4929 - class_loss: 0.0147 - l1_loss: 0.4478\n",
      "Epoch 4299/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.2915 - class_loss: 7.3677e-04 - l1_loss: 0.4291\n",
      "Epoch 4300/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.2626 - class_loss: 0.0011 - l1_loss: 0.5262\n",
      "Epoch 4301/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 6.6822 - class_loss: 0.0059 - l1_loss: 0.6676\n",
      "Epoch 4302/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.9928 - class_loss: 0.0111 - l1_loss: 0.6982\n",
      "Epoch 4303/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.4321 - class_loss: 0.0028 - l1_loss: 1.0429\n",
      "Epoch 4304/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.8204 - class_loss: 6.4382e-04 - l1_loss: 0.8820\n",
      "Epoch 4305/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.0095 - class_loss: 8.0917e-04 - l1_loss: 0.9009\n",
      "Epoch 4306/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.3529 - class_loss: 0.0025 - l1_loss: 0.7350\n",
      "Epoch 4307/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.6980 - class_loss: 0.0226 - l1_loss: 0.6675\n",
      "Epoch 4308/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9067 - class_loss: 9.1381e-04 - l1_loss: 0.4906\n",
      "Epoch 4309/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.3579 - class_loss: 0.0015 - l1_loss: 0.6356\n",
      "Epoch 4310/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.2788 - class_loss: 0.0037 - l1_loss: 0.8275\n",
      "Epoch 4311/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.6411 - class_loss: 0.0013 - l1_loss: 0.3640\n",
      "Epoch 4312/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.4087 - class_loss: 0.0026 - l1_loss: 0.4406\n",
      "Epoch 4313/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.5965 - class_loss: 0.0062 - l1_loss: 0.4590\n",
      "Epoch 4314/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.6921 - class_loss: 0.0036 - l1_loss: 0.3688\n",
      "Epoch 4315/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.2631 - class_loss: 0.0047 - l1_loss: 0.3258\n",
      "Epoch 4316/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.1629 - class_loss: 0.0035 - l1_loss: 0.3159 0s - loss: 3.0863 - class_loss: 0.0026 - l1_loss: 0.30\n",
      "Epoch 4317/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2634 - class_loss: 0.0022 - l1_loss: 0.3261\n",
      "Epoch 4318/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.7519 - class_loss: 0.0030 - l1_loss: 0.3749\n",
      "Epoch 4319/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7294 - class_loss: 0.0063 - l1_loss: 0.2723\n",
      "Epoch 4320/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2079 - class_loss: 0.0048 - l1_loss: 0.3203\n",
      "Epoch 4321/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4764 - class_loss: 0.0011 - l1_loss: 0.3475\n",
      "Epoch 4322/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8563 - class_loss: 0.0023 - l1_loss: 0.2854\n",
      "Epoch 4323/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8016 - class_loss: 0.0015 - l1_loss: 0.2800\n",
      "Epoch 4324/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.0244 - class_loss: 0.0032 - l1_loss: 0.4021\n",
      "Epoch 4325/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.9869 - class_loss: 0.0043 - l1_loss: 0.3983\n",
      "Epoch 4326/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.1056 - class_loss: 4.1531e-04 - l1_loss: 0.4105\n",
      "Epoch 4327/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.1827 - class_loss: 1.9868e-04 - l1_loss: 0.4183\n",
      "Epoch 4328/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.6769 - class_loss: 0.0046 - l1_loss: 0.5672\n",
      "Epoch 4329/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.7807 - class_loss: 0.0030 - l1_loss: 0.4778\n",
      "Epoch 4330/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.0033 - class_loss: 0.0019 - l1_loss: 0.7001\n",
      "Epoch 4331/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.5413 - class_loss: 0.0259 - l1_loss: 0.7515\n",
      "Epoch 4332/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.7719 - class_loss: 0.0015 - l1_loss: 0.9770\n",
      "Epoch 4333/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.8935 - class_loss: 0.0019 - l1_loss: 1.5892\n",
      "Epoch 4334/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 22.1944 - class_loss: 1.3542 - l1_loss: 2.0840\n",
      "Epoch 4335/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 36.2912 - class_loss: 1.1151 - l1_loss: 3.5176\n",
      "Epoch 4336/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 54.9618 - class_loss: 1.0183 - l1_loss: 5.3943\n",
      "Epoch 4337/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 38.0679 - class_loss: 0.8557 - l1_loss: 3.7212\n",
      "Epoch 4338/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 30.4119 - class_loss: 0.5294 - l1_loss: 2.9883\n",
      "Epoch 4339/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 28.5251 - class_loss: 0.2592 - l1_loss: 2.8266\n",
      "Epoch 4340/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 22.1812 - class_loss: 0.1116 - l1_loss: 2.2070\n",
      "Epoch 4341/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 23.8034 - class_loss: 0.0334 - l1_loss: 2.3770\n",
      "Epoch 4342/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 22.9419 - class_loss: 0.0808 - l1_loss: 2.2861\n",
      "Epoch 4343/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 21.9967 - class_loss: 0.0139 - l1_loss: 2.1983\n",
      "Epoch 4344/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 18.2169 - class_loss: 0.0092 - l1_loss: 1.8208\n",
      "Epoch 4345/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 22.8129 - class_loss: 0.0092 - l1_loss: 2.2804\n",
      "Epoch 4346/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 19.6445 - class_loss: 0.0155 - l1_loss: 1.9629\n",
      "Epoch 4347/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 19.9257 - class_loss: 0.0032 - l1_loss: 1.9922\n",
      "Epoch 4348/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.7981 - class_loss: 0.0074 - l1_loss: 1.3791\n",
      "Epoch 4349/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.5787 - class_loss: 0.0094 - l1_loss: 1.4569\n",
      "Epoch 4350/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 19.2817 - class_loss: 0.0111 - l1_loss: 1.9271\n",
      "Epoch 4351/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.8227 - class_loss: 5.4089e-04 - l1_loss: 1.8822\n",
      "Epoch 4352/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.3132 - class_loss: 0.0044 - l1_loss: 1.5309\n",
      "Epoch 4353/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 17.7979 - class_loss: 0.0022 - l1_loss: 1.7796\n",
      "Epoch 4354/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 14.8868 - class_loss: 0.0351 - l1_loss: 1.4852\n",
      "Epoch 4355/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 17.1038 - class_loss: 0.0018 - l1_loss: 1.7102\n",
      "Epoch 4356/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 16.1548 - class_loss: 0.0142 - l1_loss: 1.6141\n",
      "Epoch 4357/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 19.6783 - class_loss: 0.0421 - l1_loss: 1.9636\n",
      "Epoch 4358/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 24.6016 - class_loss: 0.0378 - l1_loss: 2.4564\n",
      "Epoch 4359/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 17.9973 - class_loss: 0.0030 - l1_loss: 1.7994\n",
      "Epoch 4360/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 17.1172 - class_loss: 0.0041 - l1_loss: 1.7113\n",
      "Epoch 4361/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 21.0066 - class_loss: 0.0119 - l1_loss: 2.0995\n",
      "Epoch 4362/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.0047 - class_loss: 0.0085 - l1_loss: 1.1996\n",
      "Epoch 4363/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 16.2818 - class_loss: 0.0021 - l1_loss: 1.6280\n",
      "Epoch 4364/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.6700 - class_loss: 0.0304 - l1_loss: 1.2640\n",
      "Epoch 4365/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.1266 - class_loss: 0.0071 - l1_loss: 1.5120\n",
      "Epoch 4366/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.0078 - class_loss: 0.0060 - l1_loss: 0.9002\n",
      "Epoch 4367/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.4293 - class_loss: 0.0010 - l1_loss: 1.1428\n",
      "Epoch 4368/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.1614 - class_loss: 0.1268 - l1_loss: 0.7035\n",
      "Epoch 4369/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.0443 - class_loss: 0.0037 - l1_loss: 1.0041\n",
      "Epoch 4370/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.0964 - class_loss: 0.0066 - l1_loss: 0.9090\n",
      "Epoch 4371/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.1184 - class_loss: 4.3136e-04 - l1_loss: 0.8118\n",
      "Epoch 4372/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.9738 - class_loss: 7.3663e-04 - l1_loss: 0.6973\n",
      "Epoch 4373/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.3758 - class_loss: 2.5210e-04 - l1_loss: 1.2376\n",
      "Epoch 4374/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 12.5866 - class_loss: 0.0166 - l1_loss: 1.2570\n",
      "Epoch 4375/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.6625 - class_loss: 0.0020 - l1_loss: 1.0661\n",
      "Epoch 4376/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.8038 - class_loss: 0.0016 - l1_loss: 0.6802\n",
      "Epoch 4377/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 5.8367 - class_loss: 0.0010 - l1_loss: 0.5836\n",
      "Epoch 4378/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.2028 - class_loss: 0.0032 - l1_loss: 0.7200\n",
      "Epoch 4379/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.4490 - class_loss: 0.0021 - l1_loss: 0.6447\n",
      "Epoch 4380/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.8727 - class_loss: 0.0098 - l1_loss: 0.5863\n",
      "Epoch 4381/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.1681 - class_loss: 0.0154 - l1_loss: 0.6153\n",
      "Epoch 4382/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3176 - class_loss: 0.0011 - l1_loss: 0.4316\n",
      "Epoch 4383/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6636 - class_loss: 0.0015 - l1_loss: 0.2662\n",
      "Epoch 4384/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.5355 - class_loss: 0.0011 - l1_loss: 0.3534\n",
      "Epoch 4385/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.1492 - class_loss: 4.2912e-04 - l1_loss: 0.3149\n",
      "Epoch 4386/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4171 - class_loss: 7.8114e-04 - l1_loss: 0.3416\n",
      "Epoch 4387/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.8876 - class_loss: 0.0017 - l1_loss: 0.2886 0s - loss: 2.1271 - class_loss: 0.0012 - l1_loss: 0.\n",
      "Epoch 4388/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 4.0601 - class_loss: 9.2072e-04 - l1_loss: 0.4059\n",
      "Epoch 4389/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.3467 - class_loss: 0.0026 - l1_loss: 0.2344\n",
      "Epoch 4390/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.2496 - class_loss: 0.0022 - l1_loss: 0.3247 0s - loss: 4.4546 - class_loss: 0.0018 - l1_loss: \n",
      "Epoch 4391/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0903 - class_loss: 0.0027 - l1_loss: 0.4088ETA: 0s - loss: 3.3570 - class_loss: 0.0013 - l1_loss: 0.33\n",
      "Epoch 4392/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9934 - class_loss: 0.0027 - l1_loss: 0.2991\n",
      "Epoch 4393/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0087 - class_loss: 0.0060 - l1_loss: 0.3003\n",
      "Epoch 4394/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9444 - class_loss: 6.8842e-04 - l1_loss: 0.2944\n",
      "Epoch 4395/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9629 - class_loss: 5.2624e-04 - l1_loss: 0.2962\n",
      "Epoch 4396/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6553 - class_loss: 9.8800e-04 - l1_loss: 0.2654\n",
      "Epoch 4397/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2802 - class_loss: 0.0043 - l1_loss: 0.3276\n",
      "Epoch 4398/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.9676 - class_loss: 0.0017 - l1_loss: 0.1966\n",
      "Epoch 4399/5000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.4169 - class_loss: 0.0016 - l1_loss: 0.2415\n",
      "Epoch 4400/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.8814 - class_loss: 0.0033 - l1_loss: 0.1878\n",
      "Epoch 4401/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.3402 - class_loss: 0.0105 - l1_loss: 0.2330\n",
      "Epoch 4402/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7047 - class_loss: 0.0012 - l1_loss: 0.1704\n",
      "Epoch 4403/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.8053 - class_loss: 6.5418e-04 - l1_loss: 0.1805\n",
      "Epoch 4404/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7694 - class_loss: 0.0019 - l1_loss: 0.1768\n",
      "Epoch 4405/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0526 - class_loss: 0.0025 - l1_loss: 0.2050\n",
      "Epoch 4406/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6450 - class_loss: 2.8165e-04 - l1_loss: 0.1645\n",
      "Epoch 4407/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9398 - class_loss: 2.2958e-04 - l1_loss: 0.1940\n",
      "Epoch 4408/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.8881 - class_loss: 0.0012 - l1_loss: 0.1887\n",
      "Epoch 4409/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0192 - class_loss: 0.0014 - l1_loss: 0.2018\n",
      "Epoch 4410/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.1624 - class_loss: 0.0027 - l1_loss: 0.2160\n",
      "Epoch 4411/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5689 - class_loss: 0.0142 - l1_loss: 0.1555\n",
      "Epoch 4412/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1711 - class_loss: 4.5088e-04 - l1_loss: 0.1171\n",
      "Epoch 4413/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.4555 - class_loss: 6.1227e-04 - l1_loss: 0.1455\n",
      "Epoch 4414/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.2135 - class_loss: 8.8234e-04 - l1_loss: 0.1213\n",
      "Epoch 4415/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1280 - class_loss: 0.0010 - l1_loss: 0.1127\n",
      "Epoch 4416/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9833 - class_loss: 5.2643e-04 - l1_loss: 0.0983\n",
      "Epoch 4417/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0203 - class_loss: 0.0011 - l1_loss: 0.1019\n",
      "Epoch 4418/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0646 - class_loss: 0.0030 - l1_loss: 0.1062\n",
      "Epoch 4419/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1955 - class_loss: 8.7677e-04 - l1_loss: 0.1195\n",
      "Epoch 4420/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3271 - class_loss: 0.0013 - l1_loss: 0.1326\n",
      "Epoch 4421/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4004 - class_loss: 8.5377e-04 - l1_loss: 0.1400\n",
      "Epoch 4422/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5298 - class_loss: 7.1861e-04 - l1_loss: 0.1529\n",
      "Epoch 4423/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3449 - class_loss: 5.7128e-04 - l1_loss: 0.2344\n",
      "Epoch 4424/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4585 - class_loss: 0.0013 - l1_loss: 0.1457\n",
      "Epoch 4425/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7140 - class_loss: 0.0014 - l1_loss: 0.2713\n",
      "Epoch 4426/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7845 - class_loss: 0.0027 - l1_loss: 0.1782 0s - loss: 1.9686 - class_loss: 0.0020 - l1_loss: \n",
      "Epoch 4427/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3983 - class_loss: 0.0019 - l1_loss: 0.1396\n",
      "Epoch 4428/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3325 - class_loss: 0.0017 - l1_loss: 0.1331\n",
      "Epoch 4429/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1318 - class_loss: 0.0027 - l1_loss: 0.1129\n",
      "Epoch 4430/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9799 - class_loss: 0.0017 - l1_loss: 0.0978\n",
      "Epoch 4431/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2814 - class_loss: 0.0016 - l1_loss: 0.1280\n",
      "Epoch 4432/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9453 - class_loss: 0.0022 - l1_loss: 0.0943\n",
      "Epoch 4433/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2271 - class_loss: 0.0026 - l1_loss: 0.1225\n",
      "Epoch 4434/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.1138 - class_loss: 0.0020 - l1_loss: 0.1112\n",
      "Epoch 4435/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2252 - class_loss: 0.0012 - l1_loss: 0.1224\n",
      "Epoch 4436/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0010 - class_loss: 0.0035 - l1_loss: 0.0997\n",
      "Epoch 4437/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.1014 - class_loss: 0.0012 - l1_loss: 0.1100\n",
      "Epoch 4438/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9815 - class_loss: 9.0264e-04 - l1_loss: 0.0981\n",
      "Epoch 4439/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.8249 - class_loss: 9.4702e-04 - l1_loss: 0.0824\n",
      "Epoch 4440/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9074 - class_loss: 9.3285e-04 - l1_loss: 0.0906\n",
      "Epoch 4441/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.8543 - class_loss: 8.0708e-04 - l1_loss: 0.0853\n",
      "Epoch 4442/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.6844 - class_loss: 0.0011 - l1_loss: 0.0683\n",
      "Epoch 4443/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9086 - class_loss: 0.0032 - l1_loss: 0.0905\n",
      "Epoch 4444/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3176 - class_loss: 0.0023 - l1_loss: 0.1315\n",
      "Epoch 4445/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9463 - class_loss: 7.8583e-04 - l1_loss: 0.0946\n",
      "Epoch 4446/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0712 - class_loss: 0.0015 - l1_loss: 0.1070\n",
      "Epoch 4447/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.9321 - class_loss: 0.0034 - l1_loss: 0.0929\n",
      "Epoch 4448/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.3701 - class_loss: 0.0025 - l1_loss: 0.1368\n",
      "Epoch 4449/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5904 - class_loss: 0.0049 - l1_loss: 0.1586\n",
      "Epoch 4450/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2904 - class_loss: 0.0018 - l1_loss: 0.1289\n",
      "Epoch 4451/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0096 - class_loss: 0.0011 - l1_loss: 0.2008\n",
      "Epoch 4452/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2706 - class_loss: 0.0019 - l1_loss: 0.1269\n",
      "Epoch 4453/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9886 - class_loss: 0.0011 - l1_loss: 0.0988\n",
      "Epoch 4454/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.8990 - class_loss: 0.0011 - l1_loss: 0.0898\n",
      "Epoch 4455/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.2652 - class_loss: 0.0032 - l1_loss: 0.1262\n",
      "Epoch 4456/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6149 - class_loss: 0.0020 - l1_loss: 0.1613\n",
      "Epoch 4457/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3762 - class_loss: 0.0015 - l1_loss: 0.1375\n",
      "Epoch 4458/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9958 - class_loss: 0.0079 - l1_loss: 0.1988\n",
      "Epoch 4459/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6556 - class_loss: 0.0018 - l1_loss: 0.1654\n",
      "Epoch 4460/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7674 - class_loss: 0.0024 - l1_loss: 0.1765\n",
      "Epoch 4461/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8435 - class_loss: 0.0020 - l1_loss: 0.1841\n",
      "Epoch 4462/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1695 - class_loss: 0.0070 - l1_loss: 0.2163\n",
      "Epoch 4463/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2858 - class_loss: 9.5951e-04 - l1_loss: 0.2285\n",
      "Epoch 4464/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7970 - class_loss: 0.0068 - l1_loss: 0.2790\n",
      "Epoch 4465/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.7608 - class_loss: 9.7852e-04 - l1_loss: 0.4760\n",
      "Epoch 4466/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.6565 - class_loss: 0.0353 - l1_loss: 0.7621\n",
      "Epoch 4467/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.0407 - class_loss: 7.4181e-04 - l1_loss: 0.8040\n",
      "Epoch 4468/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 16.0690 - class_loss: 4.4034e-04 - l1_loss: 1.6069\n",
      "Epoch 4469/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.9862 - class_loss: 5.8631e-04 - l1_loss: 1.3986\n",
      "Epoch 4470/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 17.6997 - class_loss: 3.5777e-04 - l1_loss: 1.7699\n",
      "Epoch 4471/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 11.4321 - class_loss: 6.9613e-04 - l1_loss: 1.1431\n",
      "Epoch 4472/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 15.3244 - class_loss: 2.1073e-04 - l1_loss: 1.5324\n",
      "Epoch 4473/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.4229 - class_loss: 0.3004 - l1_loss: 0.9122\n",
      "Epoch 4474/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.8066 - class_loss: 0.1042 - l1_loss: 1.6702\n",
      "Epoch 4475/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 16.3822 - class_loss: 0.0969 - l1_loss: 1.6285\n",
      "Epoch 4476/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 22.0343 - class_loss: 0.0062 - l1_loss: 2.2028\n",
      "Epoch 4477/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 14.1079 - class_loss: 0.0159 - l1_loss: 1.4092\n",
      "Epoch 4478/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.3949 - class_loss: 0.0029 - l1_loss: 1.7392\n",
      "Epoch 4479/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.3183 - class_loss: 0.0053 - l1_loss: 1.6313\n",
      "Epoch 4480/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 12.4445 - class_loss: 4.3472e-04 - l1_loss: 1.2444\n",
      "Epoch 4481/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 15.9364 - class_loss: 0.0070 - l1_loss: 1.5929\n",
      "Epoch 4482/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 22.2073 - class_loss: 0.0090 - l1_loss: 2.2198\n",
      "Epoch 4483/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 21.1243 - class_loss: 0.5463 - l1_loss: 2.0578\n",
      "Epoch 4484/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 19.7970 - class_loss: 0.0038 - l1_loss: 1.9793\n",
      "Epoch 4485/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 25.6656 - class_loss: 0.0559 - l1_loss: 2.5610\n",
      "Epoch 4486/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 19.0519 - class_loss: 0.0027 - l1_loss: 1.9049\n",
      "Epoch 4487/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 27.1315 - class_loss: 0.0035 - l1_loss: 2.7128\n",
      "Epoch 4488/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 26.3963 - class_loss: 0.1934 - l1_loss: 2.6203\n",
      "Epoch 4489/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 33.7789 - class_loss: 0.0025 - l1_loss: 3.3776\n",
      "Epoch 4490/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 39.0979 - class_loss: 0.0021 - l1_loss: 3.9096\n",
      "Epoch 4491/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 31.5462 - class_loss: 0.0011 - l1_loss: 3.1545\n",
      "Epoch 4492/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 27.4864 - class_loss: 9.0559e-04 - l1_loss: 2.7486\n",
      "Epoch 4493/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 27.4813 - class_loss: 0.0011 - l1_loss: 2.7480\n",
      "Epoch 4494/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 25.7863 - class_loss: 1.4630e-05 - l1_loss: 2.5786\n",
      "Epoch 4495/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 22.1847 - class_loss: 2.2603e-04 - l1_loss: 2.2184\n",
      "Epoch 4496/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 25.7730 - class_loss: 0.0090 - l1_loss: 2.5764\n",
      "Epoch 4497/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 33.0890 - class_loss: 0.0055 - l1_loss: 3.3083\n",
      "Epoch 4498/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 40.8776 - class_loss: 0.0807 - l1_loss: 4.0797\n",
      "Epoch 4499/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 33.1964 - class_loss: 9.7256e-05 - l1_loss: 3.3196\n",
      "Epoch 4500/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 20.1230 - class_loss: 2.5753e-05 - l1_loss: 2.0123\n",
      "Epoch 4501/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 18.0770 - class_loss: 1.8821e-04 - l1_loss: 1.8077\n",
      "Epoch 4502/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 17.2906 - class_loss: 4.8903e-05 - l1_loss: 1.7291\n",
      "Epoch 4503/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.4473 - class_loss: 5.0315e-05 - l1_loss: 1.4447\n",
      "Epoch 4504/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 12.8072 - class_loss: 0.0042 - l1_loss: 1.2803\n",
      "Epoch 4505/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.9582 - class_loss: 2.9300e-04 - l1_loss: 1.0958\n",
      "Epoch 4506/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.7605 - class_loss: 6.6235e-04 - l1_loss: 0.8760\n",
      "Epoch 4507/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.3489 - class_loss: 2.1241e-04 - l1_loss: 0.9349\n",
      "Epoch 4508/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.2955 - class_loss: 4.6249e-04 - l1_loss: 0.7295\n",
      "Epoch 4509/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.1693 - class_loss: 0.0345 - l1_loss: 0.8135\n",
      "Epoch 4510/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.4702 - class_loss: 0.0011 - l1_loss: 0.6469\n",
      "Epoch 4511/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.1677 - class_loss: 1.7148e-04 - l1_loss: 0.4168\n",
      "Epoch 4512/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.9267 - class_loss: 9.1133e-04 - l1_loss: 0.4926\n",
      "Epoch 4513/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.2269 - class_loss: 0.0028 - l1_loss: 0.5224\n",
      "Epoch 4514/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.7587 - class_loss: 0.0034 - l1_loss: 0.4755\n",
      "Epoch 4515/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.8264 - class_loss: 0.0396 - l1_loss: 0.4787\n",
      "Epoch 4516/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.5435 - class_loss: 0.0010 - l1_loss: 0.3542\n",
      "Epoch 4517/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.9523 - class_loss: 4.2234e-04 - l1_loss: 0.3952\n",
      "Epoch 4518/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.2863 - class_loss: 3.6422e-04 - l1_loss: 0.3286\n",
      "Epoch 4519/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8089 - class_loss: 0.0011 - l1_loss: 0.2808\n",
      "Epoch 4520/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.5059 - class_loss: 0.0020 - l1_loss: 0.2504\n",
      "Epoch 4521/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.3508 - class_loss: 4.5034e-04 - l1_loss: 0.3350\n",
      "Epoch 4522/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.5261 - class_loss: 0.0011 - l1_loss: 0.2525\n",
      "Epoch 4523/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.2005 - class_loss: 0.0032 - l1_loss: 0.2197\n",
      "Epoch 4524/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.7642 - class_loss: 0.0051 - l1_loss: 0.1759\n",
      "Epoch 4525/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6749 - class_loss: 0.0033 - l1_loss: 0.1672\n",
      "Epoch 4526/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5677 - class_loss: 0.0027 - l1_loss: 0.1565\n",
      "Epoch 4527/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.6755 - class_loss: 0.0017 - l1_loss: 0.1674\n",
      "Epoch 4528/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7563 - class_loss: 0.0079 - l1_loss: 0.1748\n",
      "Epoch 4529/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5956 - class_loss: 0.0110 - l1_loss: 0.1585\n",
      "Epoch 4530/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7074 - class_loss: 5.3545e-04 - l1_loss: 0.1707\n",
      "Epoch 4531/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.1597 - class_loss: 6.6786e-04 - l1_loss: 0.1159\n",
      "Epoch 4532/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9231 - class_loss: 0.0025 - l1_loss: 0.1921\n",
      "Epoch 4533/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2859 - class_loss: 0.0011 - l1_loss: 0.1285\n",
      "Epoch 4534/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6860 - class_loss: 0.0015 - l1_loss: 0.1684\n",
      "Epoch 4535/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1744 - class_loss: 0.0028 - l1_loss: 0.1172\n",
      "Epoch 4536/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2781 - class_loss: 0.0010 - l1_loss: 0.1277\n",
      "Epoch 4537/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.9590 - class_loss: 7.4690e-04 - l1_loss: 0.0958\n",
      "Epoch 4538/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0544 - class_loss: 6.8405e-04 - l1_loss: 0.1054\n",
      "Epoch 4539/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.1275 - class_loss: 0.0016 - l1_loss: 0.1126\n",
      "Epoch 4540/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5003 - class_loss: 0.0030 - l1_loss: 0.1497\n",
      "Epoch 4541/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8125 - class_loss: 0.0017 - l1_loss: 0.1811\n",
      "Epoch 4542/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6848 - class_loss: 0.0031 - l1_loss: 0.1682\n",
      "Epoch 4543/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.3766 - class_loss: 8.6377e-04 - l1_loss: 0.1376\n",
      "Epoch 4544/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4815 - class_loss: 0.0019 - l1_loss: 0.1480\n",
      "Epoch 4545/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4375 - class_loss: 0.0021 - l1_loss: 0.1435\n",
      "Epoch 4546/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4127 - class_loss: 0.0011 - l1_loss: 0.1412\n",
      "Epoch 4547/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5227 - class_loss: 6.9601e-04 - l1_loss: 0.1522\n",
      "Epoch 4548/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3319 - class_loss: 0.0019 - l1_loss: 0.1330\n",
      "Epoch 4549/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.4586 - class_loss: 0.0036 - l1_loss: 0.1455\n",
      "Epoch 4550/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4335 - class_loss: 0.0010 - l1_loss: 0.1432\n",
      "Epoch 4551/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0435 - class_loss: 0.0018 - l1_loss: 0.1042\n",
      "Epoch 4552/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3787 - class_loss: 0.0014 - l1_loss: 0.1377\n",
      "Epoch 4553/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3175 - class_loss: 0.0023 - l1_loss: 0.1315\n",
      "Epoch 4554/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0853 - class_loss: 5.6441e-04 - l1_loss: 0.1085\n",
      "Epoch 4555/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9271 - class_loss: 6.9885e-04 - l1_loss: 0.0926\n",
      "Epoch 4556/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.9096 - class_loss: 0.0010 - l1_loss: 0.0909\n",
      "Epoch 4557/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2888 - class_loss: 0.0011 - l1_loss: 0.1288\n",
      "Epoch 4558/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.9430 - class_loss: 0.0013 - l1_loss: 0.0942 0s - loss: 1.0015 - class_loss: 9.3044e-04 - l1_loss: 0.\n",
      "Epoch 4559/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.9270 - class_loss: 0.0027 - l1_loss: 0.0924\n",
      "Epoch 4560/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.8231 - class_loss: 0.0031 - l1_loss: 0.0820\n",
      "Epoch 4561/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.8085 - class_loss: 0.0030 - l1_loss: 0.0805\n",
      "Epoch 4562/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.8259 - class_loss: 0.0022 - l1_loss: 0.0824\n",
      "Epoch 4563/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0025 - class_loss: 0.0014 - l1_loss: 0.1001\n",
      "Epoch 4564/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.1247 - class_loss: 0.0013 - l1_loss: 0.1123\n",
      "Epoch 4565/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9641 - class_loss: 0.0010 - l1_loss: 0.0963\n",
      "Epoch 4566/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0915 - class_loss: 0.0019 - l1_loss: 0.1090\n",
      "Epoch 4567/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.1929 - class_loss: 0.0020 - l1_loss: 0.1191\n",
      "Epoch 4568/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9567 - class_loss: 0.0012 - l1_loss: 0.0956\n",
      "Epoch 4569/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.4427 - class_loss: 0.0027 - l1_loss: 0.1440\n",
      "Epoch 4570/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.2676 - class_loss: 0.0023 - l1_loss: 0.1265\n",
      "Epoch 4571/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4922 - class_loss: 0.0025 - l1_loss: 0.1490\n",
      "Epoch 4572/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3916 - class_loss: 0.0033 - l1_loss: 0.1388\n",
      "Epoch 4573/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5740 - class_loss: 0.0020 - l1_loss: 0.1572\n",
      "Epoch 4574/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.1477 - class_loss: 0.0026 - l1_loss: 0.2145\n",
      "Epoch 4575/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5799 - class_loss: 0.0037 - l1_loss: 0.1576\n",
      "Epoch 4576/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9786 - class_loss: 0.0022 - l1_loss: 0.1976\n",
      "Epoch 4577/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.1578 - class_loss: 6.1566e-04 - l1_loss: 0.2157\n",
      "Epoch 4578/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.5646 - class_loss: 3.0379e-04 - l1_loss: 0.2564\n",
      "Epoch 4579/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.7184 - class_loss: 0.0017 - l1_loss: 0.2717\n",
      "Epoch 4580/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.0913 - class_loss: 0.0035 - l1_loss: 0.4088\n",
      "Epoch 4581/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.5497 - class_loss: 0.0013 - l1_loss: 0.2548\n",
      "Epoch 4582/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.6940 - class_loss: 0.0018 - l1_loss: 0.4692\n",
      "Epoch 4583/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 6.7445 - class_loss: 0.0093 - l1_loss: 0.6735\n",
      "Epoch 4584/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.0697 - class_loss: 0.0035 - l1_loss: 0.4066\n",
      "Epoch 4585/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.7828 - class_loss: 8.2116e-04 - l1_loss: 0.2782\n",
      "Epoch 4586/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.7957 - class_loss: 0.0012 - l1_loss: 0.2794\n",
      "Epoch 4587/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.9778 - class_loss: 0.0012 - l1_loss: 0.4977\n",
      "Epoch 4588/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.8444 - class_loss: 0.0013 - l1_loss: 0.2843\n",
      "Epoch 4589/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.1408 - class_loss: 0.0015 - l1_loss: 0.3139\n",
      "Epoch 4590/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.8124 - class_loss: 0.0015 - l1_loss: 0.4811\n",
      "Epoch 4591/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 6.0797 - class_loss: 0.0138 - l1_loss: 0.6066\n",
      "Epoch 4592/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 6.7036 - class_loss: 2.9326e-04 - l1_loss: 0.6703\n",
      "Epoch 4593/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 10.3153 - class_loss: 3.7183e-04 - l1_loss: 1.0315\n",
      "Epoch 4594/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 5.6474 - class_loss: 2.0147e-04 - l1_loss: 0.5647\n",
      "Epoch 4595/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 5.3927 - class_loss: 4.1483e-04 - l1_loss: 0.5392\n",
      "Epoch 4596/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 5.4323 - class_loss: 0.0036 - l1_loss: 0.5429\n",
      "Epoch 4597/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 6.7616 - class_loss: 0.0019 - l1_loss: 0.6760\n",
      "Epoch 4598/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 5.5047 - class_loss: 0.0020 - l1_loss: 0.5503\n",
      "Epoch 4599/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.4348 - class_loss: 6.8873e-04 - l1_loss: 0.4434\n",
      "Epoch 4600/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.5166 - class_loss: 0.0016 - l1_loss: 0.4515\n",
      "Epoch 4601/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.6152 - class_loss: 0.0069 - l1_loss: 0.5608\n",
      "Epoch 4602/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.7710 - class_loss: 0.0036 - l1_loss: 0.4767\n",
      "Epoch 4603/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 7.1546 - class_loss: 0.0145 - l1_loss: 0.7140\n",
      "Epoch 4604/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 7.3024 - class_loss: 5.3427e-04 - l1_loss: 0.7302\n",
      "Epoch 4605/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 5.8730 - class_loss: 1.0551e-04 - l1_loss: 0.5873\n",
      "Epoch 4606/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.0807 - class_loss: 0.0023 - l1_loss: 0.4078\n",
      "Epoch 4607/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.8957 - class_loss: 0.0046 - l1_loss: 0.4891\n",
      "Epoch 4608/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.8446 - class_loss: 2.4139e-04 - l1_loss: 0.2844\n",
      "Epoch 4609/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 4.6942 - class_loss: 1.1445e-04 - l1_loss: 0.4694\n",
      "Epoch 4610/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.7573 - class_loss: 0.0013 - l1_loss: 0.3756\n",
      "Epoch 4611/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.6499 - class_loss: 0.0033 - l1_loss: 0.3647\n",
      "Epoch 4612/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.2332 - class_loss: 0.0013 - l1_loss: 0.6232\n",
      "Epoch 4613/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.3385 - class_loss: 2.3324e-04 - l1_loss: 0.5338\n",
      "Epoch 4614/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.4150 - class_loss: 0.0026 - l1_loss: 0.7412\n",
      "Epoch 4615/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 7.0381 - class_loss: 0.0016 - l1_loss: 0.7037\n",
      "Epoch 4616/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 7.1725 - class_loss: 0.0018 - l1_loss: 0.7171\n",
      "Epoch 4617/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 8.4589 - class_loss: 0.0026 - l1_loss: 0.8456\n",
      "Epoch 4618/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.4613 - class_loss: 0.0026 - l1_loss: 0.6459\n",
      "Epoch 4619/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 11.5691 - class_loss: 0.0029 - l1_loss: 1.1566\n",
      "Epoch 4620/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.2369 - class_loss: 8.7737e-04 - l1_loss: 0.7236\n",
      "Epoch 4621/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.7558 - class_loss: 6.2014e-04 - l1_loss: 0.7755\n",
      "Epoch 4622/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 10.6610 - class_loss: 0.0047 - l1_loss: 1.0656\n",
      "Epoch 4623/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 9.2355 - class_loss: 0.0014 - l1_loss: 0.9234\n",
      "Epoch 4624/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.9722 - class_loss: 0.0015 - l1_loss: 1.0971\n",
      "Epoch 4625/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 8.1482 - class_loss: 0.0087 - l1_loss: 0.8139\n",
      "Epoch 4626/5000\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 7.7925 - class_loss: 2.8227e-04 - l1_loss: 0.7792\n",
      "Epoch 4627/5000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 6.9722 - class_loss: 2.6388e-04 - l1_loss: 0.6972\n",
      "Epoch 4628/5000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 8.5309 - class_loss: 2.6101e-04 - l1_loss: 0.8531\n",
      "Epoch 4629/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 6.3220 - class_loss: 5.5437e-04 - l1_loss: 0.6321\n",
      "Epoch 4630/5000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 8.4824 - class_loss: 5.4916e-04 - l1_loss: 0.8482\n",
      "Epoch 4631/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.4962 - class_loss: 0.0041 - l1_loss: 0.6492\n",
      "Epoch 4632/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 9.2174 - class_loss: 0.0014 - l1_loss: 0.9216\n",
      "Epoch 4633/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 12.9548 - class_loss: 0.0077 - l1_loss: 1.2947\n",
      "Epoch 4634/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 21.1200 - class_loss: 0.0060 - l1_loss: 2.1114\n",
      "Epoch 4635/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 11.2376 - class_loss: 4.5348e-04 - l1_loss: 1.1237\n",
      "Epoch 4636/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 21.1015 - class_loss: 2.6400e-04 - l1_loss: 2.1101\n",
      "Epoch 4637/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.2224 - class_loss: 0.0019 - l1_loss: 0.7220\n",
      "Epoch 4638/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.8309 - class_loss: 0.0028 - l1_loss: 1.1828\n",
      "Epoch 4639/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 13.6727 - class_loss: 4.3143e-04 - l1_loss: 1.3672\n",
      "Epoch 4640/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 15.3770 - class_loss: 0.0022 - l1_loss: 1.5375\n",
      "Epoch 4641/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 19.9483 - class_loss: 0.0071 - l1_loss: 1.9941\n",
      "Epoch 4642/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 18.7960 - class_loss: 0.0026 - l1_loss: 1.8793\n",
      "Epoch 4643/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 16.4034 - class_loss: 0.0084 - l1_loss: 1.6395\n",
      "Epoch 4644/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.2240 - class_loss: 1.0474e-04 - l1_loss: 1.0224\n",
      "Epoch 4645/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 14.9308 - class_loss: 1.2012e-05 - l1_loss: 1.4931\n",
      "Epoch 4646/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 17.2645 - class_loss: 1.0426e-04 - l1_loss: 1.7264\n",
      "Epoch 4647/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 13.9901 - class_loss: 7.1921e-04 - l1_loss: 1.3989\n",
      "Epoch 4648/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 19.1390 - class_loss: 1.0560e-04 - l1_loss: 1.9139\n",
      "Epoch 4649/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 19.8522 - class_loss: 4.4694e-04 - l1_loss: 1.9852\n",
      "Epoch 4650/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 19.6344 - class_loss: 0.0061 - l1_loss: 1.9628\n",
      "Epoch 4651/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 32.3638 - class_loss: 0.0152 - l1_loss: 3.2349\n",
      "Epoch 4652/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 39.0461 - class_loss: 0.0269 - l1_loss: 3.9019\n",
      "Epoch 4653/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 44.0821 - class_loss: 1.1967e-04 - l1_loss: 4.4082\n",
      "Epoch 4654/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 33.0505 - class_loss: 5.3413e-05 - l1_loss: 3.3050\n",
      "Epoch 4655/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 23.1465 - class_loss: 3.1247e-04 - l1_loss: 2.3146\n",
      "Epoch 4656/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 24.8207 - class_loss: 0.0012 - l1_loss: 2.4820\n",
      "Epoch 4657/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 23.4755 - class_loss: 0.0133 - l1_loss: 2.3462\n",
      "Epoch 4658/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 26.5787 - class_loss: 3.7276e-05 - l1_loss: 2.6579\n",
      "Epoch 4659/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 24.0143 - class_loss: 8.8170e-05 - l1_loss: 2.4014\n",
      "Epoch 4660/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 26.8242 - class_loss: 8.7891e-04 - l1_loss: 2.6823\n",
      "Epoch 4661/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.7616 - class_loss: 0.0033 - l1_loss: 1.8758\n",
      "Epoch 4662/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 18.0274 - class_loss: 5.0045e-04 - l1_loss: 1.8027\n",
      "Epoch 4663/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 15.9642 - class_loss: 0.0015 - l1_loss: 1.5963\n",
      "Epoch 4664/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.4657 - class_loss: 4.0007e-04 - l1_loss: 1.3465\n",
      "Epoch 4665/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.4370 - class_loss: 0.0027 - l1_loss: 1.4434\n",
      "Epoch 4666/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.5774 - class_loss: 0.0109 - l1_loss: 1.3567 ETA: 0s - loss: 11.6258 - class_loss: 2.2500e-04 - l1_loss: 1.162 - ETA: 0s - loss: 14.6686 - class_loss: 0.0075 - l1_loss: 1.4661  \n",
      "Epoch 4667/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.9703 - class_loss: 0.0278 - l1_loss: 0.7943\n",
      "Epoch 4668/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.8967 - class_loss: 0.0010 - l1_loss: 0.7896\n",
      "Epoch 4669/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.2770 - class_loss: 5.3481e-05 - l1_loss: 0.7277\n",
      "Epoch 4670/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.9585 - class_loss: 4.1863e-04 - l1_loss: 0.6958\n",
      "Epoch 4671/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.0051 - class_loss: 0.0018 - l1_loss: 0.7003\n",
      "Epoch 4672/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.7292 - class_loss: 0.0011 - l1_loss: 0.6728\n",
      "Epoch 4673/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.9405 - class_loss: 8.1072e-04 - l1_loss: 0.8940\n",
      "Epoch 4674/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.4570 - class_loss: 4.2992e-04 - l1_loss: 0.8457\n",
      "Epoch 4675/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.9473 - class_loss: 4.3089e-04 - l1_loss: 0.5947\n",
      "Epoch 4676/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.2547 - class_loss: 0.0029 - l1_loss: 0.7252\n",
      "Epoch 4677/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.8883 - class_loss: 0.0190 - l1_loss: 0.6869\n",
      "Epoch 4678/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.2451 - class_loss: 0.0020 - l1_loss: 0.7243\n",
      "Epoch 4679/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.7734 - class_loss: 3.3822e-04 - l1_loss: 0.4773\n",
      "Epoch 4680/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5793 - class_loss: 0.0228 - l1_loss: 0.4557\n",
      "Epoch 4681/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.0252 - class_loss: 0.0026 - l1_loss: 0.4023\n",
      "Epoch 4682/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.3336 - class_loss: 2.9768e-04 - l1_loss: 0.3333\n",
      "Epoch 4683/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.4039 - class_loss: 0.0013 - l1_loss: 0.3403\n",
      "Epoch 4684/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.2729 - class_loss: 0.0033 - l1_loss: 0.3270\n",
      "Epoch 4685/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1542 - class_loss: 0.0049 - l1_loss: 0.3149\n",
      "Epoch 4686/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.7126 - class_loss: 0.0021 - l1_loss: 0.3710\n",
      "Epoch 4687/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.3615 - class_loss: 0.0032 - l1_loss: 0.3358\n",
      "Epoch 4688/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.4602 - class_loss: 0.0018 - l1_loss: 0.2458\n",
      "Epoch 4689/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7420 - class_loss: 0.0024 - l1_loss: 0.2740\n",
      "Epoch 4690/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.9465 - class_loss: 0.0027 - l1_loss: 0.2944\n",
      "Epoch 4691/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.7995 - class_loss: 0.0025 - l1_loss: 0.2797\n",
      "Epoch 4692/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0640 - class_loss: 0.0013 - l1_loss: 0.2063\n",
      "Epoch 4693/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.0846 - class_loss: 9.0195e-04 - l1_loss: 0.2084\n",
      "Epoch 4694/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0405 - class_loss: 0.0093 - l1_loss: 0.2031\n",
      "Epoch 4695/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7105 - class_loss: 0.0017 - l1_loss: 0.1709\n",
      "Epoch 4696/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9206 - class_loss: 8.7006e-04 - l1_loss: 0.1920\n",
      "Epoch 4697/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.4660 - class_loss: 0.0010 - l1_loss: 0.1465\n",
      "Epoch 4698/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.4217 - class_loss: 0.0025 - l1_loss: 0.1419\n",
      "Epoch 4699/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7582 - class_loss: 0.0012 - l1_loss: 0.1757\n",
      "Epoch 4700/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6620 - class_loss: 9.6463e-04 - l1_loss: 0.1661\n",
      "Epoch 4701/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.5806 - class_loss: 0.0049 - l1_loss: 0.1576\n",
      "Epoch 4702/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8830 - class_loss: 0.0258 - l1_loss: 0.1857\n",
      "Epoch 4703/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2895 - class_loss: 4.6067e-05 - l1_loss: 0.2289\n",
      "Epoch 4704/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7220 - class_loss: 2.0334e-04 - l1_loss: 0.1722\n",
      "Epoch 4705/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.8220 - class_loss: 1.3095e-04 - l1_loss: 0.1822\n",
      "Epoch 4706/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.1370 - class_loss: 5.8277e-04 - l1_loss: 0.1136\n",
      "Epoch 4707/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7763 - class_loss: 3.1297e-04 - l1_loss: 0.1776\n",
      "Epoch 4708/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4570 - class_loss: 0.0019 - l1_loss: 0.1455\n",
      "Epoch 4709/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5675 - class_loss: 0.0021 - l1_loss: 0.1565\n",
      "Epoch 4710/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8118 - class_loss: 0.0035 - l1_loss: 0.1808\n",
      "Epoch 4711/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8796 - class_loss: 0.0059 - l1_loss: 0.1874\n",
      "Epoch 4712/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.2157 - class_loss: 2.3896e-04 - l1_loss: 0.2215\n",
      "Epoch 4713/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8807 - class_loss: 9.4702e-04 - l1_loss: 0.1880\n",
      "Epoch 4714/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.0697 - class_loss: 0.0013 - l1_loss: 0.2068\n",
      "Epoch 4715/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1017 - class_loss: 6.5948e-04 - l1_loss: 0.2101\n",
      "Epoch 4716/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.3773 - class_loss: 3.0252e-04 - l1_loss: 0.4377\n",
      "Epoch 4717/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.3117 - class_loss: 9.5764e-04 - l1_loss: 0.4311\n",
      "Epoch 4718/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.4088 - class_loss: 0.0017 - l1_loss: 0.4407\n",
      "Epoch 4719/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4386 - class_loss: 0.0012 - l1_loss: 0.2437ETA: 0s - loss: 2.8146 - class_loss: 9.5667e-04 - l1_loss: 0.\n",
      "Epoch 4720/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.1390 - class_loss: 0.0094 - l1_loss: 0.2130\n",
      "Epoch 4721/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.2509 - class_loss: 0.0016 - l1_loss: 0.3249\n",
      "Epoch 4722/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.3559 - class_loss: 5.4225e-04 - l1_loss: 0.2355\n",
      "Epoch 4723/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.9779 - class_loss: 0.0019 - l1_loss: 0.3976\n",
      "Epoch 4724/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.2688 - class_loss: 0.0143 - l1_loss: 0.5254ETA: 0s - loss: 2.6134 - class_loss: 4.8204e-04 - l1_loss: \n",
      "Epoch 4725/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.5564 - class_loss: 3.6144e-04 - l1_loss: 0.8556\n",
      "Epoch 4726/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 16.0085 - class_loss: 9.9519e-05 - l1_loss: 1.6008\n",
      "Epoch 4727/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.9820 - class_loss: 5.8406e-04 - l1_loss: 0.7981 0s - loss: 9.0843 - class_loss: 4.7953e-04 - l1_loss: 0.\n",
      "Epoch 4728/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.6558 - class_loss: 0.0028 - l1_loss: 0.8653\n",
      "Epoch 4729/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.6346 - class_loss: 1.4350e-04 - l1_loss: 0.8634\n",
      "Epoch 4730/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.0246 - class_loss: 0.0014 - l1_loss: 1.2023\n",
      "Epoch 4731/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 17.2829 - class_loss: 0.0135 - l1_loss: 1.7269\n",
      "Epoch 4732/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 21.5279 - class_loss: 1.9288e-04 - l1_loss: 2.1528\n",
      "Epoch 4733/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 20.4126 - class_loss: 2.4496e-05 - l1_loss: 2.0413\n",
      "Epoch 4734/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 14.5910 - class_loss: 4.2151e-04 - l1_loss: 1.4591\n",
      "Epoch 4735/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.5870 - class_loss: 0.0234 - l1_loss: 1.2564\n",
      "Epoch 4736/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 10.9853 - class_loss: 0.0017 - l1_loss: 1.0984\n",
      "Epoch 4737/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 18.4850 - class_loss: 0.0051 - l1_loss: 1.8480\n",
      "Epoch 4738/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.6655 - class_loss: 0.0017 - l1_loss: 1.3664\n",
      "Epoch 4739/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.6760 - class_loss: 0.0015 - l1_loss: 1.2674\n",
      "Epoch 4740/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.7262 - class_loss: 0.0234 - l1_loss: 0.9703\n",
      "Epoch 4741/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.7199 - class_loss: 6.2999e-04 - l1_loss: 0.6719\n",
      "Epoch 4742/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.4421 - class_loss: 7.8596e-05 - l1_loss: 0.9442\n",
      "Epoch 4743/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.4117 - class_loss: 0.0229 - l1_loss: 1.2389\n",
      "Epoch 4744/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.5327 - class_loss: 0.0120 - l1_loss: 0.7521\n",
      "Epoch 4745/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.0457 - class_loss: 9.7643e-04 - l1_loss: 1.0045\n",
      "Epoch 4746/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.0098 - class_loss: 4.9222e-04 - l1_loss: 0.7009\n",
      "Epoch 4747/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.6411 - class_loss: 0.0084 - l1_loss: 0.7633\n",
      "Epoch 4748/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.2453 - class_loss: 0.0012 - l1_loss: 0.6244\n",
      "Epoch 4749/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.0258 - class_loss: 0.0034 - l1_loss: 0.7022\n",
      "Epoch 4750/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.8604 - class_loss: 0.0110 - l1_loss: 0.5849\n",
      "Epoch 4751/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.0428 - class_loss: 5.3436e-04 - l1_loss: 0.6042\n",
      "Epoch 4752/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.7385 - class_loss: 0.0095 - l1_loss: 0.6729\n",
      "Epoch 4753/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.1296 - class_loss: 1.4628e-04 - l1_loss: 0.6129\n",
      "Epoch 4754/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.1563 - class_loss: 0.0020 - l1_loss: 0.4154\n",
      "Epoch 4755/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.6528 - class_loss: 0.0036 - l1_loss: 0.4649\n",
      "Epoch 4756/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.6742 - class_loss: 0.0012 - l1_loss: 0.4673\n",
      "Epoch 4757/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.7712 - class_loss: 0.0010 - l1_loss: 0.4770\n",
      "Epoch 4758/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.5248 - class_loss: 0.0081 - l1_loss: 0.5517\n",
      "Epoch 4759/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.9052 - class_loss: 2.8243e-04 - l1_loss: 0.4905\n",
      "Epoch 4760/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3419 - class_loss: 4.7462e-05 - l1_loss: 0.3342\n",
      "Epoch 4761/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.4496 - class_loss: 7.0479e-05 - l1_loss: 0.5450\n",
      "Epoch 4762/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.3323 - class_loss: 3.1130e-04 - l1_loss: 0.5332\n",
      "Epoch 4763/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.9060 - class_loss: 0.0193 - l1_loss: 0.3887\n",
      "Epoch 4764/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2801 - class_loss: 0.0066 - l1_loss: 0.3273\n",
      "Epoch 4765/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.9151 - class_loss: 3.3606e-04 - l1_loss: 0.5915\n",
      "Epoch 4766/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.1748 - class_loss: 1.3264e-04 - l1_loss: 0.9175 0s - loss: 7.0510 - class_loss: 1.5316e-04 - l1_loss: 0.\n",
      "Epoch 4767/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 12.2190 - class_loss: 6.1221e-04 - l1_loss: 1.2218\n",
      "Epoch 4768/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.2044 - class_loss: 0.0011 - l1_loss: 0.8203\n",
      "Epoch 4769/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.2080 - class_loss: 0.0217 - l1_loss: 0.5186\n",
      "Epoch 4770/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 8.7775 - class_loss: 0.0028 - l1_loss: 0.8775\n",
      "Epoch 4771/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.6696 - class_loss: 0.0026 - l1_loss: 0.8667\n",
      "Epoch 4772/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.7434 - class_loss: 0.0025 - l1_loss: 0.6741\n",
      "Epoch 4773/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.2709 - class_loss: 0.0090 - l1_loss: 0.6262\n",
      "Epoch 4774/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.4696 - class_loss: 4.5966e-04 - l1_loss: 0.6469\n",
      "Epoch 4775/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.0248 - class_loss: 0.0022 - l1_loss: 0.6023\n",
      "Epoch 4776/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.6939 - class_loss: 5.4606e-04 - l1_loss: 0.7693\n",
      "Epoch 4777/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 9.9390 - class_loss: 8.3241e-04 - l1_loss: 0.9938\n",
      "Epoch 4778/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.2055 - class_loss: 7.3104e-04 - l1_loss: 1.0205\n",
      "Epoch 4779/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.6146 - class_loss: 1.9482e-04 - l1_loss: 0.7614\n",
      "Epoch 4780/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.9673 - class_loss: 0.0037 - l1_loss: 0.5964\n",
      "Epoch 4781/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.5697 - class_loss: 0.0019 - l1_loss: 0.7568\n",
      "Epoch 4782/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.9143 - class_loss: 0.0011 - l1_loss: 0.4913\n",
      "Epoch 4783/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.2249 - class_loss: 0.0010 - l1_loss: 0.4224\n",
      "Epoch 4784/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.4460 - class_loss: 0.0123 - l1_loss: 0.4434\n",
      "Epoch 4785/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.0703 - class_loss: 0.0135 - l1_loss: 0.5057\n",
      "Epoch 4786/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.1185 - class_loss: 0.0017 - l1_loss: 0.5117\n",
      "Epoch 4787/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.6424 - class_loss: 5.8749e-05 - l1_loss: 0.4642\n",
      "Epoch 4788/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.6541 - class_loss: 7.6105e-04 - l1_loss: 0.4653\n",
      "Epoch 4789/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.8829 - class_loss: 0.0087 - l1_loss: 0.5874\n",
      "Epoch 4790/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 4.8903 - class_loss: 0.0012 - l1_loss: 0.4889ETA: 0s - loss: 5.5964 - class_loss: 6.4986e-04 - l1_loss: \n",
      "Epoch 4791/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.5558 - class_loss: 8.6126e-04 - l1_loss: 0.3555\n",
      "Epoch 4792/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.8134 - class_loss: 0.0017 - l1_loss: 0.3812\n",
      "Epoch 4793/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9298 - class_loss: 0.0251 - l1_loss: 0.4905\n",
      "Epoch 4794/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.6315 - class_loss: 0.0019 - l1_loss: 0.4630\n",
      "Epoch 4795/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5437 - class_loss: 7.4794e-04 - l1_loss: 0.4543\n",
      "Epoch 4796/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.4394 - class_loss: 3.5817e-04 - l1_loss: 0.6439 0s - loss: 6.2776 - class_loss: 4.0777e-04 - l1_loss: 0.62\n",
      "Epoch 4797/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.2413 - class_loss: 5.8899e-04 - l1_loss: 0.5241\n",
      "Epoch 4798/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.3193 - class_loss: 5.9523e-04 - l1_loss: 0.4319\n",
      "Epoch 4799/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.6493 - class_loss: 4.3060e-04 - l1_loss: 0.5649\n",
      "Epoch 4800/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.3294 - class_loss: 0.0010 - l1_loss: 0.6328\n",
      "Epoch 4801/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 7.3637 - class_loss: 0.0011 - l1_loss: 0.7363\n",
      "Epoch 4802/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.9528 - class_loss: 0.0032 - l1_loss: 0.9950\n",
      "Epoch 4803/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.4065 - class_loss: 0.0075 - l1_loss: 1.0399TA: 0s - loss: 9.5882 - class_loss: 0.0093 - l1_loss: 0.\n",
      "Epoch 4804/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.6435 - class_loss: 9.2422e-04 - l1_loss: 0.8643\n",
      "Epoch 4805/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.4893 - class_loss: 6.6512e-05 - l1_loss: 1.0489\n",
      "Epoch 4806/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.6323 - class_loss: 7.2269e-04 - l1_loss: 0.6632\n",
      "Epoch 4807/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.1188 - class_loss: 0.0036 - l1_loss: 0.8115\n",
      "Epoch 4808/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.8964 - class_loss: 0.0019 - l1_loss: 0.6894\n",
      "Epoch 4809/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.0204 - class_loss: 0.0013 - l1_loss: 0.6019\n",
      "Epoch 4810/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.4830 - class_loss: 0.0026 - l1_loss: 0.4480\n",
      "Epoch 4811/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.5483 - class_loss: 3.7498e-04 - l1_loss: 0.4548 0s - loss: 3.8775 - class_loss: 1.9672e-04 - l1_loss: 0.\n",
      "Epoch 4812/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.3822 - class_loss: 0.0421 - l1_loss: 0.6340\n",
      "Epoch 4813/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 31ms/step - loss: 4.8444 - class_loss: 8.6011e-05 - l1_loss: 0.4844\n",
      "Epoch 4814/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.0392 - class_loss: 8.5192e-05 - l1_loss: 0.5039\n",
      "Epoch 4815/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.0223 - class_loss: 0.0175 - l1_loss: 0.8005\n",
      "Epoch 4816/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.9676 - class_loss: 8.9792e-04 - l1_loss: 0.4967\n",
      "Epoch 4817/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.4766 - class_loss: 1.8574e-04 - l1_loss: 0.6476\n",
      "Epoch 4818/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.5553 - class_loss: 3.3392e-05 - l1_loss: 0.4555\n",
      "Epoch 4819/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.8152 - class_loss: 3.4640e-04 - l1_loss: 0.4815\n",
      "Epoch 4820/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.4459 - class_loss: 0.0018 - l1_loss: 0.4444\n",
      "Epoch 4821/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.7425 - class_loss: 0.0011 - l1_loss: 0.5741\n",
      "Epoch 4822/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.2383 - class_loss: 6.3857e-04 - l1_loss: 0.4238\n",
      "Epoch 4823/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.4977 - class_loss: 0.0223 - l1_loss: 0.5475 0s - loss: 5.4788 - class_loss: 0.0254 - l1_loss: 0.54\n",
      "Epoch 4824/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.4001 - class_loss: 0.0094 - l1_loss: 0.3391\n",
      "Epoch 4825/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 4.2500 - class_loss: 0.0070 - l1_loss: 0.4243\n",
      "Epoch 4826/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.0013 - class_loss: 6.1295e-04 - l1_loss: 0.3001\n",
      "Epoch 4827/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.8006 - class_loss: 0.0041 - l1_loss: 0.2796\n",
      "Epoch 4828/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0056 - class_loss: 0.0047 - l1_loss: 0.3001\n",
      "Epoch 4829/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8442 - class_loss: 0.0027 - l1_loss: 0.3841\n",
      "Epoch 4830/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.5261 - class_loss: 0.0019 - l1_loss: 0.2524\n",
      "Epoch 4831/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7994 - class_loss: 0.0013 - l1_loss: 0.2798\n",
      "Epoch 4832/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2074 - class_loss: 8.4120e-04 - l1_loss: 0.2207\n",
      "Epoch 4833/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.9307 - class_loss: 6.9530e-04 - l1_loss: 0.2930\n",
      "Epoch 4834/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.8845 - class_loss: 0.0015 - l1_loss: 0.2883\n",
      "Epoch 4835/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.9467 - class_loss: 2.5704e-04 - l1_loss: 0.2946\n",
      "Epoch 4836/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.7863 - class_loss: 0.0014 - l1_loss: 0.2785\n",
      "Epoch 4837/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2467 - class_loss: 0.0022 - l1_loss: 0.2245\n",
      "Epoch 4838/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.0844 - class_loss: 0.0030 - l1_loss: 0.2081\n",
      "Epoch 4839/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.1337 - class_loss: 0.0026 - l1_loss: 0.2131\n",
      "Epoch 4840/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0682 - class_loss: 0.0022 - l1_loss: 0.2066\n",
      "Epoch 4841/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.8967 - class_loss: 0.0068 - l1_loss: 0.1890\n",
      "Epoch 4842/5000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.2472 - class_loss: 0.0031 - l1_loss: 0.2244\n",
      "Epoch 4843/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.9890 - class_loss: 9.6600e-04 - l1_loss: 0.1988\n",
      "Epoch 4844/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.1277 - class_loss: 0.0010 - l1_loss: 0.2127\n",
      "Epoch 4845/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.6317 - class_loss: 0.0016 - l1_loss: 0.1630\n",
      "Epoch 4846/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.4378 - class_loss: 0.0021 - l1_loss: 0.1436\n",
      "Epoch 4847/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.8469 - class_loss: 0.0050 - l1_loss: 0.1842\n",
      "Epoch 4848/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.5778 - class_loss: 0.0027 - l1_loss: 0.1575\n",
      "Epoch 4849/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8186 - class_loss: 0.0011 - l1_loss: 0.1817\n",
      "Epoch 4850/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.9034 - class_loss: 4.6865e-04 - l1_loss: 0.1903\n",
      "Epoch 4851/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.6822 - class_loss: 6.7936e-04 - l1_loss: 0.1682\n",
      "Epoch 4852/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.1099 - class_loss: 7.0798e-04 - l1_loss: 0.2109\n",
      "Epoch 4853/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.5877 - class_loss: 0.0022 - l1_loss: 0.2586\n",
      "Epoch 4854/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.0378 - class_loss: 0.0018 - l1_loss: 0.2036\n",
      "Epoch 4855/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.8208 - class_loss: 8.8617e-04 - l1_loss: 0.1820\n",
      "Epoch 4856/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6577 - class_loss: 0.0013 - l1_loss: 0.1656\n",
      "Epoch 4857/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.6806 - class_loss: 0.0023 - l1_loss: 0.1678\n",
      "Epoch 4858/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.2698 - class_loss: 0.0035 - l1_loss: 0.1266\n",
      "Epoch 4859/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.6049 - class_loss: 0.0029 - l1_loss: 0.1602\n",
      "Epoch 4860/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.8876 - class_loss: 0.0025 - l1_loss: 0.1885\n",
      "Epoch 4861/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.9380 - class_loss: 0.0018 - l1_loss: 0.1936\n",
      "Epoch 4862/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5748 - class_loss: 0.0016 - l1_loss: 0.1573\n",
      "Epoch 4863/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.0554 - class_loss: 0.0010 - l1_loss: 0.2054\n",
      "Epoch 4864/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.1372 - class_loss: 0.0041 - l1_loss: 0.5133\n",
      "Epoch 4865/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.5991 - class_loss: 0.0049 - l1_loss: 0.3594\n",
      "Epoch 4866/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.2195 - class_loss: 0.0016 - l1_loss: 0.3218\n",
      "Epoch 4867/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.6469 - class_loss: 6.3660e-04 - l1_loss: 0.3646\n",
      "Epoch 4868/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 3.6970 - class_loss: 0.0012 - l1_loss: 0.3696\n",
      "Epoch 4869/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 4.7520 - class_loss: 0.0078 - l1_loss: 0.4744\n",
      "Epoch 4870/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.1738 - class_loss: 0.0035 - l1_loss: 0.7170\n",
      "Epoch 4871/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 6.0250 - class_loss: 0.0021 - l1_loss: 0.6023\n",
      "Epoch 4872/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.6151 - class_loss: 0.0027 - l1_loss: 0.6612\n",
      "Epoch 4873/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.4067 - class_loss: 0.0015 - l1_loss: 0.5405\n",
      "Epoch 4874/5000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 6.4612 - class_loss: 0.0019 - l1_loss: 0.6459\n",
      "Epoch 4875/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.1985 - class_loss: 0.0010 - l1_loss: 0.5197\n",
      "Epoch 4876/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 7.6697 - class_loss: 4.8588e-04 - l1_loss: 0.7669\n",
      "Epoch 4877/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.2475 - class_loss: 0.0012 - l1_loss: 0.7246\n",
      "Epoch 4878/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.2477 - class_loss: 0.0015 - l1_loss: 0.7246\n",
      "Epoch 4879/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.3290 - class_loss: 6.3970e-04 - l1_loss: 0.7328\n",
      "Epoch 4880/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.3343 - class_loss: 4.2503e-04 - l1_loss: 0.8334\n",
      "Epoch 4881/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.4535 - class_loss: 3.2820e-04 - l1_loss: 0.8453\n",
      "Epoch 4882/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 9.5750 - class_loss: 6.9981e-04 - l1_loss: 0.9574\n",
      "Epoch 4883/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.1917 - class_loss: 0.0039 - l1_loss: 0.6188\n",
      "Epoch 4884/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 8.4469 - class_loss: 6.0903e-04 - l1_loss: 0.8446\n",
      "Epoch 4885/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 7.9236 - class_loss: 0.0016 - l1_loss: 0.7922\n",
      "Epoch 4886/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 10.4386 - class_loss: 0.0069 - l1_loss: 1.0432\n",
      "Epoch 4887/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 11.8176 - class_loss: 0.0076 - l1_loss: 1.1810\n",
      "Epoch 4888/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12.4113 - class_loss: 0.0019 - l1_loss: 1.2409\n",
      "Epoch 4889/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.4342 - class_loss: 8.9476e-04 - l1_loss: 1.3433\n",
      "Epoch 4890/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 12.3407 - class_loss: 0.0014 - l1_loss: 1.2339\n",
      "Epoch 4891/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.0517 - class_loss: 0.0108 - l1_loss: 0.9041\n",
      "Epoch 4892/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 7.8927 - class_loss: 0.0014 - l1_loss: 0.7891\n",
      "Epoch 4893/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.3446 - class_loss: 0.0015 - l1_loss: 0.9343\n",
      "Epoch 4894/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.4910 - class_loss: 0.0047 - l1_loss: 0.8486\n",
      "Epoch 4895/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.1181 - class_loss: 0.0034 - l1_loss: 0.8115\n",
      "Epoch 4896/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.8074 - class_loss: 0.0015 - l1_loss: 0.7806\n",
      "Epoch 4897/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.9422 - class_loss: 0.0126 - l1_loss: 1.0930\n",
      "Epoch 4898/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 10.2224 - class_loss: 0.0013 - l1_loss: 1.0221 ETA: 0s - loss: 13.4042 - class_loss: 4.5802e-04 - l1_loss: 1.\n",
      "Epoch 4899/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.9195 - class_loss: 0.0010 - l1_loss: 0.9919\n",
      "Epoch 4900/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.5545 - class_loss: 0.0011 - l1_loss: 1.1553\n",
      "Epoch 4901/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.8321 - class_loss: 5.7667e-05 - l1_loss: 0.9832\n",
      "Epoch 4902/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 22.4267 - class_loss: 1.3501e-04 - l1_loss: 2.2427\n",
      "Epoch 4903/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 17.1037 - class_loss: 0.0037 - l1_loss: 1.7100\n",
      "Epoch 4904/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 12.6789 - class_loss: 0.0037 - l1_loss: 1.2675 ETA: 0s - loss: 12.3867 - class_loss: 0.0030 - l1_loss: 1.238\n",
      "Epoch 4905/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.7435 - class_loss: 0.0055 - l1_loss: 1.6738\n",
      "Epoch 4906/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 13.2966 - class_loss: 0.0054 - l1_loss: 1.3291: 0s - loss: 14.3402 - class_loss: 0.0079 - l1_loss: 1.43\n",
      "Epoch 4907/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 10.0638 - class_loss: 0.0058 - l1_loss: 1.0058\n",
      "Epoch 4908/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.9303 - class_loss: 0.0064 - l1_loss: 1.3924\n",
      "Epoch 4909/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.7721 - class_loss: 0.0021 - l1_loss: 1.0770TA: 0s - loss: 6.3674 - class_loss: 0.0021 - l1_loss: 0.63\n",
      "Epoch 4910/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 16.5967 - class_loss: 0.0015 - l1_loss: 1.6595\n",
      "Epoch 4911/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 16.8111 - class_loss: 0.0034 - l1_loss: 1.6808\n",
      "Epoch 4912/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.7667 - class_loss: 0.0032 - l1_loss: 1.1764\n",
      "Epoch 4913/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 18.0974 - class_loss: 0.0137 - l1_loss: 1.8084\n",
      "Epoch 4914/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.8249 - class_loss: 7.8584e-05 - l1_loss: 1.3825\n",
      "Epoch 4915/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 11.7485 - class_loss: 0.0016 - l1_loss: 1.1747\n",
      "Epoch 4916/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.8477 - class_loss: 0.0789 - l1_loss: 1.4769\n",
      "Epoch 4917/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.4408 - class_loss: 0.0061 - l1_loss: 1.3435\n",
      "Epoch 4918/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.9805 - class_loss: 0.0018 - l1_loss: 1.3979\n",
      "Epoch 4919/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 19.6294 - class_loss: 0.0025 - l1_loss: 1.9627\n",
      "Epoch 4920/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 25.5847 - class_loss: 0.0053 - l1_loss: 2.5579\n",
      "Epoch 4921/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 34.5412 - class_loss: 0.0102 - l1_loss: 3.4531\n",
      "Epoch 4922/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 36.0197 - class_loss: 0.1901 - l1_loss: 3.5830\n",
      "Epoch 4923/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 53.7194 - class_loss: 1.4566 - l1_loss: 5.2263\n",
      "Epoch 4924/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 36.0285 - class_loss: 0.3923 - l1_loss: 3.5636: 0s - loss: 42.6352 - class_loss: 0.4185 - l1_loss: 4.22\n",
      "Epoch 4925/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 46.8905 - class_loss: 0.1335 - l1_loss: 4.6757\n",
      "Epoch 4926/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 28.3180 - class_loss: 9.3962e-04 - l1_loss: 2.8317\n",
      "Epoch 4927/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 34.2688 - class_loss: 0.0016 - l1_loss: 3.4267\n",
      "Epoch 4928/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 24.7577 - class_loss: 8.4039e-05 - l1_loss: 2.4758\n",
      "Epoch 4929/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 24.1169 - class_loss: 0.0131 - l1_loss: 2.4104\n",
      "Epoch 4930/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 18.6735 - class_loss: 0.0837 - l1_loss: 1.8590\n",
      "Epoch 4931/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 24.0932 - class_loss: 2.5648e-04 - l1_loss: 2.4093\n",
      "Epoch 4932/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 19.7536 - class_loss: 0.0197 - l1_loss: 1.9734\n",
      "Epoch 4933/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 17.9455 - class_loss: 0.0873 - l1_loss: 1.7858\n",
      "Epoch 4934/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 14.5174 - class_loss: 1.6851e-06 - l1_loss: 1.4517\n",
      "Epoch 4935/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 23.3499 - class_loss: 2.5426e-05 - l1_loss: 2.3350\n",
      "Epoch 4936/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 15.1972 - class_loss: 6.6766e-05 - l1_loss: 1.5197\n",
      "Epoch 4937/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 14.2270 - class_loss: 7.9807e-04 - l1_loss: 1.4226\n",
      "Epoch 4938/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.8243 - class_loss: 0.0040 - l1_loss: 0.9820\n",
      "Epoch 4939/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 13.4667 - class_loss: 0.0474 - l1_loss: 1.3419\n",
      "Epoch 4940/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 11.4945 - class_loss: 0.5261 - l1_loss: 1.0968\n",
      "Epoch 4941/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 13.3386 - class_loss: 0.0120 - l1_loss: 1.3327\n",
      "Epoch 4942/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 30ms/step - loss: 13.6511 - class_loss: 0.0290 - l1_loss: 1.3622\n",
      "Epoch 4943/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 10.2712 - class_loss: 0.0062 - l1_loss: 1.0265\n",
      "Epoch 4944/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 10.0590 - class_loss: 0.0032 - l1_loss: 1.0056\n",
      "Epoch 4945/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.9047 - class_loss: 0.0040 - l1_loss: 0.5901\n",
      "Epoch 4946/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 8.3836 - class_loss: 2.3582e-05 - l1_loss: 0.8384\n",
      "Epoch 4947/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.2029 - class_loss: 2.7481e-05 - l1_loss: 0.6203\n",
      "Epoch 4948/5000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 4.5027 - class_loss: 3.6083e-05 - l1_loss: 0.4503\n",
      "Epoch 4949/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.3262 - class_loss: 4.8085e-04 - l1_loss: 0.5326\n",
      "Epoch 4950/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.3793 - class_loss: 0.0372 - l1_loss: 0.5342 0s - loss: 5.9994 - class_loss: 0.0590 - l1_loss: 0.\n",
      "Epoch 4951/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 5.2523 - class_loss: 6.2634e-04 - l1_loss: 0.5252\n",
      "Epoch 4952/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.2250 - class_loss: 7.9783e-04 - l1_loss: 0.6224\n",
      "Epoch 4953/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.5748 - class_loss: 0.0012 - l1_loss: 0.8574\n",
      "Epoch 4954/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 6.7622 - class_loss: 0.0015 - l1_loss: 0.6761\n",
      "Epoch 4955/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.2333 - class_loss: 0.0018 - l1_loss: 0.6231\n",
      "Epoch 4956/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.7701 - class_loss: 0.0061 - l1_loss: 0.5764\n",
      "Epoch 4957/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.6483 - class_loss: 0.0032 - l1_loss: 0.6645\n",
      "Epoch 4958/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 5.9665 - class_loss: 0.0018 - l1_loss: 0.5965\n",
      "Epoch 4959/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 7.0567 - class_loss: 0.0020 - l1_loss: 0.7055ETA: 0s - loss: 7.6859 - class_loss: 0.0019 - l1_loss: 0.7684  \n",
      "Epoch 4960/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.0133 - class_loss: 0.0037 - l1_loss: 0.5010\n",
      "Epoch 4961/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.2264 - class_loss: 4.5523e-04 - l1_loss: 0.4226\n",
      "Epoch 4962/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.5916 - class_loss: 0.0022 - l1_loss: 0.3589\n",
      "Epoch 4963/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.1559 - class_loss: 0.0047 - l1_loss: 0.3151\n",
      "Epoch 4964/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.8733 - class_loss: 0.0114 - l1_loss: 0.3862\n",
      "Epoch 4965/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.4206 - class_loss: 0.0052 - l1_loss: 0.4415\n",
      "Epoch 4966/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.8626 - class_loss: 0.0019 - l1_loss: 0.3861\n",
      "Epoch 4967/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 4.2683 - class_loss: 0.0036 - l1_loss: 0.4265\n",
      "Epoch 4968/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 5.2938 - class_loss: 0.0074 - l1_loss: 0.5286\n",
      "Epoch 4969/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.2593 - class_loss: 0.0021 - l1_loss: 0.5257\n",
      "Epoch 4970/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 6.9567 - class_loss: 0.0027 - l1_loss: 0.6954\n",
      "Epoch 4971/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 5.1601 - class_loss: 0.0022 - l1_loss: 0.5158\n",
      "Epoch 4972/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.4866 - class_loss: 0.0011 - l1_loss: 0.4485\n",
      "Epoch 4973/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.3259 - class_loss: 0.0014 - l1_loss: 0.3324\n",
      "Epoch 4974/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 4.0089 - class_loss: 0.0060 - l1_loss: 0.4003\n",
      "Epoch 4975/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 3.0702 - class_loss: 0.0017 - l1_loss: 0.3068\n",
      "Epoch 4976/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6706 - class_loss: 8.6788e-04 - l1_loss: 0.2670\n",
      "Epoch 4977/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3.1870 - class_loss: 0.0014 - l1_loss: 0.3186\n",
      "Epoch 4978/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.4643 - class_loss: 0.0036 - l1_loss: 0.2461\n",
      "Epoch 4979/5000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.1348 - class_loss: 0.0060 - l1_loss: 0.2129\n",
      "Epoch 4980/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8646 - class_loss: 0.0049 - l1_loss: 0.1860\n",
      "Epoch 4981/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.2026 - class_loss: 0.0030 - l1_loss: 0.2200\n",
      "Epoch 4982/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5563 - class_loss: 0.0019 - l1_loss: 0.1554\n",
      "Epoch 4983/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.9718 - class_loss: 0.0030 - l1_loss: 0.1969\n",
      "Epoch 4984/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.0336 - class_loss: 0.0015 - l1_loss: 0.2032\n",
      "Epoch 4985/5000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.9976 - class_loss: 5.7191e-04 - l1_loss: 0.1997\n",
      "Epoch 4986/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.3676 - class_loss: 0.0012 - l1_loss: 0.1366\n",
      "Epoch 4987/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.8255 - class_loss: 0.0030 - l1_loss: 0.1822\n",
      "Epoch 4988/5000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.5408 - class_loss: 0.0036 - l1_loss: 0.1537\n",
      "Epoch 4989/5000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6103 - class_loss: 0.0043 - l1_loss: 0.1606\n",
      "Epoch 4990/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5560 - class_loss: 0.0022 - l1_loss: 0.1554\n",
      "Epoch 4991/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.5087 - class_loss: 0.0016 - l1_loss: 0.1507\n",
      "Epoch 4992/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.4676 - class_loss: 0.0019 - l1_loss: 0.1466\n",
      "Epoch 4993/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9099 - class_loss: 8.0368e-04 - l1_loss: 0.1909\n",
      "Epoch 4994/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.1192 - class_loss: 3.1404e-04 - l1_loss: 0.2119\n",
      "Epoch 4995/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.3625 - class_loss: 0.0012 - l1_loss: 0.2361\n",
      "Epoch 4996/5000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.4918 - class_loss: 0.0013 - l1_loss: 0.2490\n",
      "Epoch 4997/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.1211 - class_loss: 0.0017 - l1_loss: 0.2119\n",
      "Epoch 4998/5000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.5387 - class_loss: 0.0025 - l1_loss: 0.1536\n",
      "Epoch 4999/5000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.9227 - class_loss: 0.0062 - l1_loss: 0.1917\n",
      "Epoch 5000/5000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.2541 - class_loss: 0.0014 - l1_loss: 0.1253\n"
     ]
    }
   ],
   "source": [
    "from RPN import RPN\n",
    "\n",
    "model = RPN(9  , range_positive=0.5,\n",
    "                 range_negative=0.1,\n",
    "                 scales=np.array([0.5, 1, 2]),\n",
    "                 dims=np.array([0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]))\n",
    "model.build((None, 20, 20, 512))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam()\n",
    ")\n",
    "\n",
    "history = model.fit(res, bbox, batch_size=1, epochs=5000, callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50b99f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6\n",
    "res_one = tf.expand_dims(res[n], axis = 0)\n",
    "img = images[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8c2a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output_deltas, predicted_output_scores = model(res_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "324c504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_true = tf.constant(utils.generate_bbox_coords(dims = np.array([0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5])), dtype = tf.float32)\n",
    "deltas = tf.reshape(predicted_output_deltas, (-1,4))\n",
    "scores = tf.reshape(predicted_output_scores, (-1,1))\n",
    "    \n",
    "    ## deltas + bbox_cords \n",
    "adjusted_bbox_cords = bboxes_true + deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d32ceaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tf.image.non_max_suppression(\n",
    "    utils.to_box(adjusted_bbox_cords), scores[:,0], 2000, iou_threshold=0.2, score_threshold = 0.7,\n",
    ")\n",
    "bboxes = tf.math.ceil(utils.to_box(tf.gather(adjusted_bbox_cords, ids)) * (640., 640., 640., 640.)) # multiplied by image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00a111f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27d0da4f988>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "image_with_bounding_box = img.numpy()\n",
    "\n",
    "for i in range(bboxes.shape[0]):\n",
    "#for i in range(2):\n",
    "    image_with_bounding_box = cv2.rectangle(image_with_bounding_box, (int(bboxes[i,0]), int(bboxes[i,1])), (int(bboxes[i,2]), int(bboxes[i,3])), (0, 255, 0), 5)\n",
    "plt.imshow(image_with_bounding_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192bd9a",
   "metadata": {},
   "source": [
    "# Testing keras and tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "dbc1bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(LossLayer, self).__init__()\n",
    "        self.loss_fn = lambda x,y: tf.keras.backend.abs(x - y)\n",
    "\n",
    "    def call(self, inputs, inputs1):\n",
    "        if inputs1 is not None:\n",
    "            self.add_loss(self.loss_fn(inputs,inputs1))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "d885d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5])\n",
    "y = np.array([5,6,7,8,9]) # learn to add bias\n",
    "y_fake = np.array([0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "417ba4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_true = Input(shape=(1))\n",
    "input_x = Input(shape=(1))\n",
    "dense = Dense(1)(input_x)\n",
    "loss = LossLayer()(dense, input_true)\n",
    "\n",
    "model = Model(inputs=[input_x, input_true], outputs=[loss])\n",
    "#loss =  tf.abs((dense - input_true))\n",
    "#model.add_loss(loss)\n",
    "#model.add_metric(loss, name='kl_loss', aggregation='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "9ad2bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "9c3cf693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_112\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_146 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 1)            2           input_146[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_145 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "loss_layer_42 (LossLayer)       (None, 1)            0           dense_48[0][0]                   \n",
      "                                                                 input_145[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "9df823d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "1/1 [==============================] - 0s 989us/step - loss: 6.4944\n",
      "Epoch 2/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4904\n",
      "Epoch 3/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4864\n",
      "Epoch 4/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4824\n",
      "Epoch 5/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4784\n",
      "Epoch 6/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4744\n",
      "Epoch 7/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4704\n",
      "Epoch 8/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4664\n",
      "Epoch 9/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4624\n",
      "Epoch 10/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4584\n",
      "Epoch 11/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4544\n",
      "Epoch 12/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4504\n",
      "Epoch 13/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4464\n",
      "Epoch 14/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4424\n",
      "Epoch 15/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4384\n",
      "Epoch 16/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4344\n",
      "Epoch 17/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4304\n",
      "Epoch 18/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4264\n",
      "Epoch 19/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4224\n",
      "Epoch 20/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4184\n",
      "Epoch 21/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4144\n",
      "Epoch 22/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4104\n",
      "Epoch 23/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4064\n",
      "Epoch 24/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4024\n",
      "Epoch 25/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3984\n",
      "Epoch 26/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3944\n",
      "Epoch 27/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.3904\n",
      "Epoch 28/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.3864\n",
      "Epoch 29/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3824\n",
      "Epoch 30/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3784\n",
      "Epoch 31/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3744\n",
      "Epoch 32/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3704\n",
      "Epoch 33/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3664\n",
      "Epoch 34/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3624\n",
      "Epoch 35/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3584\n",
      "Epoch 36/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3544\n",
      "Epoch 37/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3504\n",
      "Epoch 38/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.3464\n",
      "Epoch 39/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3424\n",
      "Epoch 40/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.3384\n",
      "Epoch 41/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.3344\n",
      "Epoch 42/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3304\n",
      "Epoch 43/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3264\n",
      "Epoch 44/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3224\n",
      "Epoch 45/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.3184\n",
      "Epoch 46/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3144\n",
      "Epoch 47/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3104\n",
      "Epoch 48/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.3064\n",
      "Epoch 49/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.3024\n",
      "Epoch 50/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2984\n",
      "Epoch 51/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.2944\n",
      "Epoch 52/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2904\n",
      "Epoch 53/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2864\n",
      "Epoch 54/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.2824\n",
      "Epoch 55/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.2784\n",
      "Epoch 56/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2744\n",
      "Epoch 57/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.2704\n",
      "Epoch 58/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2664\n",
      "Epoch 59/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2624\n",
      "Epoch 60/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.2584\n",
      "Epoch 61/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2544\n",
      "Epoch 62/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.2504\n",
      "Epoch 63/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.2464\n",
      "Epoch 64/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.2424\n",
      "Epoch 65/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2384\n",
      "Epoch 66/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2344\n",
      "Epoch 67/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2304\n",
      "Epoch 68/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2264\n",
      "Epoch 69/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.2224\n",
      "Epoch 70/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2184\n",
      "Epoch 71/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 6.2144\n",
      "Epoch 72/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2104\n",
      "Epoch 73/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.2064\n",
      "Epoch 74/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2024\n",
      "Epoch 75/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1984\n",
      "Epoch 76/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1944\n",
      "Epoch 77/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.1904\n",
      "Epoch 78/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1864\n",
      "Epoch 79/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1824\n",
      "Epoch 80/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1784\n",
      "Epoch 81/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1744\n",
      "Epoch 82/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1704\n",
      "Epoch 83/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1664\n",
      "Epoch 84/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1624\n",
      "Epoch 85/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1584\n",
      "Epoch 86/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1544\n",
      "Epoch 87/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1504\n",
      "Epoch 88/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1464\n",
      "Epoch 89/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1424\n",
      "Epoch 90/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1384\n",
      "Epoch 91/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1344\n",
      "Epoch 92/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1304\n",
      "Epoch 93/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1264\n",
      "Epoch 94/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1224\n",
      "Epoch 95/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1184\n",
      "Epoch 96/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.1144\n",
      "Epoch 97/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1104\n",
      "Epoch 98/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.1064\n",
      "Epoch 99/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1024\n",
      "Epoch 100/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.0984\n",
      "Epoch 101/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0944\n",
      "Epoch 102/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0904\n",
      "Epoch 103/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.0864\n",
      "Epoch 104/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0824\n",
      "Epoch 105/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.0784\n",
      "Epoch 106/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0744\n",
      "Epoch 107/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.0704\n",
      "Epoch 108/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0664\n",
      "Epoch 109/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.0624\n",
      "Epoch 110/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0584\n",
      "Epoch 111/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0544\n",
      "Epoch 112/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0504\n",
      "Epoch 113/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.0464\n",
      "Epoch 114/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.0424\n",
      "Epoch 115/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0384\n",
      "Epoch 116/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.0344\n",
      "Epoch 117/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0304\n",
      "Epoch 118/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0264\n",
      "Epoch 119/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0224\n",
      "Epoch 120/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0184\n",
      "Epoch 121/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0144\n",
      "Epoch 122/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0104\n",
      "Epoch 123/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.0064\n",
      "Epoch 124/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0024\n",
      "Epoch 125/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9984\n",
      "Epoch 126/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9944\n",
      "Epoch 127/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9904\n",
      "Epoch 128/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.9864\n",
      "Epoch 129/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.9824\n",
      "Epoch 130/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9784\n",
      "Epoch 131/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.9744\n",
      "Epoch 132/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9704\n",
      "Epoch 133/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.9664\n",
      "Epoch 134/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.9624\n",
      "Epoch 135/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.9584\n",
      "Epoch 136/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.9544\n",
      "Epoch 137/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9504\n",
      "Epoch 138/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.9464\n",
      "Epoch 139/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 5.9424\n",
      "Epoch 140/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9384\n",
      "Epoch 141/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.9344\n",
      "Epoch 142/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9304\n",
      "Epoch 143/2000\n",
      "1/1 [==============================] - 0s 993us/step - loss: 5.9264\n",
      "Epoch 144/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.9224\n",
      "Epoch 145/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.9184\n",
      "Epoch 146/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.9144\n",
      "Epoch 147/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.9104\n",
      "Epoch 148/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 5.9064\n",
      "Epoch 149/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.9024\n",
      "Epoch 150/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.8984\n",
      "Epoch 151/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.8944\n",
      "Epoch 152/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.8904\n",
      "Epoch 153/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 5.8864\n",
      "Epoch 154/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.8824\n",
      "Epoch 155/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.8784\n",
      "Epoch 156/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8744\n",
      "Epoch 157/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.8704\n",
      "Epoch 158/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 5.8664\n",
      "Epoch 159/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8624\n",
      "Epoch 160/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.8584\n",
      "Epoch 161/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8544\n",
      "Epoch 162/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8504\n",
      "Epoch 163/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.8464\n",
      "Epoch 164/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8424\n",
      "Epoch 165/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.8384\n",
      "Epoch 166/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8344\n",
      "Epoch 167/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8304\n",
      "Epoch 168/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8264\n",
      "Epoch 169/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8224\n",
      "Epoch 170/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 5.8184\n",
      "Epoch 171/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8144\n",
      "Epoch 172/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8104\n",
      "Epoch 173/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8064\n",
      "Epoch 174/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8024\n",
      "Epoch 175/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7984\n",
      "Epoch 176/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7944\n",
      "Epoch 177/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7904\n",
      "Epoch 178/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7864\n",
      "Epoch 179/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.7824\n",
      "Epoch 180/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7784\n",
      "Epoch 181/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7744\n",
      "Epoch 182/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.7704\n",
      "Epoch 183/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7664\n",
      "Epoch 184/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7624\n",
      "Epoch 185/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 5.7584\n",
      "Epoch 186/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7544\n",
      "Epoch 187/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7504\n",
      "Epoch 188/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7464\n",
      "Epoch 189/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7424\n",
      "Epoch 190/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7384\n",
      "Epoch 191/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7344\n",
      "Epoch 192/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7304\n",
      "Epoch 193/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.7264\n",
      "Epoch 194/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.7224\n",
      "Epoch 195/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.7184\n",
      "Epoch 196/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.7144\n",
      "Epoch 197/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7104\n",
      "Epoch 198/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7064\n",
      "Epoch 199/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7024\n",
      "Epoch 200/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6984\n",
      "Epoch 201/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6944\n",
      "Epoch 202/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 999us/step - loss: 5.6904\n",
      "Epoch 203/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6864\n",
      "Epoch 204/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6824\n",
      "Epoch 205/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.6784\n",
      "Epoch 206/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6744\n",
      "Epoch 207/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.6704\n",
      "Epoch 208/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.6664\n",
      "Epoch 209/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6624\n",
      "Epoch 210/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6584\n",
      "Epoch 211/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6544\n",
      "Epoch 212/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6504\n",
      "Epoch 213/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6464\n",
      "Epoch 214/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6424\n",
      "Epoch 215/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6384\n",
      "Epoch 216/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.6344\n",
      "Epoch 217/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6304\n",
      "Epoch 218/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.6264\n",
      "Epoch 219/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.6224\n",
      "Epoch 220/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.6184\n",
      "Epoch 221/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6144\n",
      "Epoch 222/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6104\n",
      "Epoch 223/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6064\n",
      "Epoch 224/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6024\n",
      "Epoch 225/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.5984\n",
      "Epoch 226/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.5944\n",
      "Epoch 227/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5904\n",
      "Epoch 228/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.5864\n",
      "Epoch 229/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5824\n",
      "Epoch 230/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5784\n",
      "Epoch 231/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.5744\n",
      "Epoch 232/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.5704\n",
      "Epoch 233/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5664\n",
      "Epoch 234/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 5.5624\n",
      "Epoch 235/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.5584\n",
      "Epoch 236/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 5.5544\n",
      "Epoch 237/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5504\n",
      "Epoch 238/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.5464\n",
      "Epoch 239/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.5424\n",
      "Epoch 240/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.5384\n",
      "Epoch 241/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5344\n",
      "Epoch 242/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5304\n",
      "Epoch 243/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.5264\n",
      "Epoch 244/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5224\n",
      "Epoch 245/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.5184\n",
      "Epoch 246/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5144\n",
      "Epoch 247/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5104\n",
      "Epoch 248/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5064\n",
      "Epoch 249/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.5024\n",
      "Epoch 250/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4984\n",
      "Epoch 251/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4944\n",
      "Epoch 252/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4904\n",
      "Epoch 253/2000\n",
      "1/1 [==============================] - 0s 993us/step - loss: 5.4864\n",
      "Epoch 254/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.4824\n",
      "Epoch 255/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 5.4784\n",
      "Epoch 256/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.4744\n",
      "Epoch 257/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.4704\n",
      "Epoch 258/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.4664\n",
      "Epoch 259/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4624\n",
      "Epoch 260/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4584\n",
      "Epoch 261/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.4544\n",
      "Epoch 262/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4504\n",
      "Epoch 263/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4464\n",
      "Epoch 264/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.4424\n",
      "Epoch 265/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.4384\n",
      "Epoch 266/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4344\n",
      "Epoch 267/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4304\n",
      "Epoch 268/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4264\n",
      "Epoch 269/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4224\n",
      "Epoch 270/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.4184\n",
      "Epoch 271/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4144\n",
      "Epoch 272/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.4104\n",
      "Epoch 273/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4064\n",
      "Epoch 274/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.4024\n",
      "Epoch 275/2000\n",
      "1/1 [==============================] - 0s 984us/step - loss: 5.3984\n",
      "Epoch 276/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.3944\n",
      "Epoch 277/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3904\n",
      "Epoch 278/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.3864\n",
      "Epoch 279/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3824\n",
      "Epoch 280/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.3784\n",
      "Epoch 281/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3744\n",
      "Epoch 282/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.3704\n",
      "Epoch 283/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.3664\n",
      "Epoch 284/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.3624\n",
      "Epoch 285/2000\n",
      "1/1 [==============================] - 0s 994us/step - loss: 5.3584\n",
      "Epoch 286/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3544\n",
      "Epoch 287/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.3504\n",
      "Epoch 288/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3464\n",
      "Epoch 289/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.3424\n",
      "Epoch 290/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.3384\n",
      "Epoch 291/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3344\n",
      "Epoch 292/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.3304\n",
      "Epoch 293/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3264\n",
      "Epoch 294/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.3224\n",
      "Epoch 295/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3184\n",
      "Epoch 296/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.3144\n",
      "Epoch 297/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3104\n",
      "Epoch 298/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3064\n",
      "Epoch 299/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3024\n",
      "Epoch 300/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.2984\n",
      "Epoch 301/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.2944\n",
      "Epoch 302/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2904\n",
      "Epoch 303/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.2864\n",
      "Epoch 304/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2824\n",
      "Epoch 305/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2784\n",
      "Epoch 306/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2744\n",
      "Epoch 307/2000\n",
      "1/1 [==============================] - 0s 993us/step - loss: 5.2704\n",
      "Epoch 308/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2664\n",
      "Epoch 309/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2624\n",
      "Epoch 310/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2584\n",
      "Epoch 311/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2544\n",
      "Epoch 312/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2504\n",
      "Epoch 313/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2464\n",
      "Epoch 314/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.2424\n",
      "Epoch 315/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2384\n",
      "Epoch 316/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.2344\n",
      "Epoch 317/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2304\n",
      "Epoch 318/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2264\n",
      "Epoch 319/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.2224\n",
      "Epoch 320/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2184\n",
      "Epoch 321/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2144\n",
      "Epoch 322/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2104\n",
      "Epoch 323/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2064\n",
      "Epoch 324/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2024\n",
      "Epoch 325/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1984\n",
      "Epoch 326/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.1944\n",
      "Epoch 327/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1904\n",
      "Epoch 328/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1864\n",
      "Epoch 329/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1824\n",
      "Epoch 330/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.1784\n",
      "Epoch 331/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.1744\n",
      "Epoch 332/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1704\n",
      "Epoch 333/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.1664\n",
      "Epoch 334/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.1624\n",
      "Epoch 335/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.1584\n",
      "Epoch 336/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 5.1544\n",
      "Epoch 337/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1504\n",
      "Epoch 338/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1464\n",
      "Epoch 339/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1424\n",
      "Epoch 340/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1384\n",
      "Epoch 341/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.1344\n",
      "Epoch 342/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.1304\n",
      "Epoch 343/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1264\n",
      "Epoch 344/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1224\n",
      "Epoch 345/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1184\n",
      "Epoch 346/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1144\n",
      "Epoch 347/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1104\n",
      "Epoch 348/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1064\n",
      "Epoch 349/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1024\n",
      "Epoch 350/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.0984\n",
      "Epoch 351/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0944\n",
      "Epoch 352/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.0904\n",
      "Epoch 353/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0864\n",
      "Epoch 354/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0824\n",
      "Epoch 355/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0784\n",
      "Epoch 356/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.0744\n",
      "Epoch 357/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0704\n",
      "Epoch 358/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 5.0664\n",
      "Epoch 359/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.0624\n",
      "Epoch 360/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0584\n",
      "Epoch 361/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0544\n",
      "Epoch 362/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0504\n",
      "Epoch 363/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0464\n",
      "Epoch 364/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.0424\n",
      "Epoch 365/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0384\n",
      "Epoch 366/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.0344\n",
      "Epoch 367/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0304\n",
      "Epoch 368/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.0264\n",
      "Epoch 369/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0224\n",
      "Epoch 370/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0184\n",
      "Epoch 371/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0144\n",
      "Epoch 372/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0104\n",
      "Epoch 373/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0064\n",
      "Epoch 374/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.0024\n",
      "Epoch 375/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9984\n",
      "Epoch 376/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.9944\n",
      "Epoch 377/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9904\n",
      "Epoch 378/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9864\n",
      "Epoch 379/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.9824\n",
      "Epoch 380/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.9784\n",
      "Epoch 381/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.9744\n",
      "Epoch 382/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.9704\n",
      "Epoch 383/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9664\n",
      "Epoch 384/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9624\n",
      "Epoch 385/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.9584\n",
      "Epoch 386/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9544\n",
      "Epoch 387/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.9504\n",
      "Epoch 388/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.9464\n",
      "Epoch 389/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9424\n",
      "Epoch 390/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.9384\n",
      "Epoch 391/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.9344\n",
      "Epoch 392/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9304\n",
      "Epoch 393/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9264\n",
      "Epoch 394/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.9224\n",
      "Epoch 395/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9184\n",
      "Epoch 396/2000\n",
      "1/1 [==============================] - 0s 976us/step - loss: 4.9144\n",
      "Epoch 397/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.9104\n",
      "Epoch 398/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.9064\n",
      "Epoch 399/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.9024\n",
      "Epoch 400/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.8984\n",
      "Epoch 401/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8944\n",
      "Epoch 402/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8904\n",
      "Epoch 403/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.8864\n",
      "Epoch 404/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.8824\n",
      "Epoch 405/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.8784\n",
      "Epoch 406/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.8744\n",
      "Epoch 407/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.8704\n",
      "Epoch 408/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8664\n",
      "Epoch 409/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.8624\n",
      "Epoch 410/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8584\n",
      "Epoch 411/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8544\n",
      "Epoch 412/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.8504\n",
      "Epoch 413/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8464\n",
      "Epoch 414/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8424\n",
      "Epoch 415/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8384\n",
      "Epoch 416/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8344\n",
      "Epoch 417/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8304\n",
      "Epoch 418/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8264\n",
      "Epoch 419/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.8224\n",
      "Epoch 420/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 4.8184\n",
      "Epoch 421/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8144\n",
      "Epoch 422/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.8104\n",
      "Epoch 423/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8064\n",
      "Epoch 424/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.8024\n",
      "Epoch 425/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.7984\n",
      "Epoch 426/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.7944\n",
      "Epoch 427/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7904\n",
      "Epoch 428/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.7864\n",
      "Epoch 429/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.7824\n",
      "Epoch 430/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7784\n",
      "Epoch 431/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7744\n",
      "Epoch 432/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7704\n",
      "Epoch 433/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7664\n",
      "Epoch 434/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7624\n",
      "Epoch 435/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7584\n",
      "Epoch 436/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7544\n",
      "Epoch 437/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 4.7504\n",
      "Epoch 438/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.7464\n",
      "Epoch 439/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7424\n",
      "Epoch 440/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.7384\n",
      "Epoch 441/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7344\n",
      "Epoch 442/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7304\n",
      "Epoch 443/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.7264\n",
      "Epoch 444/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.7224\n",
      "Epoch 445/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.7184\n",
      "Epoch 446/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.7144\n",
      "Epoch 447/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.7104\n",
      "Epoch 448/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.7064\n",
      "Epoch 449/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7024\n",
      "Epoch 450/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.6984\n",
      "Epoch 451/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6944\n",
      "Epoch 452/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6904\n",
      "Epoch 453/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.6864\n",
      "Epoch 454/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.6824\n",
      "Epoch 455/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6784\n",
      "Epoch 456/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.6744\n",
      "Epoch 457/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6704\n",
      "Epoch 458/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6664\n",
      "Epoch 459/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.6624\n",
      "Epoch 460/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6584\n",
      "Epoch 461/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6544\n",
      "Epoch 462/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.6504\n",
      "Epoch 463/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6464\n",
      "Epoch 464/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6424\n",
      "Epoch 465/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.6384\n",
      "Epoch 466/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.6344\n",
      "Epoch 467/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6304\n",
      "Epoch 468/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6264\n",
      "Epoch 469/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6224\n",
      "Epoch 470/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.6184\n",
      "Epoch 471/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6144\n",
      "Epoch 472/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6104\n",
      "Epoch 473/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.6064\n",
      "Epoch 474/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6024\n",
      "Epoch 475/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.5984\n",
      "Epoch 476/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5944\n",
      "Epoch 477/2000\n",
      "1/1 [==============================] - 0s 987us/step - loss: 4.5904\n",
      "Epoch 478/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.5864\n",
      "Epoch 479/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5824\n",
      "Epoch 480/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.5784\n",
      "Epoch 481/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.5744\n",
      "Epoch 482/2000\n",
      "1/1 [==============================] - 0s 990us/step - loss: 4.5704\n",
      "Epoch 483/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5664\n",
      "Epoch 484/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.5624\n",
      "Epoch 485/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.5584\n",
      "Epoch 486/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.5544\n",
      "Epoch 487/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 4.5504\n",
      "Epoch 488/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.5464\n",
      "Epoch 489/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5424\n",
      "Epoch 490/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.5384\n",
      "Epoch 491/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.5344\n",
      "Epoch 492/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5304\n",
      "Epoch 493/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.5264\n",
      "Epoch 494/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.5224\n",
      "Epoch 495/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.5184\n",
      "Epoch 496/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.5144\n",
      "Epoch 497/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.5104\n",
      "Epoch 498/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.5064\n",
      "Epoch 499/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5024\n",
      "Epoch 500/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.4984\n",
      "Epoch 501/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.4944\n",
      "Epoch 502/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 998us/step - loss: 4.4904\n",
      "Epoch 503/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.4864\n",
      "Epoch 504/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.4824\n",
      "Epoch 505/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4784\n",
      "Epoch 506/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4744\n",
      "Epoch 507/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.4704\n",
      "Epoch 508/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4664\n",
      "Epoch 509/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4624\n",
      "Epoch 510/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.4584\n",
      "Epoch 511/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4544\n",
      "Epoch 512/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.4504\n",
      "Epoch 513/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4464\n",
      "Epoch 514/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.4424\n",
      "Epoch 515/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4384\n",
      "Epoch 516/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4344\n",
      "Epoch 517/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4304\n",
      "Epoch 518/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.4264\n",
      "Epoch 519/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4224\n",
      "Epoch 520/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4184\n",
      "Epoch 521/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4144\n",
      "Epoch 522/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4104\n",
      "Epoch 523/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.4064\n",
      "Epoch 524/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.4024\n",
      "Epoch 525/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.3984\n",
      "Epoch 526/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.3944\n",
      "Epoch 527/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 4.3904\n",
      "Epoch 528/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.3864\n",
      "Epoch 529/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.3824\n",
      "Epoch 530/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3784\n",
      "Epoch 531/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3744\n",
      "Epoch 532/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.3704\n",
      "Epoch 533/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3664\n",
      "Epoch 534/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.3624\n",
      "Epoch 535/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.3584\n",
      "Epoch 536/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3544\n",
      "Epoch 537/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.3504\n",
      "Epoch 538/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.3464\n",
      "Epoch 539/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3424\n",
      "Epoch 540/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3384\n",
      "Epoch 541/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.3344\n",
      "Epoch 542/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3304\n",
      "Epoch 543/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.3264\n",
      "Epoch 544/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3224\n",
      "Epoch 545/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3184\n",
      "Epoch 546/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.3144\n",
      "Epoch 547/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.3104\n",
      "Epoch 548/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3064\n",
      "Epoch 549/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3024\n",
      "Epoch 550/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2984\n",
      "Epoch 551/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2944\n",
      "Epoch 552/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2904\n",
      "Epoch 553/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2864\n",
      "Epoch 554/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.2824\n",
      "Epoch 555/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.2784\n",
      "Epoch 556/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.2744\n",
      "Epoch 557/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.2704\n",
      "Epoch 558/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.2664\n",
      "Epoch 559/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2624\n",
      "Epoch 560/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2584\n",
      "Epoch 561/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.2544\n",
      "Epoch 562/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.2504\n",
      "Epoch 563/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.2464\n",
      "Epoch 564/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.2424\n",
      "Epoch 565/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2384\n",
      "Epoch 566/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2344\n",
      "Epoch 567/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2304\n",
      "Epoch 568/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.2264\n",
      "Epoch 569/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.2224\n",
      "Epoch 570/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2184\n",
      "Epoch 571/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2144\n",
      "Epoch 572/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2104\n",
      "Epoch 573/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2064\n",
      "Epoch 574/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2024\n",
      "Epoch 575/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1984\n",
      "Epoch 576/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.1944\n",
      "Epoch 577/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1904\n",
      "Epoch 578/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.1864\n",
      "Epoch 579/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1824\n",
      "Epoch 580/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.1784\n",
      "Epoch 581/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.1744\n",
      "Epoch 582/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1704\n",
      "Epoch 583/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1664\n",
      "Epoch 584/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.1624\n",
      "Epoch 585/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.1584\n",
      "Epoch 586/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1544\n",
      "Epoch 587/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1504\n",
      "Epoch 588/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.1464\n",
      "Epoch 589/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.1424\n",
      "Epoch 590/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1384\n",
      "Epoch 591/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.1344\n",
      "Epoch 592/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1304\n",
      "Epoch 593/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.1264\n",
      "Epoch 594/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.1224\n",
      "Epoch 595/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.1184\n",
      "Epoch 596/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1144\n",
      "Epoch 597/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1104\n",
      "Epoch 598/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.1064\n",
      "Epoch 599/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.1024\n",
      "Epoch 600/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0984\n",
      "Epoch 601/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0944\n",
      "Epoch 602/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0904\n",
      "Epoch 603/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0864\n",
      "Epoch 604/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.0824\n",
      "Epoch 605/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.0784\n",
      "Epoch 606/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0744\n",
      "Epoch 607/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.0704\n",
      "Epoch 608/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0664\n",
      "Epoch 609/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.0624\n",
      "Epoch 610/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0584\n",
      "Epoch 611/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0544\n",
      "Epoch 612/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0504\n",
      "Epoch 613/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.0464\n",
      "Epoch 614/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.0424\n",
      "Epoch 615/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.0384\n",
      "Epoch 616/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 4.0344\n",
      "Epoch 617/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0304\n",
      "Epoch 618/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.0264\n",
      "Epoch 619/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0224\n",
      "Epoch 620/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0184\n",
      "Epoch 621/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.0144\n",
      "Epoch 622/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0104\n",
      "Epoch 623/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0064\n",
      "Epoch 624/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0024\n",
      "Epoch 625/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 3.9984\n",
      "Epoch 626/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9944\n",
      "Epoch 627/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.9904\n",
      "Epoch 628/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9864\n",
      "Epoch 629/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9824\n",
      "Epoch 630/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.9784\n",
      "Epoch 631/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9744\n",
      "Epoch 632/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9704\n",
      "Epoch 633/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.9664\n",
      "Epoch 634/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.9624\n",
      "Epoch 635/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.9584\n",
      "Epoch 636/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9544\n",
      "Epoch 637/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9504\n",
      "Epoch 638/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.9464\n",
      "Epoch 639/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 3.9424\n",
      "Epoch 640/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9384\n",
      "Epoch 641/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9344\n",
      "Epoch 642/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.9304\n",
      "Epoch 643/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.9264\n",
      "Epoch 644/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9224\n",
      "Epoch 645/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.9184\n",
      "Epoch 646/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9144\n",
      "Epoch 647/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9104\n",
      "Epoch 648/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9064\n",
      "Epoch 649/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.9024\n",
      "Epoch 650/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.8984\n",
      "Epoch 651/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8944\n",
      "Epoch 652/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8904\n",
      "Epoch 653/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.8864\n",
      "Epoch 654/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8824\n",
      "Epoch 655/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8784\n",
      "Epoch 656/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.8744\n",
      "Epoch 657/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.8704\n",
      "Epoch 658/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8664\n",
      "Epoch 659/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8624\n",
      "Epoch 660/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8584\n",
      "Epoch 661/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.8544\n",
      "Epoch 662/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8504\n",
      "Epoch 663/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8464\n",
      "Epoch 664/2000\n",
      "1/1 [==============================] - 0s 974us/step - loss: 3.8424\n",
      "Epoch 665/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.8384\n",
      "Epoch 666/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.8344\n",
      "Epoch 667/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.8304\n",
      "Epoch 668/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8264\n",
      "Epoch 669/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8224\n",
      "Epoch 670/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.8184\n",
      "Epoch 671/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8144\n",
      "Epoch 672/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8104\n",
      "Epoch 673/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8064\n",
      "Epoch 674/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.8024\n",
      "Epoch 675/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7984\n",
      "Epoch 676/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7944\n",
      "Epoch 677/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7904\n",
      "Epoch 678/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.7864\n",
      "Epoch 679/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7824\n",
      "Epoch 680/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7784\n",
      "Epoch 681/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7744\n",
      "Epoch 682/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7704\n",
      "Epoch 683/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7664\n",
      "Epoch 684/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.7624\n",
      "Epoch 685/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7584\n",
      "Epoch 686/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7544\n",
      "Epoch 687/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7504\n",
      "Epoch 688/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7464\n",
      "Epoch 689/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7424\n",
      "Epoch 690/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7384\n",
      "Epoch 691/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 3.7344\n",
      "Epoch 692/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7304\n",
      "Epoch 693/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7264\n",
      "Epoch 694/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.7224\n",
      "Epoch 695/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7184\n",
      "Epoch 696/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7144\n",
      "Epoch 697/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.7104\n",
      "Epoch 698/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7064\n",
      "Epoch 699/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.7024\n",
      "Epoch 700/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6984\n",
      "Epoch 701/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6944\n",
      "Epoch 702/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.6904\n",
      "Epoch 703/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6864\n",
      "Epoch 704/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6824\n",
      "Epoch 705/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6784\n",
      "Epoch 706/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6744\n",
      "Epoch 707/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6704\n",
      "Epoch 708/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6664\n",
      "Epoch 709/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.6624\n",
      "Epoch 710/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6584\n",
      "Epoch 711/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6544\n",
      "Epoch 712/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.6504\n",
      "Epoch 713/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.6464\n",
      "Epoch 714/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.6424\n",
      "Epoch 715/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.6384\n",
      "Epoch 716/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6344\n",
      "Epoch 717/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6304\n",
      "Epoch 718/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 3.6264\n",
      "Epoch 719/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.6224\n",
      "Epoch 720/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6184\n",
      "Epoch 721/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6144\n",
      "Epoch 722/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6104\n",
      "Epoch 723/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6064\n",
      "Epoch 724/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6024\n",
      "Epoch 725/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.5984\n",
      "Epoch 726/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.5944\n",
      "Epoch 727/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5904\n",
      "Epoch 728/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.5864\n",
      "Epoch 729/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.5824\n",
      "Epoch 730/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5784\n",
      "Epoch 731/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5744\n",
      "Epoch 732/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5704\n",
      "Epoch 733/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5664\n",
      "Epoch 734/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.5624\n",
      "Epoch 735/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5584\n",
      "Epoch 736/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5544\n",
      "Epoch 737/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5504\n",
      "Epoch 738/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.5464\n",
      "Epoch 739/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.5424\n",
      "Epoch 740/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.5384\n",
      "Epoch 741/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5344\n",
      "Epoch 742/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5304\n",
      "Epoch 743/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5264\n",
      "Epoch 744/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.5224\n",
      "Epoch 745/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5184\n",
      "Epoch 746/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5144\n",
      "Epoch 747/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5104\n",
      "Epoch 748/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5064\n",
      "Epoch 749/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5024\n",
      "Epoch 750/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4984\n",
      "Epoch 751/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4944\n",
      "Epoch 752/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4904\n",
      "Epoch 753/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.4864\n",
      "Epoch 754/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4824\n",
      "Epoch 755/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4784\n",
      "Epoch 756/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.4744\n",
      "Epoch 757/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4704\n",
      "Epoch 758/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4664\n",
      "Epoch 759/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4624\n",
      "Epoch 760/2000\n",
      "1/1 [==============================] - 0s 977us/step - loss: 3.4584\n",
      "Epoch 761/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4544\n",
      "Epoch 762/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4504\n",
      "Epoch 763/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.4464\n",
      "Epoch 764/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.4424\n",
      "Epoch 765/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4384\n",
      "Epoch 766/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 3.4344\n",
      "Epoch 767/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4304\n",
      "Epoch 768/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4264\n",
      "Epoch 769/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4224\n",
      "Epoch 770/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.4184\n",
      "Epoch 771/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4144\n",
      "Epoch 772/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4104\n",
      "Epoch 773/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.4064\n",
      "Epoch 774/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4024\n",
      "Epoch 775/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3984\n",
      "Epoch 776/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.3944\n",
      "Epoch 777/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3904\n",
      "Epoch 778/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3864\n",
      "Epoch 779/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3824\n",
      "Epoch 780/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3784\n",
      "Epoch 781/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.3744\n",
      "Epoch 782/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3704\n",
      "Epoch 783/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3664\n",
      "Epoch 784/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.3624\n",
      "Epoch 785/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3584\n",
      "Epoch 786/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.3544\n",
      "Epoch 787/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3504\n",
      "Epoch 788/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3464\n",
      "Epoch 789/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 3.3424\n",
      "Epoch 790/2000\n",
      "1/1 [==============================] - 0s 993us/step - loss: 3.3384\n",
      "Epoch 791/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.3344\n",
      "Epoch 792/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3304\n",
      "Epoch 793/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3264\n",
      "Epoch 794/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3224\n",
      "Epoch 795/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.3184\n",
      "Epoch 796/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3144\n",
      "Epoch 797/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3104\n",
      "Epoch 798/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.3064\n",
      "Epoch 799/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3024\n",
      "Epoch 800/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.2984\n",
      "Epoch 801/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.2944\n",
      "Epoch 802/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2904\n",
      "Epoch 803/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2864\n",
      "Epoch 804/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2824\n",
      "Epoch 805/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2784\n",
      "Epoch 806/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2744\n",
      "Epoch 807/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2704\n",
      "Epoch 808/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2664\n",
      "Epoch 809/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2624\n",
      "Epoch 810/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2584\n",
      "Epoch 811/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2544\n",
      "Epoch 812/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2504\n",
      "Epoch 813/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2464\n",
      "Epoch 814/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2424\n",
      "Epoch 815/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.2384\n",
      "Epoch 816/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2344\n",
      "Epoch 817/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2304\n",
      "Epoch 818/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.2264\n",
      "Epoch 819/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2224\n",
      "Epoch 820/2000\n",
      "1/1 [==============================] - 0s 993us/step - loss: 3.2184\n",
      "Epoch 821/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2144\n",
      "Epoch 822/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2104\n",
      "Epoch 823/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2064\n",
      "Epoch 824/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2024\n",
      "Epoch 825/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1984\n",
      "Epoch 826/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1944\n",
      "Epoch 827/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.1904\n",
      "Epoch 828/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1864\n",
      "Epoch 829/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.1824\n",
      "Epoch 830/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1784\n",
      "Epoch 831/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.1744\n",
      "Epoch 832/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.1704\n",
      "Epoch 833/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.1664\n",
      "Epoch 834/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.1624\n",
      "Epoch 835/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1584\n",
      "Epoch 836/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1544\n",
      "Epoch 837/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.1504\n",
      "Epoch 838/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1464\n",
      "Epoch 839/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.1424\n",
      "Epoch 840/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1384\n",
      "Epoch 841/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1344\n",
      "Epoch 842/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.1304\n",
      "Epoch 843/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1264\n",
      "Epoch 844/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1224\n",
      "Epoch 845/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1184\n",
      "Epoch 846/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1144\n",
      "Epoch 847/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 3.1104\n",
      "Epoch 848/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 3.1064\n",
      "Epoch 849/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1024\n",
      "Epoch 850/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0984\n",
      "Epoch 851/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.0944\n",
      "Epoch 852/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.0904\n",
      "Epoch 853/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 3.0864\n",
      "Epoch 854/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0824\n",
      "Epoch 855/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0784\n",
      "Epoch 856/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.0744\n",
      "Epoch 857/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.0704\n",
      "Epoch 858/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.0664\n",
      "Epoch 859/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0624\n",
      "Epoch 860/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 3.0584\n",
      "Epoch 861/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.0544\n",
      "Epoch 862/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0504\n",
      "Epoch 863/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0464\n",
      "Epoch 864/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.0424\n",
      "Epoch 865/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0384\n",
      "Epoch 866/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0344\n",
      "Epoch 867/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0304\n",
      "Epoch 868/2000\n",
      "1/1 [==============================] - 0s 993us/step - loss: 3.0264\n",
      "Epoch 869/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.0224\n",
      "Epoch 870/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0184\n",
      "Epoch 871/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0144\n",
      "Epoch 872/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.0104\n",
      "Epoch 873/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0064\n",
      "Epoch 874/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0024\n",
      "Epoch 875/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.9984\n",
      "Epoch 876/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9944\n",
      "Epoch 877/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9904\n",
      "Epoch 878/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9864\n",
      "Epoch 879/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9824\n",
      "Epoch 880/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9784\n",
      "Epoch 881/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9744\n",
      "Epoch 882/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9704\n",
      "Epoch 883/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9664\n",
      "Epoch 884/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9624\n",
      "Epoch 885/2000\n",
      "1/1 [==============================] - 0s 986us/step - loss: 2.9584\n",
      "Epoch 886/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.9544\n",
      "Epoch 887/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9504\n",
      "Epoch 888/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.9464\n",
      "Epoch 889/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9424\n",
      "Epoch 890/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9384\n",
      "Epoch 891/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9344\n",
      "Epoch 892/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9304\n",
      "Epoch 893/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.9264\n",
      "Epoch 894/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.9224\n",
      "Epoch 895/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9184\n",
      "Epoch 896/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9144\n",
      "Epoch 897/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9104\n",
      "Epoch 898/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9064\n",
      "Epoch 899/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.9024\n",
      "Epoch 900/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8984\n",
      "Epoch 901/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8944\n",
      "Epoch 902/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 2.8904\n",
      "Epoch 903/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8864\n",
      "Epoch 904/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8824\n",
      "Epoch 905/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8784\n",
      "Epoch 906/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.8744\n",
      "Epoch 907/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.8704\n",
      "Epoch 908/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8664\n",
      "Epoch 909/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8624\n",
      "Epoch 910/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8584\n",
      "Epoch 911/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8544\n",
      "Epoch 912/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8504\n",
      "Epoch 913/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.8464\n",
      "Epoch 914/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.8424\n",
      "Epoch 915/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8384\n",
      "Epoch 916/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8344\n",
      "Epoch 917/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8304\n",
      "Epoch 918/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.8264\n",
      "Epoch 919/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8224\n",
      "Epoch 920/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8184\n",
      "Epoch 921/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.8144\n",
      "Epoch 922/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8104\n",
      "Epoch 923/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8064\n",
      "Epoch 924/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.8024\n",
      "Epoch 925/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7984\n",
      "Epoch 926/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7944\n",
      "Epoch 927/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7904\n",
      "Epoch 928/2000\n",
      "1/1 [==============================] - 0s 993us/step - loss: 2.7864\n",
      "Epoch 929/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7824\n",
      "Epoch 930/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7784\n",
      "Epoch 931/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7744\n",
      "Epoch 932/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7704\n",
      "Epoch 933/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.7664\n",
      "Epoch 934/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7624\n",
      "Epoch 935/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7584\n",
      "Epoch 936/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7544\n",
      "Epoch 937/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7504\n",
      "Epoch 938/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7464\n",
      "Epoch 939/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7424\n",
      "Epoch 940/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7384\n",
      "Epoch 941/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.7344\n",
      "Epoch 942/2000\n",
      "1/1 [==============================] - 0s 981us/step - loss: 2.7304\n",
      "Epoch 943/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 2.7264\n",
      "Epoch 944/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7224\n",
      "Epoch 945/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7184\n",
      "Epoch 946/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7144\n",
      "Epoch 947/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7104\n",
      "Epoch 948/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.7064\n",
      "Epoch 949/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7024\n",
      "Epoch 950/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.6984\n",
      "Epoch 951/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.6944\n",
      "Epoch 952/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.6904\n",
      "Epoch 953/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.6864\n",
      "Epoch 954/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.6824\n",
      "Epoch 955/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6784\n",
      "Epoch 956/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6744\n",
      "Epoch 957/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.6704\n",
      "Epoch 958/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.6664\n",
      "Epoch 959/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6624\n",
      "Epoch 960/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6584\n",
      "Epoch 961/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6544\n",
      "Epoch 962/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.6504\n",
      "Epoch 963/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.6464\n",
      "Epoch 964/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.6424\n",
      "Epoch 965/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.6384\n",
      "Epoch 966/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.6344\n",
      "Epoch 967/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6304\n",
      "Epoch 968/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.6264\n",
      "Epoch 969/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.6224\n",
      "Epoch 970/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.6184\n",
      "Epoch 971/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6144\n",
      "Epoch 972/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6104\n",
      "Epoch 973/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.6064\n",
      "Epoch 974/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6024\n",
      "Epoch 975/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5984\n",
      "Epoch 976/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5944\n",
      "Epoch 977/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.5904\n",
      "Epoch 978/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5864\n",
      "Epoch 979/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5824\n",
      "Epoch 980/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.5784\n",
      "Epoch 981/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.5744\n",
      "Epoch 982/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5704\n",
      "Epoch 983/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5664\n",
      "Epoch 984/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5624\n",
      "Epoch 985/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.5584\n",
      "Epoch 986/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5544\n",
      "Epoch 987/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.5504\n",
      "Epoch 988/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.5464\n",
      "Epoch 989/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5424\n",
      "Epoch 990/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5384\n",
      "Epoch 991/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.5344\n",
      "Epoch 992/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5304\n",
      "Epoch 993/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.5264\n",
      "Epoch 994/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.5224\n",
      "Epoch 995/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5184\n",
      "Epoch 996/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5144\n",
      "Epoch 997/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5104\n",
      "Epoch 998/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5064\n",
      "Epoch 999/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5024\n",
      "Epoch 1000/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4984\n",
      "Epoch 1001/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 999us/step - loss: 2.4944\n",
      "Epoch 1002/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4904\n",
      "Epoch 1003/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4864\n",
      "Epoch 1004/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.4824\n",
      "Epoch 1005/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.4784\n",
      "Epoch 1006/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4744\n",
      "Epoch 1007/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.4704\n",
      "Epoch 1008/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.4664\n",
      "Epoch 1009/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4624\n",
      "Epoch 1010/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.4584\n",
      "Epoch 1011/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4544\n",
      "Epoch 1012/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4504\n",
      "Epoch 1013/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4464\n",
      "Epoch 1014/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4424\n",
      "Epoch 1015/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4384\n",
      "Epoch 1016/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4344\n",
      "Epoch 1017/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4304\n",
      "Epoch 1018/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.4264\n",
      "Epoch 1019/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4224\n",
      "Epoch 1020/2000\n",
      "1/1 [==============================] - 0s 985us/step - loss: 2.4184\n",
      "Epoch 1021/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.4144\n",
      "Epoch 1022/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.4104\n",
      "Epoch 1023/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4064\n",
      "Epoch 1024/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4024\n",
      "Epoch 1025/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3984\n",
      "Epoch 1026/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.3944\n",
      "Epoch 1027/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3904\n",
      "Epoch 1028/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.3864\n",
      "Epoch 1029/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3824\n",
      "Epoch 1030/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.3784\n",
      "Epoch 1031/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3744\n",
      "Epoch 1032/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3704\n",
      "Epoch 1033/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.3664\n",
      "Epoch 1034/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3624\n",
      "Epoch 1035/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3584\n",
      "Epoch 1036/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.3544\n",
      "Epoch 1037/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3504\n",
      "Epoch 1038/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3464\n",
      "Epoch 1039/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3424\n",
      "Epoch 1040/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3384\n",
      "Epoch 1041/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3344\n",
      "Epoch 1042/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.3304\n",
      "Epoch 1043/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3264\n",
      "Epoch 1044/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3224\n",
      "Epoch 1045/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3184\n",
      "Epoch 1046/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3144\n",
      "Epoch 1047/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.3104\n",
      "Epoch 1048/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.3064\n",
      "Epoch 1049/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3024\n",
      "Epoch 1050/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2984\n",
      "Epoch 1051/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2944\n",
      "Epoch 1052/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.2904\n",
      "Epoch 1053/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2864\n",
      "Epoch 1054/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.2824\n",
      "Epoch 1055/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.2784\n",
      "Epoch 1056/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.2744\n",
      "Epoch 1057/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2704\n",
      "Epoch 1058/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.2664\n",
      "Epoch 1059/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.2624\n",
      "Epoch 1060/2000\n",
      "1/1 [==============================] - 0s 985us/step - loss: 2.2584\n",
      "Epoch 1061/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2544\n",
      "Epoch 1062/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.2504\n",
      "Epoch 1063/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2464\n",
      "Epoch 1064/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2424\n",
      "Epoch 1065/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2384\n",
      "Epoch 1066/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.2344\n",
      "Epoch 1067/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.2304\n",
      "Epoch 1068/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2264\n",
      "Epoch 1069/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2224\n",
      "Epoch 1070/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2184\n",
      "Epoch 1071/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2144\n",
      "Epoch 1072/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2104\n",
      "Epoch 1073/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.2064\n",
      "Epoch 1074/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2024\n",
      "Epoch 1075/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1984\n",
      "Epoch 1076/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1944\n",
      "Epoch 1077/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.1904\n",
      "Epoch 1078/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.1864\n",
      "Epoch 1079/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.1824\n",
      "Epoch 1080/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1784\n",
      "Epoch 1081/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1744\n",
      "Epoch 1082/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.1704\n",
      "Epoch 1083/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1664\n",
      "Epoch 1084/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1624\n",
      "Epoch 1085/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.1584\n",
      "Epoch 1086/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1544\n",
      "Epoch 1087/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1504\n",
      "Epoch 1088/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1464\n",
      "Epoch 1089/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1424\n",
      "Epoch 1090/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.1384\n",
      "Epoch 1091/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1344\n",
      "Epoch 1092/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1304\n",
      "Epoch 1093/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1264\n",
      "Epoch 1094/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1224\n",
      "Epoch 1095/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1184\n",
      "Epoch 1096/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1144\n",
      "Epoch 1097/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.1104\n",
      "Epoch 1098/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.1064\n",
      "Epoch 1099/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1024\n",
      "Epoch 1100/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0984\n",
      "Epoch 1101/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.0944\n",
      "Epoch 1102/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0904\n",
      "Epoch 1103/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.0864\n",
      "Epoch 1104/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0824\n",
      "Epoch 1105/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0784\n",
      "Epoch 1106/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.0744\n",
      "Epoch 1107/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0704\n",
      "Epoch 1108/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0664\n",
      "Epoch 1109/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.0624\n",
      "Epoch 1110/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0584\n",
      "Epoch 1111/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0544\n",
      "Epoch 1112/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.0504\n",
      "Epoch 1113/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0464\n",
      "Epoch 1114/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.0424\n",
      "Epoch 1115/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0384\n",
      "Epoch 1116/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.0344\n",
      "Epoch 1117/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.0304\n",
      "Epoch 1118/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.0264\n",
      "Epoch 1119/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.0224\n",
      "Epoch 1120/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.0184\n",
      "Epoch 1121/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0144\n",
      "Epoch 1122/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0104\n",
      "Epoch 1123/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0064\n",
      "Epoch 1124/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0024\n",
      "Epoch 1125/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9984\n",
      "Epoch 1126/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.9944\n",
      "Epoch 1127/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9904\n",
      "Epoch 1128/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.9864\n",
      "Epoch 1129/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9824\n",
      "Epoch 1130/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9784\n",
      "Epoch 1131/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9744\n",
      "Epoch 1132/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.9704\n",
      "Epoch 1133/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.9664\n",
      "Epoch 1134/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.9624\n",
      "Epoch 1135/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9584\n",
      "Epoch 1136/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.9544\n",
      "Epoch 1137/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9504\n",
      "Epoch 1138/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9464\n",
      "Epoch 1139/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.9424\n",
      "Epoch 1140/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9384\n",
      "Epoch 1141/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9344\n",
      "Epoch 1142/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.9304\n",
      "Epoch 1143/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9264\n",
      "Epoch 1144/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9224\n",
      "Epoch 1145/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.9184\n",
      "Epoch 1146/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9144\n",
      "Epoch 1147/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9104\n",
      "Epoch 1148/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9064\n",
      "Epoch 1149/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9024\n",
      "Epoch 1150/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8984\n",
      "Epoch 1151/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8944\n",
      "Epoch 1152/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8904\n",
      "Epoch 1153/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8864\n",
      "Epoch 1154/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8824\n",
      "Epoch 1155/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8784\n",
      "Epoch 1156/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8744\n",
      "Epoch 1157/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8704\n",
      "Epoch 1158/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8664\n",
      "Epoch 1159/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.8624\n",
      "Epoch 1160/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.8584\n",
      "Epoch 1161/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8544\n",
      "Epoch 1162/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8504\n",
      "Epoch 1163/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.8464\n",
      "Epoch 1164/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.8424\n",
      "Epoch 1165/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8384\n",
      "Epoch 1166/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8344\n",
      "Epoch 1167/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.8304\n",
      "Epoch 1168/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8264\n",
      "Epoch 1169/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.8224\n",
      "Epoch 1170/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8184\n",
      "Epoch 1171/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8144\n",
      "Epoch 1172/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8104\n",
      "Epoch 1173/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8064\n",
      "Epoch 1174/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.8024\n",
      "Epoch 1175/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.7984\n",
      "Epoch 1176/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7944\n",
      "Epoch 1177/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 1.7904\n",
      "Epoch 1178/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.7864\n",
      "Epoch 1179/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7824\n",
      "Epoch 1180/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.7784\n",
      "Epoch 1181/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7744\n",
      "Epoch 1182/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7704\n",
      "Epoch 1183/2000\n",
      "1/1 [==============================] - 0s 990us/step - loss: 1.7664\n",
      "Epoch 1184/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 1.7624\n",
      "Epoch 1185/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7584\n",
      "Epoch 1186/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7544\n",
      "Epoch 1187/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.7504\n",
      "Epoch 1188/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.7464\n",
      "Epoch 1189/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.7424\n",
      "Epoch 1190/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7384\n",
      "Epoch 1191/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.7344\n",
      "Epoch 1192/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.7304\n",
      "Epoch 1193/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.7264\n",
      "Epoch 1194/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7224\n",
      "Epoch 1195/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.7184\n",
      "Epoch 1196/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.7144\n",
      "Epoch 1197/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7104\n",
      "Epoch 1198/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.7064\n",
      "Epoch 1199/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7024\n",
      "Epoch 1200/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6984\n",
      "Epoch 1201/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6944\n",
      "Epoch 1202/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6904\n",
      "Epoch 1203/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.6864\n",
      "Epoch 1204/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.6824\n",
      "Epoch 1205/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.6784\n",
      "Epoch 1206/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6744\n",
      "Epoch 1207/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6704\n",
      "Epoch 1208/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6664\n",
      "Epoch 1209/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6624\n",
      "Epoch 1210/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6584\n",
      "Epoch 1211/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.6544\n",
      "Epoch 1212/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 1.6504\n",
      "Epoch 1213/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6464\n",
      "Epoch 1214/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.6424\n",
      "Epoch 1215/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6384\n",
      "Epoch 1216/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6344\n",
      "Epoch 1217/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.6304\n",
      "Epoch 1218/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6264\n",
      "Epoch 1219/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6224\n",
      "Epoch 1220/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6184\n",
      "Epoch 1221/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.6144\n",
      "Epoch 1222/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.6104\n",
      "Epoch 1223/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.6064\n",
      "Epoch 1224/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6024\n",
      "Epoch 1225/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5984\n",
      "Epoch 1226/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5944\n",
      "Epoch 1227/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5904\n",
      "Epoch 1228/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5864\n",
      "Epoch 1229/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5824\n",
      "Epoch 1230/2000\n",
      "1/1 [==============================] - 0s 993us/step - loss: 1.5784\n",
      "Epoch 1231/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5744\n",
      "Epoch 1232/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5704\n",
      "Epoch 1233/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5664\n",
      "Epoch 1234/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5624\n",
      "Epoch 1235/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5584\n",
      "Epoch 1236/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.5544\n",
      "Epoch 1237/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5504\n",
      "Epoch 1238/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5464\n",
      "Epoch 1239/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.5424\n",
      "Epoch 1240/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.5384\n",
      "Epoch 1241/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.5344\n",
      "Epoch 1242/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5304\n",
      "Epoch 1243/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.5264\n",
      "Epoch 1244/2000\n",
      "1/1 [==============================] - 0s 991us/step - loss: 1.5224\n",
      "Epoch 1245/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5183\n",
      "Epoch 1246/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.5143\n",
      "Epoch 1247/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.5103\n",
      "Epoch 1248/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.5063\n",
      "Epoch 1249/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.5023\n",
      "Epoch 1250/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4983\n",
      "Epoch 1251/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.4943\n",
      "Epoch 1252/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.4903\n",
      "Epoch 1253/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4863\n",
      "Epoch 1254/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4823\n",
      "Epoch 1255/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4783\n",
      "Epoch 1256/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4743\n",
      "Epoch 1257/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.4703\n",
      "Epoch 1258/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.4663\n",
      "Epoch 1259/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.4623\n",
      "Epoch 1260/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4583\n",
      "Epoch 1261/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.4543\n",
      "Epoch 1262/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4503\n",
      "Epoch 1263/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4463\n",
      "Epoch 1264/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4423\n",
      "Epoch 1265/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.4383\n",
      "Epoch 1266/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4343\n",
      "Epoch 1267/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4303\n",
      "Epoch 1268/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4263\n",
      "Epoch 1269/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.4223\n",
      "Epoch 1270/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.4183\n",
      "Epoch 1271/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4143\n",
      "Epoch 1272/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4103\n",
      "Epoch 1273/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.4063\n",
      "Epoch 1274/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.4023\n",
      "Epoch 1275/2000\n",
      "1/1 [==============================] - 0s 993us/step - loss: 1.3983\n",
      "Epoch 1276/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3943\n",
      "Epoch 1277/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.3903\n",
      "Epoch 1278/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.3863\n",
      "Epoch 1279/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3823\n",
      "Epoch 1280/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3783\n",
      "Epoch 1281/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3743\n",
      "Epoch 1282/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3703\n",
      "Epoch 1283/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3663\n",
      "Epoch 1284/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.3623\n",
      "Epoch 1285/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.3583\n",
      "Epoch 1286/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.3543\n",
      "Epoch 1287/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3503\n",
      "Epoch 1288/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3463\n",
      "Epoch 1289/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3423\n",
      "Epoch 1290/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3383\n",
      "Epoch 1291/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3343\n",
      "Epoch 1292/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.3303\n",
      "Epoch 1293/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3263\n",
      "Epoch 1294/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3223\n",
      "Epoch 1295/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3183\n",
      "Epoch 1296/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3143\n",
      "Epoch 1297/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3103\n",
      "Epoch 1298/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3063\n",
      "Epoch 1299/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3023\n",
      "Epoch 1300/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2983\n",
      "Epoch 1301/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2943\n",
      "Epoch 1302/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.2903\n",
      "Epoch 1303/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2863\n",
      "Epoch 1304/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.2823\n",
      "Epoch 1305/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2783\n",
      "Epoch 1306/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2743\n",
      "Epoch 1307/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.2703\n",
      "Epoch 1308/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.2663\n",
      "Epoch 1309/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2623\n",
      "Epoch 1310/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2583\n",
      "Epoch 1311/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.2543\n",
      "Epoch 1312/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.2503\n",
      "Epoch 1313/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2463\n",
      "Epoch 1314/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.2423\n",
      "Epoch 1315/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2383\n",
      "Epoch 1316/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2343\n",
      "Epoch 1317/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2303\n",
      "Epoch 1318/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2263\n",
      "Epoch 1319/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2223\n",
      "Epoch 1320/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2183\n",
      "Epoch 1321/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2143\n",
      "Epoch 1322/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.2103\n",
      "Epoch 1323/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2063\n",
      "Epoch 1324/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.2023\n",
      "Epoch 1325/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.1983\n",
      "Epoch 1326/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1943\n",
      "Epoch 1327/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.1903\n",
      "Epoch 1328/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1863\n",
      "Epoch 1329/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1823\n",
      "Epoch 1330/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.1783\n",
      "Epoch 1331/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1743\n",
      "Epoch 1332/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1703\n",
      "Epoch 1333/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1663\n",
      "Epoch 1334/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.1623\n",
      "Epoch 1335/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.1583\n",
      "Epoch 1336/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1543\n",
      "Epoch 1337/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1503\n",
      "Epoch 1338/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 1.1463\n",
      "Epoch 1339/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1423\n",
      "Epoch 1340/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.1383\n",
      "Epoch 1341/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1343\n",
      "Epoch 1342/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.1303\n",
      "Epoch 1343/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1263\n",
      "Epoch 1344/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.1223\n",
      "Epoch 1345/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1183\n",
      "Epoch 1346/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1143\n",
      "Epoch 1347/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.1103\n",
      "Epoch 1348/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 1.1063\n",
      "Epoch 1349/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1023\n",
      "Epoch 1350/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.0983\n",
      "Epoch 1351/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.0943\n",
      "Epoch 1352/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0903\n",
      "Epoch 1353/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.0863\n",
      "Epoch 1354/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0823\n",
      "Epoch 1355/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0783\n",
      "Epoch 1356/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0743\n",
      "Epoch 1357/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.0703\n",
      "Epoch 1358/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0663\n",
      "Epoch 1359/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.0623\n",
      "Epoch 1360/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0583\n",
      "Epoch 1361/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.0543\n",
      "Epoch 1362/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0503\n",
      "Epoch 1363/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.0463\n",
      "Epoch 1364/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.0423\n",
      "Epoch 1365/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0383\n",
      "Epoch 1366/2000\n",
      "1/1 [==============================] - 0s 973us/step - loss: 1.0343\n",
      "Epoch 1367/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.0303\n",
      "Epoch 1368/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.0263\n",
      "Epoch 1369/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0223\n",
      "Epoch 1370/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0183\n",
      "Epoch 1371/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.0143\n",
      "Epoch 1372/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.0103\n",
      "Epoch 1373/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0063\n",
      "Epoch 1374/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 1.0023\n",
      "Epoch 1375/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.9983\n",
      "Epoch 1376/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9943\n",
      "Epoch 1377/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.9903\n",
      "Epoch 1378/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.9863\n",
      "Epoch 1379/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.9823\n",
      "Epoch 1380/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9783\n",
      "Epoch 1381/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9743\n",
      "Epoch 1382/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.9703\n",
      "Epoch 1383/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.9663\n",
      "Epoch 1384/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.9629\n",
      "Epoch 1385/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9621\n",
      "Epoch 1386/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.9614\n",
      "Epoch 1387/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.9606\n",
      "Epoch 1388/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9598\n",
      "Epoch 1389/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9590\n",
      "Epoch 1390/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9581\n",
      "Epoch 1391/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9572\n",
      "Epoch 1392/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.9563\n",
      "Epoch 1393/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.9553\n",
      "Epoch 1394/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9543\n",
      "Epoch 1395/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9533\n",
      "Epoch 1396/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.9523\n",
      "Epoch 1397/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.9512\n",
      "Epoch 1398/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9501\n",
      "Epoch 1399/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9490\n",
      "Epoch 1400/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.9478\n",
      "Epoch 1401/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9466\n",
      "Epoch 1402/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.9454\n",
      "Epoch 1403/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9442\n",
      "Epoch 1404/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.9430\n",
      "Epoch 1405/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9417\n",
      "Epoch 1406/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9404\n",
      "Epoch 1407/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9390\n",
      "Epoch 1408/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.9377\n",
      "Epoch 1409/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.9363\n",
      "Epoch 1410/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.9350\n",
      "Epoch 1411/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.9336\n",
      "Epoch 1412/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.9322\n",
      "Epoch 1413/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9307\n",
      "Epoch 1414/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9293\n",
      "Epoch 1415/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.9278\n",
      "Epoch 1416/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9264\n",
      "Epoch 1417/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.9249\n",
      "Epoch 1418/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9234\n",
      "Epoch 1419/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.9219\n",
      "Epoch 1420/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9204\n",
      "Epoch 1421/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.9189\n",
      "Epoch 1422/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9173\n",
      "Epoch 1423/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9158\n",
      "Epoch 1424/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9143\n",
      "Epoch 1425/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.9127\n",
      "Epoch 1426/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.9112\n",
      "Epoch 1427/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.9096\n",
      "Epoch 1428/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.9081\n",
      "Epoch 1429/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.9065\n",
      "Epoch 1430/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9049\n",
      "Epoch 1431/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.9034\n",
      "Epoch 1432/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9018\n",
      "Epoch 1433/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9002\n",
      "Epoch 1434/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8986\n",
      "Epoch 1435/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.8971\n",
      "Epoch 1436/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8955\n",
      "Epoch 1437/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8939\n",
      "Epoch 1438/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8923\n",
      "Epoch 1439/2000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8907\n",
      "Epoch 1440/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8891\n",
      "Epoch 1441/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8875\n",
      "Epoch 1442/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8860\n",
      "Epoch 1443/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8844\n",
      "Epoch 1444/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8828\n",
      "Epoch 1445/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8812\n",
      "Epoch 1446/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8796\n",
      "Epoch 1447/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8780\n",
      "Epoch 1448/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8764\n",
      "Epoch 1449/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8748\n",
      "Epoch 1450/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8732\n",
      "Epoch 1451/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8716\n",
      "Epoch 1452/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8700\n",
      "Epoch 1453/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8684\n",
      "Epoch 1454/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8668\n",
      "Epoch 1455/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8652\n",
      "Epoch 1456/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8636\n",
      "Epoch 1457/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.8620\n",
      "Epoch 1458/2000\n",
      "1/1 [==============================] - 0s 991us/step - loss: 0.8604\n",
      "Epoch 1459/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8588\n",
      "Epoch 1460/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8572\n",
      "Epoch 1461/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.8556\n",
      "Epoch 1462/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8540\n",
      "Epoch 1463/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8524\n",
      "Epoch 1464/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8508\n",
      "Epoch 1465/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.8492\n",
      "Epoch 1466/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.8476\n",
      "Epoch 1467/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.8460\n",
      "Epoch 1468/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8444\n",
      "Epoch 1469/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8428\n",
      "Epoch 1470/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8412\n",
      "Epoch 1471/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.8396\n",
      "Epoch 1472/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8380\n",
      "Epoch 1473/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8364\n",
      "Epoch 1474/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8348\n",
      "Epoch 1475/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8332\n",
      "Epoch 1476/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8316\n",
      "Epoch 1477/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.8300\n",
      "Epoch 1478/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8284\n",
      "Epoch 1479/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8268\n",
      "Epoch 1480/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8252\n",
      "Epoch 1481/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.8236\n",
      "Epoch 1482/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8220\n",
      "Epoch 1483/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8204\n",
      "Epoch 1484/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8188\n",
      "Epoch 1485/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.8172\n",
      "Epoch 1486/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8156\n",
      "Epoch 1487/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8140\n",
      "Epoch 1488/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8124\n",
      "Epoch 1489/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8108\n",
      "Epoch 1490/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8092\n",
      "Epoch 1491/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8076\n",
      "Epoch 1492/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8072\n",
      "Epoch 1493/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8068\n",
      "Epoch 1494/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8069\n",
      "Epoch 1495/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8067\n",
      "Epoch 1496/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8063\n",
      "Epoch 1497/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8060\n",
      "Epoch 1498/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8062\n",
      "Epoch 1499/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8057\n",
      "Epoch 1500/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8052\n",
      "Epoch 1501/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8056\n",
      "Epoch 1502/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8052\n",
      "Epoch 1503/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8046\n",
      "Epoch 1504/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8046\n",
      "Epoch 1505/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8046\n",
      "Epoch 1506/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8040\n",
      "Epoch 1507/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8035\n",
      "Epoch 1508/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8039\n",
      "Epoch 1509/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8034\n",
      "Epoch 1510/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8028\n",
      "Epoch 1511/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8028\n",
      "Epoch 1512/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8027\n",
      "Epoch 1513/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8021\n",
      "Epoch 1514/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8017\n",
      "Epoch 1515/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.8020\n",
      "Epoch 1516/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8015\n",
      "Epoch 1517/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8009\n",
      "Epoch 1518/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8009\n",
      "Epoch 1519/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8008\n",
      "Epoch 1520/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8002\n",
      "Epoch 1521/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7997\n",
      "Epoch 1522/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.8001\n",
      "Epoch 1523/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7995\n",
      "Epoch 1524/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7989\n",
      "Epoch 1525/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7989\n",
      "Epoch 1526/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7987\n",
      "Epoch 1527/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7982\n",
      "Epoch 1528/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7977\n",
      "Epoch 1529/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7980\n",
      "Epoch 1530/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7974\n",
      "Epoch 1531/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7968\n",
      "Epoch 1532/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7969\n",
      "Epoch 1533/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7967\n",
      "Epoch 1534/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7961\n",
      "Epoch 1535/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7956\n",
      "Epoch 1536/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7960\n",
      "Epoch 1537/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7954\n",
      "Epoch 1538/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7948\n",
      "Epoch 1539/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7948\n",
      "Epoch 1540/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7947\n",
      "Epoch 1541/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7941\n",
      "Epoch 1542/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7934\n",
      "Epoch 1543/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7939\n",
      "Epoch 1544/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7933\n",
      "Epoch 1545/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7927\n",
      "Epoch 1546/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7926\n",
      "Epoch 1547/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7926\n",
      "Epoch 1548/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7920\n",
      "Epoch 1549/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7914\n",
      "Epoch 1550/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.7918\n",
      "Epoch 1551/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7912\n",
      "Epoch 1552/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7906\n",
      "Epoch 1553/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7905\n",
      "Epoch 1554/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7905\n",
      "Epoch 1555/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7899\n",
      "Epoch 1556/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7893\n",
      "Epoch 1557/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7897\n",
      "Epoch 1558/2000\n",
      "1/1 [==============================] - 0s 994us/step - loss: 0.7892\n",
      "Epoch 1559/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7886\n",
      "Epoch 1560/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7883\n",
      "Epoch 1561/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7884\n",
      "Epoch 1562/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7878\n",
      "Epoch 1563/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7872\n",
      "Epoch 1564/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7875\n",
      "Epoch 1565/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7871\n",
      "Epoch 1566/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7865\n",
      "Epoch 1567/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7862\n",
      "Epoch 1568/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7864\n",
      "Epoch 1569/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7858\n",
      "Epoch 1570/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7852\n",
      "Epoch 1571/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7854\n",
      "Epoch 1572/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7850\n",
      "Epoch 1573/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7844\n",
      "Epoch 1574/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7841\n",
      "Epoch 1575/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7843\n",
      "Epoch 1576/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7837\n",
      "Epoch 1577/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7831\n",
      "Epoch 1578/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7832\n",
      "Epoch 1579/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7829\n",
      "Epoch 1580/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7823\n",
      "Epoch 1581/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7819\n",
      "Epoch 1582/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7822\n",
      "Epoch 1583/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7816\n",
      "Epoch 1584/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7810\n",
      "Epoch 1585/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7811\n",
      "Epoch 1586/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7809\n",
      "Epoch 1587/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7803\n",
      "Epoch 1588/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7798\n",
      "Epoch 1589/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7801\n",
      "Epoch 1590/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7795\n",
      "Epoch 1591/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7789\n",
      "Epoch 1592/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7790\n",
      "Epoch 1593/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7788\n",
      "Epoch 1594/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7782\n",
      "Epoch 1595/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7777\n",
      "Epoch 1596/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7781\n",
      "Epoch 1597/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7775\n",
      "Epoch 1598/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7769\n",
      "Epoch 1599/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7768\n",
      "Epoch 1600/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7767\n",
      "Epoch 1601/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7761\n",
      "Epoch 1602/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7755\n",
      "Epoch 1603/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7760\n",
      "Epoch 1604/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7754\n",
      "Epoch 1605/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7748\n",
      "Epoch 1606/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7747\n",
      "Epoch 1607/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7747\n",
      "Epoch 1608/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7741\n",
      "Epoch 1609/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7735\n",
      "Epoch 1610/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7739\n",
      "Epoch 1611/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7733\n",
      "Epoch 1612/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7727\n",
      "Epoch 1613/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7725\n",
      "Epoch 1614/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7726\n",
      "Epoch 1615/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7720\n",
      "Epoch 1616/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7714\n",
      "Epoch 1617/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7717\n",
      "Epoch 1618/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7712\n",
      "Epoch 1619/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7706\n",
      "Epoch 1620/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7704\n",
      "Epoch 1621/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7705\n",
      "Epoch 1622/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7699\n",
      "Epoch 1623/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7693\n",
      "Epoch 1624/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7696\n",
      "Epoch 1625/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7692\n",
      "Epoch 1626/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7686\n",
      "Epoch 1627/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7683\n",
      "Epoch 1628/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7684\n",
      "Epoch 1629/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7678\n",
      "Epoch 1630/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7672\n",
      "Epoch 1631/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7674\n",
      "Epoch 1632/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7671\n",
      "Epoch 1633/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7665\n",
      "Epoch 1634/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7661\n",
      "Epoch 1635/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7664\n",
      "Epoch 1636/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7658\n",
      "Epoch 1637/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7652\n",
      "Epoch 1638/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7653\n",
      "Epoch 1639/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7650\n",
      "Epoch 1640/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7644\n",
      "Epoch 1641/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7640\n",
      "Epoch 1642/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7643\n",
      "Epoch 1643/2000\n",
      "1/1 [==============================] - 0s 993us/step - loss: 0.7637\n",
      "Epoch 1644/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7631\n",
      "Epoch 1645/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7632\n",
      "Epoch 1646/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7629\n",
      "Epoch 1647/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7623\n",
      "Epoch 1648/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7618\n",
      "Epoch 1649/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7622\n",
      "Epoch 1650/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7616\n",
      "Epoch 1651/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7610\n",
      "Epoch 1652/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7610\n",
      "Epoch 1653/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7609\n",
      "Epoch 1654/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7603\n",
      "Epoch 1655/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7597\n",
      "Epoch 1656/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7601\n",
      "Epoch 1657/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7595\n",
      "Epoch 1658/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7589\n",
      "Epoch 1659/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7589\n",
      "Epoch 1660/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7588\n",
      "Epoch 1661/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7582\n",
      "Epoch 1662/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7576\n",
      "Epoch 1663/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7581\n",
      "Epoch 1664/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7575\n",
      "Epoch 1665/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7568\n",
      "Epoch 1666/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7567\n",
      "Epoch 1667/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7567\n",
      "Epoch 1668/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7561\n",
      "Epoch 1669/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7555\n",
      "Epoch 1670/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7559\n",
      "Epoch 1671/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.7554\n",
      "Epoch 1672/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7548\n",
      "Epoch 1673/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7546\n",
      "Epoch 1674/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7546\n",
      "Epoch 1675/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7540\n",
      "Epoch 1676/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7534\n",
      "Epoch 1677/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7538\n",
      "Epoch 1678/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7533\n",
      "Epoch 1679/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7527\n",
      "Epoch 1680/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7524\n",
      "Epoch 1681/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7526\n",
      "Epoch 1682/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7520\n",
      "Epoch 1683/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7514\n",
      "Epoch 1684/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7516\n",
      "Epoch 1685/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7512\n",
      "Epoch 1686/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7506\n",
      "Epoch 1687/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7503\n",
      "Epoch 1688/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.7505\n",
      "Epoch 1689/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7499\n",
      "Epoch 1690/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7493\n",
      "Epoch 1691/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1692/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7492\n",
      "Epoch 1693/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7486\n",
      "Epoch 1694/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7482\n",
      "Epoch 1695/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7484\n",
      "Epoch 1696/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7478\n",
      "Epoch 1697/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7472\n",
      "Epoch 1698/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7474\n",
      "Epoch 1699/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7471\n",
      "Epoch 1700/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7465\n",
      "Epoch 1701/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7460\n",
      "Epoch 1702/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7463\n",
      "Epoch 1703/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7458\n",
      "Epoch 1704/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7451\n",
      "Epoch 1705/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7452\n",
      "Epoch 1706/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7450\n",
      "Epoch 1707/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7444\n",
      "Epoch 1708/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7439\n",
      "Epoch 1709/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7443\n",
      "Epoch 1710/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7437\n",
      "Epoch 1711/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7431\n",
      "Epoch 1712/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7431\n",
      "Epoch 1713/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7429\n",
      "Epoch 1714/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7423\n",
      "Epoch 1715/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7418\n",
      "Epoch 1716/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.7422\n",
      "Epoch 1717/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7416\n",
      "Epoch 1718/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7410\n",
      "Epoch 1719/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7410\n",
      "Epoch 1720/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7409\n",
      "Epoch 1721/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7403\n",
      "Epoch 1722/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7397\n",
      "Epoch 1723/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7401\n",
      "Epoch 1724/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7395\n",
      "Epoch 1725/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7389\n",
      "Epoch 1726/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7388\n",
      "Epoch 1727/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.7388\n",
      "Epoch 1728/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7382\n",
      "Epoch 1729/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.7376\n",
      "Epoch 1730/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7380\n",
      "Epoch 1731/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7374\n",
      "Epoch 1732/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7368\n",
      "Epoch 1733/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7366\n",
      "Epoch 1734/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7367\n",
      "Epoch 1735/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7361\n",
      "Epoch 1736/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7355\n",
      "Epoch 1737/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7358\n",
      "Epoch 1738/2000\n",
      "1/1 [==============================] - 0s 994us/step - loss: 0.7354\n",
      "Epoch 1739/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7348\n",
      "Epoch 1740/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7345\n",
      "Epoch 1741/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7346\n",
      "Epoch 1742/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7340\n",
      "Epoch 1743/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7334\n",
      "Epoch 1744/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7337\n",
      "Epoch 1745/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7333\n",
      "Epoch 1746/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7327\n",
      "Epoch 1747/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7324\n",
      "Epoch 1748/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7326\n",
      "Epoch 1749/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7320\n",
      "Epoch 1750/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.7314\n",
      "Epoch 1751/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7316\n",
      "Epoch 1752/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7312\n",
      "Epoch 1753/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7306\n",
      "Epoch 1754/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7302\n",
      "Epoch 1755/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7305\n",
      "Epoch 1756/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7299\n",
      "Epoch 1757/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7293\n",
      "Epoch 1758/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7294\n",
      "Epoch 1759/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7291\n",
      "Epoch 1760/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7286\n",
      "Epoch 1761/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7281\n",
      "Epoch 1762/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7284\n",
      "Epoch 1763/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7278\n",
      "Epoch 1764/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7272\n",
      "Epoch 1765/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7273\n",
      "Epoch 1766/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7271\n",
      "Epoch 1767/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7265\n",
      "Epoch 1768/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7260\n",
      "Epoch 1769/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7263\n",
      "Epoch 1770/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7258\n",
      "Epoch 1771/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7251\n",
      "Epoch 1772/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7252\n",
      "Epoch 1773/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7250\n",
      "Epoch 1774/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7244\n",
      "Epoch 1775/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7238\n",
      "Epoch 1776/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7243\n",
      "Epoch 1777/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7237\n",
      "Epoch 1778/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7231\n",
      "Epoch 1779/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7230\n",
      "Epoch 1780/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7229\n",
      "Epoch 1781/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7223\n",
      "Epoch 1782/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7217\n",
      "Epoch 1783/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7222\n",
      "Epoch 1784/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7216\n",
      "Epoch 1785/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7210\n",
      "Epoch 1786/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7209\n",
      "Epoch 1787/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7209\n",
      "Epoch 1788/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7203\n",
      "Epoch 1789/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7196\n",
      "Epoch 1790/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7200\n",
      "Epoch 1791/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7195\n",
      "Epoch 1792/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7189\n",
      "Epoch 1793/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7187\n",
      "Epoch 1794/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7188\n",
      "Epoch 1795/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7182\n",
      "Epoch 1796/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7176\n",
      "Epoch 1797/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7179\n",
      "Epoch 1798/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.7174\n",
      "Epoch 1799/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7168\n",
      "Epoch 1800/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7166\n",
      "Epoch 1801/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7167\n",
      "Epoch 1802/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7161\n",
      "Epoch 1803/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7155\n",
      "Epoch 1804/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7157\n",
      "Epoch 1805/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7154\n",
      "Epoch 1806/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7148\n",
      "Epoch 1807/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7144\n",
      "Epoch 1808/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7146\n",
      "Epoch 1809/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7140\n",
      "Epoch 1810/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7134\n",
      "Epoch 1811/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7136\n",
      "Epoch 1812/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7133\n",
      "Epoch 1813/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7127\n",
      "Epoch 1814/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7123\n",
      "Epoch 1815/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7126\n",
      "Epoch 1816/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7120\n",
      "Epoch 1817/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7114\n",
      "Epoch 1818/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7115\n",
      "Epoch 1819/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7112\n",
      "Epoch 1820/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7106\n",
      "Epoch 1821/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7102\n",
      "Epoch 1822/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7105\n",
      "Epoch 1823/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7099\n",
      "Epoch 1824/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7093\n",
      "Epoch 1825/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7093\n",
      "Epoch 1826/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7091\n",
      "Epoch 1827/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7086\n",
      "Epoch 1828/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7080\n",
      "Epoch 1829/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7084\n",
      "Epoch 1830/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7078\n",
      "Epoch 1831/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7072\n",
      "Epoch 1832/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7072\n",
      "Epoch 1833/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7071\n",
      "Epoch 1834/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7065\n",
      "Epoch 1835/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7059\n",
      "Epoch 1836/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7063\n",
      "Epoch 1837/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7058\n",
      "Epoch 1838/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7051\n",
      "Epoch 1839/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7051\n",
      "Epoch 1840/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7050\n",
      "Epoch 1841/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.7044\n",
      "Epoch 1842/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7038\n",
      "Epoch 1843/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7042\n",
      "Epoch 1844/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7037\n",
      "Epoch 1845/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7031\n",
      "Epoch 1846/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7029\n",
      "Epoch 1847/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7029\n",
      "Epoch 1848/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7023\n",
      "Epoch 1849/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7017\n",
      "Epoch 1850/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.7021\n",
      "Epoch 1851/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7016\n",
      "Epoch 1852/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7010\n",
      "Epoch 1853/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7008\n",
      "Epoch 1854/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7008\n",
      "Epoch 1855/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7002\n",
      "Epoch 1856/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6996\n",
      "Epoch 1857/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6999\n",
      "Epoch 1858/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6995\n",
      "Epoch 1859/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6989\n",
      "Epoch 1860/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6986\n",
      "Epoch 1861/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6988\n",
      "Epoch 1862/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6982\n",
      "Epoch 1863/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6976\n",
      "Epoch 1864/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6978\n",
      "Epoch 1865/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6974\n",
      "Epoch 1866/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6968\n",
      "Epoch 1867/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6965\n",
      "Epoch 1868/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6967\n",
      "Epoch 1869/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6961\n",
      "Epoch 1870/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6955\n",
      "Epoch 1871/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6957\n",
      "Epoch 1872/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6954\n",
      "Epoch 1873/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6948\n",
      "Epoch 1874/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6944\n",
      "Epoch 1875/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6946\n",
      "Epoch 1876/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6940\n",
      "Epoch 1877/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6934\n",
      "Epoch 1878/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.6935\n",
      "Epoch 1879/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6933\n",
      "Epoch 1880/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6927\n",
      "Epoch 1881/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6922\n",
      "Epoch 1882/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6926\n",
      "Epoch 1883/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6920\n",
      "Epoch 1884/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6914\n",
      "Epoch 1885/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6914\n",
      "Epoch 1886/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6912\n",
      "Epoch 1887/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6906\n",
      "Epoch 1888/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1889/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6905\n",
      "Epoch 1890/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6899\n",
      "Epoch 1891/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6893\n",
      "Epoch 1892/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6893\n",
      "Epoch 1893/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6891\n",
      "Epoch 1894/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6885\n",
      "Epoch 1895/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6880\n",
      "Epoch 1896/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6884\n",
      "Epoch 1897/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6878\n",
      "Epoch 1898/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6872\n",
      "Epoch 1899/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6871\n",
      "Epoch 1900/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6871\n",
      "Epoch 1901/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6865\n",
      "Epoch 1902/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6859\n",
      "Epoch 1903/2000\n",
      "1/1 [==============================] - 0s 984us/step - loss: 0.6863\n",
      "Epoch 1904/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6857\n",
      "Epoch 1905/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6851\n",
      "Epoch 1906/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6850\n",
      "Epoch 1907/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6850\n",
      "Epoch 1908/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6844\n",
      "Epoch 1909/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6838\n",
      "Epoch 1910/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6841\n",
      "Epoch 1911/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6836\n",
      "Epoch 1912/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6830\n",
      "Epoch 1913/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6828\n",
      "Epoch 1914/2000\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.6829\n",
      "Epoch 1915/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.6823\n",
      "Epoch 1916/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6817\n",
      "Epoch 1917/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6820\n",
      "Epoch 1918/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6816\n",
      "Epoch 1919/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6810\n",
      "Epoch 1920/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6807\n",
      "Epoch 1921/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6808\n",
      "Epoch 1922/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6802\n",
      "Epoch 1923/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6796\n",
      "Epoch 1924/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6799\n",
      "Epoch 1925/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6795\n",
      "Epoch 1926/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6789\n",
      "Epoch 1927/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6785\n",
      "Epoch 1928/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6788\n",
      "Epoch 1929/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6782\n",
      "Epoch 1930/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6776\n",
      "Epoch 1931/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6777\n",
      "Epoch 1932/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6774\n",
      "Epoch 1933/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6768\n",
      "Epoch 1934/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6764\n",
      "Epoch 1935/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6767\n",
      "Epoch 1936/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6761\n",
      "Epoch 1937/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6755\n",
      "Epoch 1938/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6756\n",
      "Epoch 1939/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6754\n",
      "Epoch 1940/2000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.6748\n",
      "Epoch 1941/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6743\n",
      "Epoch 1942/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6746\n",
      "Epoch 1943/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6740\n",
      "Epoch 1944/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6734\n",
      "Epoch 1945/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6735\n",
      "Epoch 1946/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6733\n",
      "Epoch 1947/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6727\n",
      "Epoch 1948/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6721\n",
      "Epoch 1949/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6726\n",
      "Epoch 1950/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6720\n",
      "Epoch 1951/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6714\n",
      "Epoch 1952/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6713\n",
      "Epoch 1953/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6712\n",
      "Epoch 1954/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6706\n",
      "Epoch 1955/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6700\n",
      "Epoch 1956/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6705\n",
      "Epoch 1957/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6699\n",
      "Epoch 1958/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6693\n",
      "Epoch 1959/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6692\n",
      "Epoch 1960/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6691\n",
      "Epoch 1961/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6685\n",
      "Epoch 1962/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6679\n",
      "Epoch 1963/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6684\n",
      "Epoch 1964/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6678\n",
      "Epoch 1965/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6672\n",
      "Epoch 1966/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6670\n",
      "Epoch 1967/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6671\n",
      "Epoch 1968/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6665\n",
      "Epoch 1969/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6659\n",
      "Epoch 1970/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6662\n",
      "Epoch 1971/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6657\n",
      "Epoch 1972/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6651\n",
      "Epoch 1973/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6649\n",
      "Epoch 1974/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6650\n",
      "Epoch 1975/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6644\n",
      "Epoch 1976/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6638\n",
      "Epoch 1977/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6641\n",
      "Epoch 1978/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6636\n",
      "Epoch 1979/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6630\n",
      "Epoch 1980/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6627\n",
      "Epoch 1981/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6629\n",
      "Epoch 1982/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6623\n",
      "Epoch 1983/2000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6617\n",
      "Epoch 1984/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6619\n",
      "Epoch 1985/2000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6616\n",
      "Epoch 1986/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6610\n",
      "Epoch 1987/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6606\n",
      "Epoch 1988/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6608\n",
      "Epoch 1989/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6602\n",
      "Epoch 1990/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6596\n",
      "Epoch 1991/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6598\n",
      "Epoch 1992/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6595\n",
      "Epoch 1993/2000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6589\n",
      "Epoch 1994/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6585\n",
      "Epoch 1995/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6588\n",
      "Epoch 1996/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6582\n",
      "Epoch 1997/2000\n",
      "1/1 [==============================] - 0s 988us/step - loss: 0.6576\n",
      "Epoch 1998/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6577\n",
      "Epoch 1999/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6574\n",
      "Epoch 2000/2000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a85bbf4188>"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x,y],y_fake, epochs = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "68e6f607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.5936537],\n",
       "       [5.062336 ],\n",
       "       [6.5310183],\n",
       "       [7.9997005],\n",
       "       [9.468383 ]], dtype=float32)"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([x, y_fake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "c9577163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_47/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[1.7529033]], dtype=float32)>,\n",
       " <tf.Variable 'dense_47/bias:0' shape=(1,) dtype=float32, numpy=array([0.20914188], dtype=float32)>]"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4fdf142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 2, 3, 4, 5])>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = LossLayer()\n",
    "loss(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "b1db3b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.651556 ],\n",
       "        [ 7.3031116],\n",
       "        [ 8.954667 ],\n",
       "        [10.606223 ],\n",
       "        [12.257779 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "57bac850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-391-b70f3f3e983c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Compute the loss value for this minibatch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Use the gradient tape to automatically retrieve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(x, axis, keepdims)\u001b[0m\n\u001b[0;32m   2259\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmean\u001b[0m \u001b[0mof\u001b[0m \u001b[0melements\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2260\u001b[0m   \"\"\"\n\u001b[1;32m-> 2261\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2262\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2263\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "true_box =  tf.expand_dims(bbox_data[5], 0)\n",
    "ost = tf.expand_dims(output_scores_test[5], 0)\n",
    "odt = tf.expand_dims(output_deltas_test[5], 0)\n",
    "my_loss_fn(true_box, ost, odt, tf.cast(generate_bbox_coords(), dtype = tf.float32))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            result = model([x, y], training = True)  # Logits for this minibatch\n",
    "\n",
    "        # Compute the loss value for this minibatch.\n",
    "            loss_value = my_loss_fn(true_box, ost, odt, tf.cast(generate_bbox_coords(), dtype = tf.float32))\n",
    "            print(loss_value)\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            #print(model.trainable_weights)\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "cc2117d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_21/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-0.6515558]], dtype=float32)>,\n",
       " <tf.Variable 'dense_21/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "7dfb612e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_21/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-0.64155585]], dtype=float32)>,\n",
       " <tf.Variable 'dense_21/bias:0' shape=(1,) dtype=float32, numpy=array([0.01000006], dtype=float32)>]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "1e965b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=16724.955>"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_sum(model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "8feef6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[15.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)>]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "ab3a335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_x = Input(shape=(1))\n",
    "dense = Dense(1)(input_x)\n",
    "\n",
    "model = Model(inputs=[input_x], outputs=[dense])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "b5aa9fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.array([1])\n",
    "ytest = np.array([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "a81785e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 8.3848 - accuracy: 0.0000e+00\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.3094 - accuracy: 0.0000e+00\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.2550 - accuracy: 0.0000e+00\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.2096 - accuracy: 0.0000e+00\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1695 - accuracy: 0.0000e+00\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.1329 - accuracy: 0.0000e+00\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.0988 - accuracy: 0.0000e+00\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.0665 - accuracy: 0.0000e+00\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.0357 - accuracy: 0.0000e+00\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.0061 - accuracy: 0.0000e+00\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.9775 - accuracy: 0.0000e+00\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.9496 - accuracy: 0.0000e+00\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.9224 - accuracy: 0.0000e+00\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.8958 - accuracy: 0.0000e+00\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8697 - accuracy: 0.0000e+00\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8441 - accuracy: 0.0000e+00\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8188 - accuracy: 0.0000e+00\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.7938 - accuracy: 0.0000e+00\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 995us/step - loss: 7.7691 - accuracy: 0.0000e+00\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.7447 - accuracy: 0.0000e+00\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.7206 - accuracy: 0.0000e+00\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6966 - accuracy: 0.0000e+00\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6728 - accuracy: 0.0000e+00\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 7.6492 - accuracy: 0.0000e+00\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 7.6258 - accuracy: 0.0000e+00\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.6025 - accuracy: 0.0000e+00\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.5793 - accuracy: 0.0000e+00\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5563 - accuracy: 0.0000e+00\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5333 - accuracy: 0.0000e+00\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5105 - accuracy: 0.0000e+00\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4877 - accuracy: 0.0000e+00\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.4651 - accuracy: 0.0000e+00\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.4425 - accuracy: 0.0000e+00\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4200 - accuracy: 0.0000e+00\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3976 - accuracy: 0.0000e+00\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3753 - accuracy: 0.0000e+00\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3530 - accuracy: 0.0000e+00\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3308 - accuracy: 0.0000e+00\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3086 - accuracy: 0.0000e+00\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.2865 - accuracy: 0.0000e+00\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2645 - accuracy: 0.0000e+00\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.2425 - accuracy: 0.0000e+00\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 7.2206 - accuracy: 0.0000e+00\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.1987 - accuracy: 0.0000e+00\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 7.1768 - accuracy: 0.0000e+00\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1550 - accuracy: 0.0000e+00\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1333 - accuracy: 0.0000e+00\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 7.1116 - accuracy: 0.0000e+00\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0899 - accuracy: 0.0000e+00\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0683 - accuracy: 0.0000e+00\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0467 - accuracy: 0.0000e+00\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0252 - accuracy: 0.0000e+00\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0037 - accuracy: 0.0000e+00\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.9823 - accuracy: 0.0000e+00\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.9609 - accuracy: 0.0000e+00\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.9395 - accuracy: 0.0000e+00\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.9182 - accuracy: 0.0000e+00\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.8969 - accuracy: 0.0000e+00\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.8756 - accuracy: 0.0000e+00\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.8544 - accuracy: 0.0000e+00\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 994us/step - loss: 6.8332 - accuracy: 0.0000e+00\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8121 - accuracy: 0.0000e+00\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.7910 - accuracy: 0.0000e+00\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.7699 - accuracy: 0.0000e+00\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7489 - accuracy: 0.0000e+00\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7279 - accuracy: 0.0000e+00\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7069 - accuracy: 0.0000e+00\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6860 - accuracy: 0.0000e+00\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6651 - accuracy: 0.0000e+00\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6443 - accuracy: 0.0000e+00\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.6235 - accuracy: 0.0000e+00\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.6027 - accuracy: 0.0000e+00\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.5820 - accuracy: 0.0000e+00\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5613 - accuracy: 0.0000e+00\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5406 - accuracy: 0.0000e+00\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.5200 - accuracy: 0.0000e+00\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4994 - accuracy: 0.0000e+00\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4788 - accuracy: 0.0000e+00\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4583 - accuracy: 0.0000e+00\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4378 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4174 - accuracy: 0.0000e+00\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3970 - accuracy: 0.0000e+00\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3766 - accuracy: 0.0000e+00\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3562 - accuracy: 0.0000e+00\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.3359 - accuracy: 0.0000e+00\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3157 - accuracy: 0.0000e+00\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.2954 - accuracy: 0.0000e+00\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2752 - accuracy: 0.0000e+00\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2551 - accuracy: 0.0000e+00\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.2349 - accuracy: 0.0000e+00\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2148 - accuracy: 0.0000e+00\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1948 - accuracy: 0.0000e+00\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1747 - accuracy: 0.0000e+00\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1548 - accuracy: 0.0000e+00\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1348 - accuracy: 0.0000e+00\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.1149 - accuracy: 0.0000e+00\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.0950 - accuracy: 0.0000e+00\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.0752 - accuracy: 0.0000e+00\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.0554 - accuracy: 0.0000e+00\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0356 - accuracy: 0.0000e+00\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0158 - accuracy: 0.0000e+00\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9961 - accuracy: 0.0000e+00\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9765 - accuracy: 0.0000e+00\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9568 - accuracy: 0.0000e+00\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9372 - accuracy: 0.0000e+00\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.9177 - accuracy: 0.0000e+00\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.8982 - accuracy: 0.0000e+00\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8787 - accuracy: 0.0000e+00\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8592 - accuracy: 0.0000e+00\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8398 - accuracy: 0.0000e+00\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8204 - accuracy: 0.0000e+00\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.8011 - accuracy: 0.0000e+00\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.7817 - accuracy: 0.0000e+00\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.7625 - accuracy: 0.0000e+00\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7432 - accuracy: 0.0000e+00\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7240 - accuracy: 0.0000e+00\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7048 - accuracy: 0.0000e+00\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.6857 - accuracy: 0.0000e+00\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6666 - accuracy: 0.0000e+00\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6475 - accuracy: 0.0000e+00\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6285 - accuracy: 0.0000e+00\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6095 - accuracy: 0.0000e+00\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.5905 - accuracy: 0.0000e+00\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 5.5716 - accuracy: 0.0000e+00\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5527 - accuracy: 0.0000e+00\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.5339 - accuracy: 0.0000e+00\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.5151 - accuracy: 0.0000e+00\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4963 - accuracy: 0.0000e+00\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.4775 - accuracy: 0.0000e+00\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4588 - accuracy: 0.0000e+00\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4401 - accuracy: 0.0000e+00\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4215 - accuracy: 0.0000e+00\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4029 - accuracy: 0.0000e+00\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3843 - accuracy: 0.0000e+00\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.3658 - accuracy: 0.0000e+00\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3473 - accuracy: 0.0000e+00\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3288 - accuracy: 0.0000e+00\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.3104 - accuracy: 0.0000e+00\n",
      "Epoch 139/400\n",
      "1/1 [==============================] - 0s 995us/step - loss: 5.2920 - accuracy: 0.0000e+00\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2736 - accuracy: 0.0000e+00\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2553 - accuracy: 0.0000e+00\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2370 - accuracy: 0.0000e+00\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2188 - accuracy: 0.0000e+00\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2006 - accuracy: 0.0000e+00\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.1824 - accuracy: 0.0000e+00\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1642 - accuracy: 0.0000e+00\n",
      "Epoch 147/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1461 - accuracy: 0.0000e+00\n",
      "Epoch 148/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1281 - accuracy: 0.0000e+00\n",
      "Epoch 149/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1100 - accuracy: 0.0000e+00\n",
      "Epoch 150/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0920 - accuracy: 0.0000e+00\n",
      "Epoch 151/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.0740 - accuracy: 0.0000e+00\n",
      "Epoch 152/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.0561 - accuracy: 0.0000e+00\n",
      "Epoch 153/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0382 - accuracy: 0.0000e+00\n",
      "Epoch 154/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0203 - accuracy: 0.0000e+00\n",
      "Epoch 155/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0025 - accuracy: 0.0000e+00\n",
      "Epoch 156/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9847 - accuracy: 0.0000e+00\n",
      "Epoch 157/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9670 - accuracy: 0.0000e+00\n",
      "Epoch 158/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.9492 - accuracy: 0.0000e+00\n",
      "Epoch 159/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9316 - accuracy: 0.0000e+00\n",
      "Epoch 160/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9139 - accuracy: 0.0000e+00\n",
      "Epoch 161/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8963 - accuracy: 0.0000e+00\n",
      "Epoch 162/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.8787 - accuracy: 0.0000e+00\n",
      "Epoch 163/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8612 - accuracy: 0.0000e+00\n",
      "Epoch 164/400\n",
      "1/1 [==============================] - 0s 993us/step - loss: 4.8437 - accuracy: 0.0000e+00\n",
      "Epoch 165/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.8262 - accuracy: 0.0000e+00\n",
      "Epoch 166/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8087 - accuracy: 0.0000e+00\n",
      "Epoch 167/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7913 - accuracy: 0.0000e+00\n",
      "Epoch 168/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7740 - accuracy: 0.0000e+00\n",
      "Epoch 169/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7566 - accuracy: 0.0000e+00\n",
      "Epoch 170/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7393 - accuracy: 0.0000e+00\n",
      "Epoch 171/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.7221 - accuracy: 0.0000e+00\n",
      "Epoch 172/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7049 - accuracy: 0.0000e+00\n",
      "Epoch 173/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6877 - accuracy: 0.0000e+00\n",
      "Epoch 174/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6705 - accuracy: 0.0000e+00\n",
      "Epoch 175/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.6534 - accuracy: 0.0000e+00\n",
      "Epoch 176/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6363 - accuracy: 0.0000e+00\n",
      "Epoch 177/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6192 - accuracy: 0.0000e+00\n",
      "Epoch 178/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6022 - accuracy: 0.0000e+00\n",
      "Epoch 179/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5853 - accuracy: 0.0000e+00\n",
      "Epoch 180/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.5683 - accuracy: 0.0000e+00\n",
      "Epoch 181/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5514 - accuracy: 0.0000e+00\n",
      "Epoch 182/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.5345 - accuracy: 0.0000e+00\n",
      "Epoch 183/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5177 - accuracy: 0.0000e+00\n",
      "Epoch 184/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5009 - accuracy: 0.0000e+00\n",
      "Epoch 185/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.4841 - accuracy: 0.0000e+00\n",
      "Epoch 186/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.4674 - accuracy: 0.0000e+00\n",
      "Epoch 187/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4507 - accuracy: 0.0000e+00\n",
      "Epoch 188/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4340 - accuracy: 0.0000e+00\n",
      "Epoch 189/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.4174 - accuracy: 0.0000e+00\n",
      "Epoch 190/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.4008 - accuracy: 0.0000e+00\n",
      "Epoch 191/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.3843 - accuracy: 0.0000e+00\n",
      "Epoch 192/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.3677 - accuracy: 0.0000e+00\n",
      "Epoch 193/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3512 - accuracy: 0.0000e+00\n",
      "Epoch 194/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3348 - accuracy: 0.0000e+00\n",
      "Epoch 195/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.3184 - accuracy: 0.0000e+00\n",
      "Epoch 196/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.3020 - accuracy: 0.0000e+00\n",
      "Epoch 197/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2857 - accuracy: 0.0000e+00\n",
      "Epoch 198/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2694 - accuracy: 0.0000e+00\n",
      "Epoch 199/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.2531 - accuracy: 0.0000e+00\n",
      "Epoch 200/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2368 - accuracy: 0.0000e+00\n",
      "Epoch 201/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.2206 - accuracy: 0.0000e+00\n",
      "Epoch 202/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.2045 - accuracy: 0.0000e+00\n",
      "Epoch 203/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1883 - accuracy: 0.0000e+00\n",
      "Epoch 204/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1722 - accuracy: 0.0000e+00\n",
      "Epoch 205/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1562 - accuracy: 0.0000e+00\n",
      "Epoch 206/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.1402 - accuracy: 0.0000e+00\n",
      "Epoch 207/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.1242 - accuracy: 0.0000e+00\n",
      "Epoch 208/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1082 - accuracy: 0.0000e+00\n",
      "Epoch 209/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0923 - accuracy: 0.0000e+00\n",
      "Epoch 210/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0764 - accuracy: 0.0000e+00\n",
      "Epoch 211/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0605 - accuracy: 0.0000e+00\n",
      "Epoch 212/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0447 - accuracy: 0.0000e+00\n",
      "Epoch 213/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.0290 - accuracy: 0.0000e+00\n",
      "Epoch 214/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0132 - accuracy: 0.0000e+00\n",
      "Epoch 215/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9975 - accuracy: 0.0000e+00\n",
      "Epoch 216/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.9818 - accuracy: 0.0000e+00\n",
      "Epoch 217/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.9662 - accuracy: 0.0000e+00\n",
      "Epoch 218/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9506 - accuracy: 0.0000e+00\n",
      "Epoch 219/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9350 - accuracy: 0.0000e+00\n",
      "Epoch 220/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.9195 - accuracy: 0.0000e+00\n",
      "Epoch 221/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9040 - accuracy: 0.0000e+00\n",
      "Epoch 222/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8885 - accuracy: 0.0000e+00\n",
      "Epoch 223/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8731 - accuracy: 0.0000e+00\n",
      "Epoch 224/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.8577 - accuracy: 0.0000e+00\n",
      "Epoch 225/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8424 - accuracy: 0.0000e+00\n",
      "Epoch 226/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.8270 - accuracy: 0.0000e+00\n",
      "Epoch 227/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8117 - accuracy: 0.0000e+00\n",
      "Epoch 228/400\n",
      "1/1 [==============================] - 0s 995us/step - loss: 3.7965 - accuracy: 0.0000e+00\n",
      "Epoch 229/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7813 - accuracy: 0.0000e+00\n",
      "Epoch 230/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.7661 - accuracy: 0.0000e+00\n",
      "Epoch 231/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7510 - accuracy: 0.0000e+00\n",
      "Epoch 232/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7358 - accuracy: 0.0000e+00\n",
      "Epoch 233/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.7208 - accuracy: 0.0000e+00\n",
      "Epoch 234/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7057 - accuracy: 0.0000e+00\n",
      "Epoch 235/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.6907 - accuracy: 0.0000e+00\n",
      "Epoch 236/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6758 - accuracy: 0.0000e+00\n",
      "Epoch 237/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.6608 - accuracy: 0.0000e+00\n",
      "Epoch 238/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.6459 - accuracy: 0.0000e+00\n",
      "Epoch 239/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6311 - accuracy: 0.0000e+00\n",
      "Epoch 240/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6162 - accuracy: 0.0000e+00\n",
      "Epoch 241/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6014 - accuracy: 0.0000e+00\n",
      "Epoch 242/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.5867 - accuracy: 0.0000e+00\n",
      "Epoch 243/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.5720 - accuracy: 0.0000e+00\n",
      "Epoch 244/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5573 - accuracy: 0.0000e+00\n",
      "Epoch 245/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5426 - accuracy: 0.0000e+00\n",
      "Epoch 246/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.5280 - accuracy: 0.0000e+00\n",
      "Epoch 247/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.5134 - accuracy: 0.0000e+00\n",
      "Epoch 248/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4989 - accuracy: 0.0000e+00\n",
      "Epoch 249/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4844 - accuracy: 0.0000e+00\n",
      "Epoch 250/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4699 - accuracy: 0.0000e+00\n",
      "Epoch 251/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.4555 - accuracy: 0.0000e+00\n",
      "Epoch 252/400\n",
      "1/1 [==============================] - 0s 994us/step - loss: 3.4411 - accuracy: 0.0000e+00\n",
      "Epoch 253/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4267 - accuracy: 0.0000e+00\n",
      "Epoch 254/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4124 - accuracy: 0.0000e+00\n",
      "Epoch 255/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3981 - accuracy: 0.0000e+00\n",
      "Epoch 256/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3838 - accuracy: 0.0000e+00\n",
      "Epoch 257/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.3696 - accuracy: 0.0000e+00\n",
      "Epoch 258/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3554 - accuracy: 0.0000e+00\n",
      "Epoch 259/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3412 - accuracy: 0.0000e+00\n",
      "Epoch 260/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3271 - accuracy: 0.0000e+00\n",
      "Epoch 261/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.3130 - accuracy: 0.0000e+00\n",
      "Epoch 262/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2989 - accuracy: 0.0000e+00\n",
      "Epoch 263/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2849 - accuracy: 0.0000e+00\n",
      "Epoch 264/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2709 - accuracy: 0.0000e+00\n",
      "Epoch 265/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.2570 - accuracy: 0.0000e+00\n",
      "Epoch 266/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2431 - accuracy: 0.0000e+00\n",
      "Epoch 267/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 3.2292 - accuracy: 0.0000e+00\n",
      "Epoch 268/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.2154 - accuracy: 0.0000e+00\n",
      "Epoch 269/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2016 - accuracy: 0.0000e+00\n",
      "Epoch 270/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1878 - accuracy: 0.0000e+00\n",
      "Epoch 271/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.1740 - accuracy: 0.0000e+00\n",
      "Epoch 272/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1603 - accuracy: 0.0000e+00\n",
      "Epoch 273/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.1467 - accuracy: 0.0000e+00\n",
      "Epoch 274/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1330 - accuracy: 0.0000e+00\n",
      "Epoch 275/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1195 - accuracy: 0.0000e+00\n",
      "Epoch 276/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1059 - accuracy: 0.0000e+00\n",
      "Epoch 277/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.0924 - accuracy: 0.0000e+00\n",
      "Epoch 278/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.0789 - accuracy: 0.0000e+00\n",
      "Epoch 279/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.0654 - accuracy: 0.0000e+00\n",
      "Epoch 280/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.0520 - accuracy: 0.0000e+00\n",
      "Epoch 281/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0386 - accuracy: 0.0000e+00\n",
      "Epoch 282/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0253 - accuracy: 0.0000e+00\n",
      "Epoch 283/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 3.0120 - accuracy: 0.0000e+00\n",
      "Epoch 284/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9987 - accuracy: 0.0000e+00\n",
      "Epoch 285/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.9854 - accuracy: 0.0000e+00\n",
      "Epoch 286/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.9722 - accuracy: 0.0000e+00\n",
      "Epoch 287/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.9590 - accuracy: 0.0000e+00\n",
      "Epoch 288/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9459 - accuracy: 0.0000e+00\n",
      "Epoch 289/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.9328 - accuracy: 0.0000e+00\n",
      "Epoch 290/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.9197 - accuracy: 0.0000e+00\n",
      "Epoch 291/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.9067 - accuracy: 0.0000e+00\n",
      "Epoch 292/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.8937 - accuracy: 0.0000e+00\n",
      "Epoch 293/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8807 - accuracy: 0.0000e+00\n",
      "Epoch 294/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8678 - accuracy: 0.0000e+00\n",
      "Epoch 295/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8549 - accuracy: 0.0000e+00\n",
      "Epoch 296/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8421 - accuracy: 0.0000e+00\n",
      "Epoch 297/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8292 - accuracy: 0.0000e+00\n",
      "Epoch 298/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.8164 - accuracy: 0.0000e+00\n",
      "Epoch 299/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8037 - accuracy: 0.0000e+00\n",
      "Epoch 300/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7910 - accuracy: 0.0000e+00\n",
      "Epoch 301/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7783 - accuracy: 0.0000e+00\n",
      "Epoch 302/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7656 - accuracy: 0.0000e+00\n",
      "Epoch 303/400\n",
      "1/1 [==============================] - 0s 995us/step - loss: 2.7530 - accuracy: 0.0000e+00\n",
      "Epoch 304/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7405 - accuracy: 0.0000e+00\n",
      "Epoch 305/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7279 - accuracy: 0.0000e+00\n",
      "Epoch 306/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7154 - accuracy: 0.0000e+00\n",
      "Epoch 307/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7029 - accuracy: 0.0000e+00\n",
      "Epoch 308/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.6905 - accuracy: 0.0000e+00\n",
      "Epoch 309/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6781 - accuracy: 0.0000e+00\n",
      "Epoch 310/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6657 - accuracy: 0.0000e+00\n",
      "Epoch 311/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6534 - accuracy: 0.0000e+00\n",
      "Epoch 312/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.6411 - accuracy: 0.0000e+00\n",
      "Epoch 313/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6288 - accuracy: 0.0000e+00\n",
      "Epoch 314/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.6166 - accuracy: 0.0000e+00\n",
      "Epoch 315/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6044 - accuracy: 0.0000e+00\n",
      "Epoch 316/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5923 - accuracy: 0.0000e+00\n",
      "Epoch 317/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.5801 - accuracy: 0.0000e+00\n",
      "Epoch 318/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5681 - accuracy: 0.0000e+00\n",
      "Epoch 319/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5560 - accuracy: 0.0000e+00\n",
      "Epoch 320/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.5440 - accuracy: 0.0000e+00\n",
      "Epoch 321/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5320 - accuracy: 0.0000e+00\n",
      "Epoch 322/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5201 - accuracy: 0.0000e+00\n",
      "Epoch 323/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.5082 - accuracy: 0.0000e+00\n",
      "Epoch 324/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4963 - accuracy: 0.0000e+00\n",
      "Epoch 325/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.4844 - accuracy: 0.0000e+00\n",
      "Epoch 326/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4726 - accuracy: 0.0000e+00\n",
      "Epoch 327/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4609 - accuracy: 0.0000e+00\n",
      "Epoch 328/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4491 - accuracy: 0.0000e+00\n",
      "Epoch 329/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.4374 - accuracy: 0.0000e+00\n",
      "Epoch 330/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4258 - accuracy: 0.0000e+00\n",
      "Epoch 331/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.4141 - accuracy: 0.0000e+00\n",
      "Epoch 332/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4025 - accuracy: 0.0000e+00\n",
      "Epoch 333/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3910 - accuracy: 0.0000e+00\n",
      "Epoch 334/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.3795 - accuracy: 0.0000e+00\n",
      "Epoch 335/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3680 - accuracy: 0.0000e+00\n",
      "Epoch 336/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3565 - accuracy: 0.0000e+00\n",
      "Epoch 337/400\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.3451 - accuracy: 0.0000e+00\n",
      "Epoch 338/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3337 - accuracy: 0.0000e+00\n",
      "Epoch 339/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3223 - accuracy: 0.0000e+00\n",
      "Epoch 340/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3110 - accuracy: 0.0000e+00\n",
      "Epoch 341/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2998 - accuracy: 0.0000e+00\n",
      "Epoch 342/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.2885 - accuracy: 0.0000e+00\n",
      "Epoch 343/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2773 - accuracy: 0.0000e+00\n",
      "Epoch 344/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2661 - accuracy: 0.0000e+00\n",
      "Epoch 345/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2550 - accuracy: 0.0000e+00\n",
      "Epoch 346/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2439 - accuracy: 0.0000e+00\n",
      "Epoch 347/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2328 - accuracy: 0.0000e+00\n",
      "Epoch 348/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.2218 - accuracy: 0.0000e+00\n",
      "Epoch 349/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2108 - accuracy: 0.0000e+00\n",
      "Epoch 350/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1998 - accuracy: 0.0000e+00\n",
      "Epoch 351/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1889 - accuracy: 0.0000e+00\n",
      "Epoch 352/400\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.1780 - accuracy: 0.0000e+00\n",
      "Epoch 353/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1671 - accuracy: 0.0000e+00\n",
      "Epoch 354/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1563 - accuracy: 0.0000e+00\n",
      "Epoch 355/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1455 - accuracy: 0.0000e+00\n",
      "Epoch 356/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1347 - accuracy: 0.0000e+00\n",
      "Epoch 357/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1240 - accuracy: 0.0000e+00\n",
      "Epoch 358/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1133 - accuracy: 0.0000e+00\n",
      "Epoch 359/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1027 - accuracy: 0.0000e+00\n",
      "Epoch 360/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0921 - accuracy: 0.0000e+00\n",
      "Epoch 361/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0815 - accuracy: 0.0000e+00\n",
      "Epoch 362/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0709 - accuracy: 0.0000e+00\n",
      "Epoch 363/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0604 - accuracy: 0.0000e+00\n",
      "Epoch 364/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0499 - accuracy: 0.0000e+00\n",
      "Epoch 365/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0395 - accuracy: 0.0000e+00\n",
      "Epoch 366/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0291 - accuracy: 0.0000e+00\n",
      "Epoch 367/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.0187 - accuracy: 0.0000e+00\n",
      "Epoch 368/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0084 - accuracy: 0.0000e+00\n",
      "Epoch 369/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.9981 - accuracy: 0.0000e+00\n",
      "Epoch 370/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.9878 - accuracy: 0.0000e+00\n",
      "Epoch 371/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9776 - accuracy: 0.0000e+00\n",
      "Epoch 372/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9674 - accuracy: 0.0000e+00\n",
      "Epoch 373/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9572 - accuracy: 0.0000e+00\n",
      "Epoch 374/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9471 - accuracy: 0.0000e+00\n",
      "Epoch 375/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9370 - accuracy: 0.0000e+00\n",
      "Epoch 376/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9269 - accuracy: 0.0000e+00\n",
      "Epoch 377/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9169 - accuracy: 0.0000e+00\n",
      "Epoch 378/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.9069 - accuracy: 0.0000e+00\n",
      "Epoch 379/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8970 - accuracy: 0.0000e+00\n",
      "Epoch 380/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8871 - accuracy: 0.0000e+00\n",
      "Epoch 381/400\n",
      "1/1 [==============================] - 0s 994us/step - loss: 1.8772 - accuracy: 0.0000e+00\n",
      "Epoch 382/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 1.8673 - accuracy: 0.0000e+00\n",
      "Epoch 383/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.8575 - accuracy: 0.0000e+00\n",
      "Epoch 384/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.8477 - accuracy: 0.0000e+00\n",
      "Epoch 385/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8380 - accuracy: 0.0000e+00\n",
      "Epoch 386/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8283 - accuracy: 0.0000e+00\n",
      "Epoch 387/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.8186 - accuracy: 0.0000e+00\n",
      "Epoch 388/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8090 - accuracy: 0.0000e+00\n",
      "Epoch 389/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7994 - accuracy: 0.0000e+00\n",
      "Epoch 390/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7898 - accuracy: 0.0000e+00\n",
      "Epoch 391/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7803 - accuracy: 0.0000e+00\n",
      "Epoch 392/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7708 - accuracy: 0.0000e+00\n",
      "Epoch 393/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.7613 - accuracy: 0.0000e+00\n",
      "Epoch 394/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7519 - accuracy: 0.0000e+00\n",
      "Epoch 395/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7425 - accuracy: 0.0000e+00\n",
      "Epoch 396/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7331 - accuracy: 0.0000e+00\n",
      "Epoch 397/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.7238 - accuracy: 0.0000e+00\n",
      "Epoch 398/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7145 - accuracy: 0.0000e+00\n",
      "Epoch 399/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7052 - accuracy: 0.0000e+00\n",
      "Epoch 400/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6960 - accuracy: 0.0000e+ - 0s 2ms/step - loss: 1.6960 - accuracy: 0.0000e+00\n",
      "1/1 - 0s - loss: 2.6598 - accuracy: 0.0000e+00\n",
      "Test loss: 2.659801959991455\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"),\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(x, y, batch_size=5, epochs=400)\n",
    "\n",
    "test_scores = model.evaluate(xtest, ytest, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "30abf103",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "in user code:\n\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:386 call\n        inputs, training=training, mask=mask)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:517 _run_internal_graph\n        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n\n    AssertionError: Could not compute output Tensor(\"loss_layer_24/loss_layer_24/Identity:0\", shape=(None, 1), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-381-ac4c0e8accd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2828\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3141\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 3142\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: in user code:\n\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:386 call\n        inputs, training=training, mask=mask)\n    C:\\Users\\marty\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:517 _run_internal_graph\n        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n\n    AssertionError: Could not compute output Tensor(\"loss_layer_24/loss_layer_24/Identity:0\", shape=(None, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model.predict([[4], [4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "5be79671",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "05021cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=16>"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss([5],[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "e61ee6da",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-393-5ec89d3d52ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(x, axis, keepdims)\u001b[0m\n\u001b[0;32m   2259\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmean\u001b[0m \u001b[0mof\u001b[0m \u001b[0melements\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2260\u001b[0m   \"\"\"\n\u001b[1;32m-> 2261\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2262\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2263\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.mean(model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "46a1e36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8372464],\n",
       "        [ 7.674493 ],\n",
       "        [ 9.511739 ],\n",
       "        [11.348986 ],\n",
       "        [13.186232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.835246 ],\n",
       "        [ 7.6714926],\n",
       "        [ 9.507739 ],\n",
       "        [11.343985 ],\n",
       "        [13.180231 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.833246 ],\n",
       "        [ 7.6684923],\n",
       "        [ 9.503738 ],\n",
       "        [11.338985 ],\n",
       "        [13.174232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8312464],\n",
       "        [ 7.6654925],\n",
       "        [ 9.499739 ],\n",
       "        [11.333985 ],\n",
       "        [13.168231 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.829246 ],\n",
       "        [ 7.6624928],\n",
       "        [ 9.495739 ],\n",
       "        [11.328985 ],\n",
       "        [13.162231 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.827246 ],\n",
       "        [ 7.6594925],\n",
       "        [ 9.491739 ],\n",
       "        [11.323985 ],\n",
       "        [13.156231 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8252463],\n",
       "        [ 7.6564927],\n",
       "        [ 9.487739 ],\n",
       "        [11.318985 ],\n",
       "        [13.150231 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8232465],\n",
       "        [ 7.6534925],\n",
       "        [ 9.483739 ],\n",
       "        [11.313986 ],\n",
       "        [13.144232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.821246 ],\n",
       "        [ 7.6504927],\n",
       "        [ 9.479739 ],\n",
       "        [11.308985 ],\n",
       "        [13.138231 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8192463],\n",
       "        [ 7.6474924],\n",
       "        [ 9.475739 ],\n",
       "        [11.303986 ],\n",
       "        [13.132232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8172464],\n",
       "        [ 7.6444926],\n",
       "        [ 9.471739 ],\n",
       "        [11.2989855],\n",
       "        [13.126232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8152466],\n",
       "        [ 7.641493 ],\n",
       "        [ 9.467739 ],\n",
       "        [11.293985 ],\n",
       "        [13.120232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8132463],\n",
       "        [ 7.6384926],\n",
       "        [ 9.463739 ],\n",
       "        [11.288985 ],\n",
       "        [13.114232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8112464],\n",
       "        [ 7.635493 ],\n",
       "        [ 9.45974  ],\n",
       "        [11.283985 ],\n",
       "        [13.1082325]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8092465],\n",
       "        [ 7.632493 ],\n",
       "        [ 9.455739 ],\n",
       "        [11.278986 ],\n",
       "        [13.102232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.807246 ],\n",
       "        [ 7.6294928],\n",
       "        [ 9.451739 ],\n",
       "        [11.273986 ],\n",
       "        [13.096232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8052464],\n",
       "        [ 7.626493 ],\n",
       "        [ 9.44774  ],\n",
       "        [11.268986 ],\n",
       "        [13.090232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8032465],\n",
       "        [ 7.6234927],\n",
       "        [ 9.443739 ],\n",
       "        [11.263986 ],\n",
       "        [13.084232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.8012466],\n",
       "        [ 7.620493 ],\n",
       "        [ 9.439739 ],\n",
       "        [11.2589855],\n",
       "        [13.078232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7992463],\n",
       "        [ 7.6174927],\n",
       "        [ 9.4357395],\n",
       "        [11.253986 ],\n",
       "        [13.072232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7972465],\n",
       "        [ 7.614493 ],\n",
       "        [ 9.43174  ],\n",
       "        [11.248986 ],\n",
       "        [13.066233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7952466],\n",
       "        [ 7.611493 ],\n",
       "        [ 9.427739 ],\n",
       "        [11.243986 ],\n",
       "        [13.060232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7932463],\n",
       "        [ 7.608493 ],\n",
       "        [ 9.423739 ],\n",
       "        [11.238986 ],\n",
       "        [13.054233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7912464],\n",
       "        [ 7.605493 ],\n",
       "        [ 9.41974  ],\n",
       "        [11.233986 ],\n",
       "        [13.048233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7892466],\n",
       "        [ 7.6024933],\n",
       "        [ 9.415739 ],\n",
       "        [11.228986 ],\n",
       "        [13.0422325]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7872467],\n",
       "        [ 7.599493 ],\n",
       "        [ 9.411739 ],\n",
       "        [11.223986 ],\n",
       "        [13.036232 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7852464],\n",
       "        [ 7.5964932],\n",
       "        [ 9.40774  ],\n",
       "        [11.2189865],\n",
       "        [13.030233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7832465],\n",
       "        [ 7.593493 ],\n",
       "        [ 9.40374  ],\n",
       "        [11.213986 ],\n",
       "        [13.024233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7812467],\n",
       "        [ 7.590493 ],\n",
       "        [ 9.39974  ],\n",
       "        [11.208986 ],\n",
       "        [13.018233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7792463],\n",
       "        [ 7.587493 ],\n",
       "        [ 9.39574  ],\n",
       "        [11.203986 ],\n",
       "        [13.012233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7772465],\n",
       "        [ 7.584493 ],\n",
       "        [ 9.39174  ],\n",
       "        [11.198986 ],\n",
       "        [13.006233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7752466],\n",
       "        [ 7.5814934],\n",
       "        [ 9.387739 ],\n",
       "        [11.193987 ],\n",
       "        [13.000233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.773247],\n",
       "        [ 7.578493],\n",
       "        [ 9.383739],\n",
       "        [11.188987],\n",
       "        [12.994233]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7712464],\n",
       "        [ 7.5754933],\n",
       "        [ 9.37974  ],\n",
       "        [11.183987 ],\n",
       "        [12.988234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7692466],\n",
       "        [ 7.5724936],\n",
       "        [ 9.37574  ],\n",
       "        [11.178987 ],\n",
       "        [12.982233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7672467],\n",
       "        [ 7.5694933],\n",
       "        [ 9.37174  ],\n",
       "        [11.173986 ],\n",
       "        [12.9762335]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7652464],\n",
       "        [ 7.5664935],\n",
       "        [ 9.367741 ],\n",
       "        [11.168987 ],\n",
       "        [12.970234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7632465],\n",
       "        [ 7.5634933],\n",
       "        [ 9.36374  ],\n",
       "        [11.163986 ],\n",
       "        [12.964233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7612467],\n",
       "        [ 7.5604935],\n",
       "        [ 9.35974  ],\n",
       "        [11.158987 ],\n",
       "        [12.958234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.759247],\n",
       "        [ 7.557493],\n",
       "        [ 9.35574 ],\n",
       "        [11.153987],\n",
       "        [12.952234]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7572465],\n",
       "        [ 7.5544934],\n",
       "        [ 9.35174  ],\n",
       "        [11.148987 ],\n",
       "        [12.946234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7552466],\n",
       "        [ 7.5514936],\n",
       "        [ 9.34774  ],\n",
       "        [11.143987 ],\n",
       "        [12.940233 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.753247 ],\n",
       "        [ 7.5484934],\n",
       "        [ 9.34374  ],\n",
       "        [11.138987 ],\n",
       "        [12.934234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.751247 ],\n",
       "        [ 7.5454936],\n",
       "        [ 9.339741 ],\n",
       "        [11.133987 ],\n",
       "        [12.928234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7492466],\n",
       "        [ 7.542494 ],\n",
       "        [ 9.33574  ],\n",
       "        [11.128987 ],\n",
       "        [12.922234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7472467],\n",
       "        [ 7.5394936],\n",
       "        [ 9.33174  ],\n",
       "        [11.123987 ],\n",
       "        [12.916234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.745247 ],\n",
       "        [ 7.5364933],\n",
       "        [ 9.327741 ],\n",
       "        [11.118987 ],\n",
       "        [12.910234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.743247 ],\n",
       "        [ 7.5334935],\n",
       "        [ 9.32374  ],\n",
       "        [11.113987 ],\n",
       "        [12.904234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7412467],\n",
       "        [ 7.5304937],\n",
       "        [ 9.31974  ],\n",
       "        [11.108988 ],\n",
       "        [12.898234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.739247 ],\n",
       "        [ 7.5274935],\n",
       "        [ 9.315741 ],\n",
       "        [11.103988 ],\n",
       "        [12.892235 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.737247 ],\n",
       "        [ 7.5244937],\n",
       "        [ 9.311741 ],\n",
       "        [11.098988 ],\n",
       "        [12.886234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7352467],\n",
       "        [ 7.521494 ],\n",
       "        [ 9.307741 ],\n",
       "        [11.093987 ],\n",
       "        [12.880234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.733247 ],\n",
       "        [ 7.5184937],\n",
       "        [ 9.3037405],\n",
       "        [11.088987 ],\n",
       "        [12.874234 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.731247],\n",
       "        [ 7.515494],\n",
       "        [ 9.299741],\n",
       "        [11.083988],\n",
       "        [12.868235]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.729247],\n",
       "        [ 7.512494],\n",
       "        [ 9.29574 ],\n",
       "        [11.078987],\n",
       "        [12.862234]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7272468],\n",
       "        [ 7.509494 ],\n",
       "        [ 9.29174  ],\n",
       "        [11.073988 ],\n",
       "        [12.856235 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.725247 ],\n",
       "        [ 7.5064936],\n",
       "        [ 9.287741 ],\n",
       "        [11.068988 ],\n",
       "        [12.850235 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.723247],\n",
       "        [ 7.503494],\n",
       "        [ 9.283741],\n",
       "        [11.063988],\n",
       "        [12.844234]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.7212467],\n",
       "        [ 7.500494 ],\n",
       "        [ 9.279741 ],\n",
       "        [11.058988 ],\n",
       "        [12.838235 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.719247 ],\n",
       "        [ 7.4974937],\n",
       "        [ 9.275742 ],\n",
       "        [11.0539875],\n",
       "        [12.832235 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.717247],\n",
       "        [ 7.494494],\n",
       "        [ 9.271741],\n",
       "        [11.048988],\n",
       "        [12.826235]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.715247],\n",
       "        [ 7.491494],\n",
       "        [ 9.267741],\n",
       "        [11.043988],\n",
       "        [12.820235]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.713247],\n",
       "        [ 7.488494],\n",
       "        [ 9.263741],\n",
       "        [11.038988],\n",
       "        [12.814236]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.711247],\n",
       "        [ 7.485494],\n",
       "        [ 9.259741],\n",
       "        [11.033988],\n",
       "        [12.808235]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.709247],\n",
       "        [ 7.482494],\n",
       "        [ 9.255741],\n",
       "        [11.028988],\n",
       "        [12.802235]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.707247],\n",
       "        [ 7.479494],\n",
       "        [ 9.251741],\n",
       "        [11.023989],\n",
       "        [12.796235]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.705247 ],\n",
       "        [ 7.476494 ],\n",
       "        [ 9.247742 ],\n",
       "        [11.018989 ],\n",
       "        [12.7902355]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.703247 ],\n",
       "        [ 7.473494 ],\n",
       "        [ 9.243741 ],\n",
       "        [11.0139885],\n",
       "        [12.784235 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.701247 ],\n",
       "        [ 7.4704943],\n",
       "        [ 9.239741 ],\n",
       "        [11.008988 ],\n",
       "        [12.778235 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.699247],\n",
       "        [ 7.467494],\n",
       "        [ 9.235742],\n",
       "        [11.003988],\n",
       "        [12.772236]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.697247],\n",
       "        [ 7.464494],\n",
       "        [ 9.231741],\n",
       "        [10.998988],\n",
       "        [12.766235]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.695247 ],\n",
       "        [ 7.4614944],\n",
       "        [ 9.227741 ],\n",
       "        [10.993988 ],\n",
       "        [12.760236 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.693247],\n",
       "        [ 7.458494],\n",
       "        [ 9.223742],\n",
       "        [10.988989],\n",
       "        [12.754236]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.691247 ],\n",
       "        [ 7.4554944],\n",
       "        [ 9.219742 ],\n",
       "        [10.983989 ],\n",
       "        [12.748236 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.689247],\n",
       "        [ 7.452494],\n",
       "        [ 9.215741],\n",
       "        [10.978989],\n",
       "        [12.742236]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6872473],\n",
       "        [ 7.4494944],\n",
       "        [ 9.211741 ],\n",
       "        [10.973989 ],\n",
       "        [12.736236 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.685247],\n",
       "        [ 7.446494],\n",
       "        [ 9.207742],\n",
       "        [10.968988],\n",
       "        [12.730236]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.683247 ],\n",
       "        [ 7.4434943],\n",
       "        [ 9.203741 ],\n",
       "        [10.963989 ],\n",
       "        [12.724236 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.681247 ],\n",
       "        [ 7.4404945],\n",
       "        [ 9.199741 ],\n",
       "        [10.958989 ],\n",
       "        [12.718236 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.679247 ],\n",
       "        [ 7.4374943],\n",
       "        [ 9.195742 ],\n",
       "        [10.953989 ],\n",
       "        [12.712236 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.677247 ],\n",
       "        [ 7.4344945],\n",
       "        [ 9.191742 ],\n",
       "        [10.948989 ],\n",
       "        [12.706236 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.675247 ],\n",
       "        [ 7.4314947],\n",
       "        [ 9.187742 ],\n",
       "        [10.943989 ],\n",
       "        [12.700236 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6732473],\n",
       "        [ 7.4284945],\n",
       "        [ 9.183742 ],\n",
       "        [10.93899  ],\n",
       "        [12.694237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.671247],\n",
       "        [ 7.425494],\n",
       "        [ 9.179742],\n",
       "        [10.933989],\n",
       "        [12.688236]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.669247 ],\n",
       "        [ 7.4224944],\n",
       "        [ 9.175742 ],\n",
       "        [10.928989 ],\n",
       "        [12.682237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6672473],\n",
       "        [ 7.4194946],\n",
       "        [ 9.1717415],\n",
       "        [10.923989 ],\n",
       "        [12.676237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.665247 ],\n",
       "        [ 7.4164944],\n",
       "        [ 9.167742 ],\n",
       "        [10.918989 ],\n",
       "        [12.670237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.663247 ],\n",
       "        [ 7.4134946],\n",
       "        [ 9.163742 ],\n",
       "        [10.913989 ],\n",
       "        [12.664237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6612473],\n",
       "        [ 7.410495 ],\n",
       "        [ 9.159742 ],\n",
       "        [10.908989 ],\n",
       "        [12.6582365]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6592474],\n",
       "        [ 7.4074945],\n",
       "        [ 9.155742 ],\n",
       "        [10.90399  ],\n",
       "        [12.652237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.657247],\n",
       "        [ 7.404495],\n",
       "        [ 9.151742],\n",
       "        [10.89899 ],\n",
       "        [12.646236]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.655247],\n",
       "        [ 7.401495],\n",
       "        [ 9.147742],\n",
       "        [10.89399 ],\n",
       "        [12.640237]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6532474],\n",
       "        [ 7.3984947],\n",
       "        [ 9.143742 ],\n",
       "        [10.888989 ],\n",
       "        [12.634237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.651247 ],\n",
       "        [ 7.3954945],\n",
       "        [ 9.139742 ],\n",
       "        [10.883989 ],\n",
       "        [12.628237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.649247 ],\n",
       "        [ 7.3924947],\n",
       "        [ 9.135742 ],\n",
       "        [10.87899  ],\n",
       "        [12.622237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6472473],\n",
       "        [ 7.389495 ],\n",
       "        [ 9.1317425],\n",
       "        [10.873989 ],\n",
       "        [12.616238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6452475],\n",
       "        [ 7.3864946],\n",
       "        [ 9.127743 ],\n",
       "        [10.86899  ],\n",
       "        [12.610237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6432476],\n",
       "        [ 7.383495 ],\n",
       "        [ 9.123742 ],\n",
       "        [10.86399  ],\n",
       "        [12.604238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6412473],\n",
       "        [ 7.380495 ],\n",
       "        [ 9.119742 ],\n",
       "        [10.85899  ],\n",
       "        [12.598238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6392474],\n",
       "        [ 7.377495 ],\n",
       "        [ 9.115743 ],\n",
       "        [10.85399  ],\n",
       "        [12.592237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6372476],\n",
       "        [ 7.374495 ],\n",
       "        [ 9.111742 ],\n",
       "        [10.8489895],\n",
       "        [12.586237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.635247],\n",
       "        [ 7.371495],\n",
       "        [ 9.107742],\n",
       "        [10.84399 ],\n",
       "        [12.580237]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6332474],\n",
       "        [ 7.368495 ],\n",
       "        [ 9.103743 ],\n",
       "        [10.83899  ],\n",
       "        [12.574238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6312475],\n",
       "        [ 7.3654947],\n",
       "        [ 9.099743 ],\n",
       "        [10.83399  ],\n",
       "        [12.568237 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6292477],\n",
       "        [ 7.362495 ],\n",
       "        [ 9.095743 ],\n",
       "        [10.82899  ],\n",
       "        [12.562238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6272473],\n",
       "        [ 7.359495 ],\n",
       "        [ 9.0917425],\n",
       "        [10.82399  ],\n",
       "        [12.556238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6252475],\n",
       "        [ 7.356495 ],\n",
       "        [ 9.087743 ],\n",
       "        [10.818991 ],\n",
       "        [12.550238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6232476],\n",
       "        [ 7.353495 ],\n",
       "        [ 9.083743 ],\n",
       "        [10.813991 ],\n",
       "        [12.544238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6212473],\n",
       "        [ 7.3504953],\n",
       "        [ 9.079742 ],\n",
       "        [10.8089905],\n",
       "        [12.538238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6192474],\n",
       "        [ 7.347495 ],\n",
       "        [ 9.075743 ],\n",
       "        [10.80399  ],\n",
       "        [12.532238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6172476],\n",
       "        [ 7.3444953],\n",
       "        [ 9.071743 ],\n",
       "        [10.79899  ],\n",
       "        [12.526238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6152477],\n",
       "        [ 7.341495 ],\n",
       "        [ 9.067743 ],\n",
       "        [10.793991 ],\n",
       "        [12.520239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6132474],\n",
       "        [ 7.3384953],\n",
       "        [ 9.063743 ],\n",
       "        [10.78899  ],\n",
       "        [12.514238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6112475],\n",
       "        [ 7.335495 ],\n",
       "        [ 9.059743 ],\n",
       "        [10.783991 ],\n",
       "        [12.508238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6092477],\n",
       "        [ 7.332495 ],\n",
       "        [ 9.055743 ],\n",
       "        [10.778991 ],\n",
       "        [12.502238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6072474],\n",
       "        [ 7.3294954],\n",
       "        [ 9.051743 ],\n",
       "        [10.773991 ],\n",
       "        [12.496239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6052475],\n",
       "        [ 7.326495 ],\n",
       "        [ 9.047743 ],\n",
       "        [10.7689905],\n",
       "        [12.490238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.6032476],\n",
       "        [ 7.3234954],\n",
       "        [ 9.043743 ],\n",
       "        [10.76399  ],\n",
       "        [12.484239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.601248 ],\n",
       "        [ 7.3204956],\n",
       "        [ 9.039743 ],\n",
       "        [10.758991 ],\n",
       "        [12.478239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5992475],\n",
       "        [ 7.3174953],\n",
       "        [ 9.035743 ],\n",
       "        [10.753991 ],\n",
       "        [12.472239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5972476],\n",
       "        [ 7.314495 ],\n",
       "        [ 9.031743 ],\n",
       "        [10.748991 ],\n",
       "        [12.466239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5952477],\n",
       "        [ 7.3114953],\n",
       "        [ 9.027743 ],\n",
       "        [10.743991 ],\n",
       "        [12.460238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5932474],\n",
       "        [ 7.3084955],\n",
       "        [ 9.023743 ],\n",
       "        [10.738991 ],\n",
       "        [12.454239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5912476],\n",
       "        [ 7.3054953],\n",
       "        [ 9.019743 ],\n",
       "        [10.733992 ],\n",
       "        [12.448238 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5892477],\n",
       "        [ 7.3024955],\n",
       "        [ 9.015743 ],\n",
       "        [10.728991 ],\n",
       "        [12.442239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.587248 ],\n",
       "        [ 7.2994957],\n",
       "        [ 9.011744 ],\n",
       "        [10.723991 ],\n",
       "        [12.436239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5852475],\n",
       "        [ 7.2964954],\n",
       "        [ 9.007744 ],\n",
       "        [10.718991 ],\n",
       "        [12.430239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5832477],\n",
       "        [ 7.2934957],\n",
       "        [ 9.003743 ],\n",
       "        [10.713991 ],\n",
       "        [12.424239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.581248 ],\n",
       "        [ 7.2904954],\n",
       "        [ 8.999743 ],\n",
       "        [10.708991 ],\n",
       "        [12.41824  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5792475],\n",
       "        [ 7.2874956],\n",
       "        [ 8.995744 ],\n",
       "        [10.703991 ],\n",
       "        [12.412239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5772476],\n",
       "        [ 7.2844954],\n",
       "        [ 8.991743 ],\n",
       "        [10.698992 ],\n",
       "        [12.4062395]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.575248 ],\n",
       "        [ 7.2814956],\n",
       "        [ 8.987743 ],\n",
       "        [10.693992 ],\n",
       "        [12.400239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.573248],\n",
       "        [ 7.278496],\n",
       "        [ 8.983744],\n",
       "        [10.688992],\n",
       "        [12.394239]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5712476],\n",
       "        [ 7.2754955],\n",
       "        [ 8.979744 ],\n",
       "        [10.683991 ],\n",
       "        [12.38824  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5692477],\n",
       "        [ 7.2724957],\n",
       "        [ 8.975743 ],\n",
       "        [10.678991 ],\n",
       "        [12.382239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.567248],\n",
       "        [ 7.269496],\n",
       "        [ 8.971744],\n",
       "        [10.673992],\n",
       "        [12.37624 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5652475],\n",
       "        [ 7.2664957],\n",
       "        [ 8.967744 ],\n",
       "        [10.668992 ],\n",
       "        [12.370239 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5632477],\n",
       "        [ 7.263496 ],\n",
       "        [ 8.963744 ],\n",
       "        [10.663992 ],\n",
       "        [12.36424  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.561248 ],\n",
       "        [ 7.2604957],\n",
       "        [ 8.9597435],\n",
       "        [10.658992 ],\n",
       "        [12.35824  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.559248],\n",
       "        [ 7.257496],\n",
       "        [ 8.955744],\n",
       "        [10.653992],\n",
       "        [12.35224 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5572476],\n",
       "        [ 7.2544956],\n",
       "        [ 8.951744 ],\n",
       "        [10.648992 ],\n",
       "        [12.34624  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.555248 ],\n",
       "        [ 7.251496 ],\n",
       "        [ 8.947744 ],\n",
       "        [10.643991 ],\n",
       "        [12.3402405]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.553248],\n",
       "        [ 7.248496],\n",
       "        [ 8.943744],\n",
       "        [10.638992],\n",
       "        [12.33424 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5512476],\n",
       "        [ 7.245496 ],\n",
       "        [ 8.939744 ],\n",
       "        [10.633992 ],\n",
       "        [12.32824  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5492477],\n",
       "        [ 7.242496 ],\n",
       "        [ 8.935744 ],\n",
       "        [10.628992 ],\n",
       "        [12.32224  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.547248],\n",
       "        [ 7.239496],\n",
       "        [ 8.931744],\n",
       "        [10.623992],\n",
       "        [12.31624 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.545248],\n",
       "        [ 7.236496],\n",
       "        [ 8.927744],\n",
       "        [10.618992],\n",
       "        [12.310241]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5432477],\n",
       "        [ 7.2334957],\n",
       "        [ 8.923744 ],\n",
       "        [10.613993 ],\n",
       "        [12.30424  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.541248 ],\n",
       "        [ 7.230496 ],\n",
       "        [ 8.9197445],\n",
       "        [10.608993 ],\n",
       "        [12.298241 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.539248],\n",
       "        [ 7.227496],\n",
       "        [ 8.915744],\n",
       "        [10.603992],\n",
       "        [12.29224 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5372477],\n",
       "        [ 7.224496 ],\n",
       "        [ 8.911744 ],\n",
       "        [10.598992 ],\n",
       "        [12.286241 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.535248],\n",
       "        [ 7.221496],\n",
       "        [ 8.907744],\n",
       "        [10.593992],\n",
       "        [12.280241]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.533248 ],\n",
       "        [ 7.2184963],\n",
       "        [ 8.903745 ],\n",
       "        [10.588993 ],\n",
       "        [12.2742405]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.531248],\n",
       "        [ 7.215496],\n",
       "        [ 8.899744],\n",
       "        [10.583992],\n",
       "        [12.268241]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5292478],\n",
       "        [ 7.2124963],\n",
       "        [ 8.895744 ],\n",
       "        [10.578993 ],\n",
       "        [12.262241 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.527248 ],\n",
       "        [ 7.2094965],\n",
       "        [ 8.891745 ],\n",
       "        [10.573993 ],\n",
       "        [12.256241 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.525248 ],\n",
       "        [ 7.2064962],\n",
       "        [ 8.887745 ],\n",
       "        [10.568993 ],\n",
       "        [12.25024  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5232477],\n",
       "        [ 7.203496 ],\n",
       "        [ 8.883744 ],\n",
       "        [10.5639925],\n",
       "        [12.244241 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.521248],\n",
       "        [ 7.200496],\n",
       "        [ 8.879745],\n",
       "        [10.558992],\n",
       "        [12.238241]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.519248 ],\n",
       "        [ 7.1974964],\n",
       "        [ 8.875745 ],\n",
       "        [10.553993 ],\n",
       "        [12.232241 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.517248],\n",
       "        [ 7.194496],\n",
       "        [ 8.871744],\n",
       "        [10.548993],\n",
       "        [12.226241]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5152483],\n",
       "        [ 7.1914964],\n",
       "        [ 8.867744 ],\n",
       "        [10.543993 ],\n",
       "        [12.220242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.513248 ],\n",
       "        [ 7.1884966],\n",
       "        [ 8.863745 ],\n",
       "        [10.538993 ],\n",
       "        [12.214241 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.511248 ],\n",
       "        [ 7.1854963],\n",
       "        [ 8.859745 ],\n",
       "        [10.533993 ],\n",
       "        [12.208241 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5092483],\n",
       "        [ 7.1824965],\n",
       "        [ 8.855745 ],\n",
       "        [10.528994 ],\n",
       "        [12.202242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.507248 ],\n",
       "        [ 7.1794963],\n",
       "        [ 8.851745 ],\n",
       "        [10.5239935],\n",
       "        [12.196241 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.505248 ],\n",
       "        [ 7.1764965],\n",
       "        [ 8.847745 ],\n",
       "        [10.518993 ],\n",
       "        [12.190242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.503248 ],\n",
       "        [ 7.1734962],\n",
       "        [ 8.843744 ],\n",
       "        [10.513993 ],\n",
       "        [12.184242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.5012484],\n",
       "        [ 7.1704965],\n",
       "        [ 8.839745 ],\n",
       "        [10.508993 ],\n",
       "        [12.178242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.499248 ],\n",
       "        [ 7.1674967],\n",
       "        [ 8.835745 ],\n",
       "        [10.503993 ],\n",
       "        [12.172241 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.497248 ],\n",
       "        [ 7.1644964],\n",
       "        [ 8.831745 ],\n",
       "        [10.498993 ],\n",
       "        [12.166242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4952483],\n",
       "        [ 7.1614966],\n",
       "        [ 8.827745 ],\n",
       "        [10.493994 ],\n",
       "        [12.160242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.493248],\n",
       "        [ 7.158497],\n",
       "        [ 8.823745],\n",
       "        [10.488994],\n",
       "        [12.154242]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.491248 ],\n",
       "        [ 7.1554966],\n",
       "        [ 8.819745 ],\n",
       "        [10.483994 ],\n",
       "        [12.148242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4892483],\n",
       "        [ 7.152497 ],\n",
       "        [ 8.815745 ],\n",
       "        [10.478993 ],\n",
       "        [12.142242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4872484],\n",
       "        [ 7.1494966],\n",
       "        [ 8.811745 ],\n",
       "        [10.473993 ],\n",
       "        [12.136242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.485248],\n",
       "        [ 7.146497],\n",
       "        [ 8.807745],\n",
       "        [10.468994],\n",
       "        [12.130242]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.483248 ],\n",
       "        [ 7.1434965],\n",
       "        [ 8.803745 ],\n",
       "        [10.463994 ],\n",
       "        [12.124243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4812484],\n",
       "        [ 7.1404967],\n",
       "        [ 8.799746 ],\n",
       "        [10.458994 ],\n",
       "        [12.118242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.479248],\n",
       "        [ 7.137497],\n",
       "        [ 8.795746],\n",
       "        [10.453994],\n",
       "        [12.112242]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.477248 ],\n",
       "        [ 7.1344967],\n",
       "        [ 8.791745 ],\n",
       "        [10.448994 ],\n",
       "        [12.106242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4752483],\n",
       "        [ 7.131497 ],\n",
       "        [ 8.787745 ],\n",
       "        [10.4439945],\n",
       "        [12.100243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4732485],\n",
       "        [ 7.128497 ],\n",
       "        [ 8.783746 ],\n",
       "        [10.438993 ],\n",
       "        [12.094242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.471248],\n",
       "        [ 7.125497],\n",
       "        [ 8.779745],\n",
       "        [10.433994],\n",
       "        [12.088243]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4692483],\n",
       "        [ 7.1224966],\n",
       "        [ 8.775745 ],\n",
       "        [10.428994 ],\n",
       "        [12.082243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4672484],\n",
       "        [ 7.119497 ],\n",
       "        [ 8.771746 ],\n",
       "        [10.423994 ],\n",
       "        [12.076242 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.465248],\n",
       "        [ 7.116497],\n",
       "        [ 8.767746],\n",
       "        [10.418994],\n",
       "        [12.070243]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4632483],\n",
       "        [ 7.113497 ],\n",
       "        [ 8.763745 ],\n",
       "        [10.413994 ],\n",
       "        [12.064243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4612484],\n",
       "        [ 7.110497 ],\n",
       "        [ 8.759746 ],\n",
       "        [10.408995 ],\n",
       "        [12.058243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4592485],\n",
       "        [ 7.107497 ],\n",
       "        [ 8.755746 ],\n",
       "        [10.403995 ],\n",
       "        [12.052243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.457248],\n",
       "        [ 7.104497],\n",
       "        [ 8.751745],\n",
       "        [10.398994],\n",
       "        [12.046244]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4552484],\n",
       "        [ 7.101497 ],\n",
       "        [ 8.7477455],\n",
       "        [10.393994 ],\n",
       "        [12.040243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4532485],\n",
       "        [ 7.098497 ],\n",
       "        [ 8.743746 ],\n",
       "        [10.388994 ],\n",
       "        [12.034243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.451248],\n",
       "        [ 7.095497],\n",
       "        [ 8.739746],\n",
       "        [10.383995],\n",
       "        [12.028243]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4492483],\n",
       "        [ 7.092497 ],\n",
       "        [ 8.735745 ],\n",
       "        [10.378995 ],\n",
       "        [12.0222435]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4472485],\n",
       "        [ 7.089497 ],\n",
       "        [ 8.731746 ],\n",
       "        [10.373995 ],\n",
       "        [12.016243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4452486],\n",
       "        [ 7.0864973],\n",
       "        [ 8.727746 ],\n",
       "        [10.368995 ],\n",
       "        [12.010243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4432483],\n",
       "        [ 7.083497 ],\n",
       "        [ 8.723746 ],\n",
       "        [10.363995 ],\n",
       "        [12.004244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4412484],\n",
       "        [ 7.0804973],\n",
       "        [ 8.719746 ],\n",
       "        [10.3589945],\n",
       "        [11.998243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4392486],\n",
       "        [ 7.0774975],\n",
       "        [ 8.715746 ],\n",
       "        [10.353994 ],\n",
       "        [11.992244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.437248],\n",
       "        [ 7.074497],\n",
       "        [ 8.711746],\n",
       "        [10.348995],\n",
       "        [11.986244]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4352484],\n",
       "        [ 7.0714974],\n",
       "        [ 8.7077465],\n",
       "        [10.343995 ],\n",
       "        [11.980244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4332485],\n",
       "        [ 7.068497 ],\n",
       "        [ 8.703746 ],\n",
       "        [10.338995 ],\n",
       "        [11.974244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4312487],\n",
       "        [ 7.0654974],\n",
       "        [ 8.699746 ],\n",
       "        [10.333995 ],\n",
       "        [11.968244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4292483],\n",
       "        [ 7.062497 ],\n",
       "        [ 8.695746 ],\n",
       "        [10.328995 ],\n",
       "        [11.962244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4272485],\n",
       "        [ 7.0594974],\n",
       "        [ 8.691746 ],\n",
       "        [10.323996 ],\n",
       "        [11.9562435]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4252486],\n",
       "        [ 7.0564976],\n",
       "        [ 8.687746 ],\n",
       "        [10.318995 ],\n",
       "        [11.950244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4232483],\n",
       "        [ 7.0534973],\n",
       "        [ 8.683746 ],\n",
       "        [10.313995 ],\n",
       "        [11.944244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4212484],\n",
       "        [ 7.0504975],\n",
       "        [ 8.679747 ],\n",
       "        [10.308995 ],\n",
       "        [11.938244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4192486],\n",
       "        [ 7.0474977],\n",
       "        [ 8.675747 ],\n",
       "        [10.303995 ],\n",
       "        [11.932244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4172487],\n",
       "        [ 7.0444975],\n",
       "        [ 8.671746 ],\n",
       "        [10.298996 ],\n",
       "        [11.926245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4152484],\n",
       "        [ 7.041497 ],\n",
       "        [ 8.667747 ],\n",
       "        [10.293995 ],\n",
       "        [11.920244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4132485],\n",
       "        [ 7.0384974],\n",
       "        [ 8.663747 ],\n",
       "        [10.288996 ],\n",
       "        [11.914245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4112487],\n",
       "        [ 7.0354977],\n",
       "        [ 8.659746 ],\n",
       "        [10.283996 ],\n",
       "        [11.908245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4092484],\n",
       "        [ 7.0324974],\n",
       "        [ 8.655746 ],\n",
       "        [10.2789955],\n",
       "        [11.902245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4072485],\n",
       "        [ 7.0294976],\n",
       "        [ 8.651747 ],\n",
       "        [10.273995 ],\n",
       "        [11.896245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4052486],\n",
       "        [ 7.026498 ],\n",
       "        [ 8.647747 ],\n",
       "        [10.268995 ],\n",
       "        [11.8902445]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.403249 ],\n",
       "        [ 7.0234976],\n",
       "        [ 8.643746 ],\n",
       "        [10.263996 ],\n",
       "        [11.884245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4012485],\n",
       "        [ 7.020498 ],\n",
       "        [ 8.639747 ],\n",
       "        [10.258996 ],\n",
       "        [11.878244 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3992486],\n",
       "        [ 7.017498 ],\n",
       "        [ 8.635747 ],\n",
       "        [10.253996 ],\n",
       "        [11.872245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3972487],\n",
       "        [ 7.0144978],\n",
       "        [ 8.631747 ],\n",
       "        [10.248996 ],\n",
       "        [11.866245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.395249 ],\n",
       "        [ 7.0114975],\n",
       "        [ 8.627747 ],\n",
       "        [10.243996 ],\n",
       "        [11.860245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3932486],\n",
       "        [ 7.0084977],\n",
       "        [ 8.623747 ],\n",
       "        [10.2389965],\n",
       "        [11.854245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3912487],\n",
       "        [ 7.005498 ],\n",
       "        [ 8.619747 ],\n",
       "        [10.233995 ],\n",
       "        [11.848246 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.389249 ],\n",
       "        [ 7.0024977],\n",
       "        [ 8.615747 ],\n",
       "        [10.228996 ],\n",
       "        [11.842245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3872485],\n",
       "        [ 6.999498 ],\n",
       "        [ 8.611747 ],\n",
       "        [10.223996 ],\n",
       "        [11.836246 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3852487],\n",
       "        [ 6.996498 ],\n",
       "        [ 8.607747 ],\n",
       "        [10.218996 ],\n",
       "        [11.830246 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.383249],\n",
       "        [ 6.993498],\n",
       "        [ 8.603747],\n",
       "        [10.213996],\n",
       "        [11.824245]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.381249],\n",
       "        [ 6.990498],\n",
       "        [ 8.599747],\n",
       "        [10.208996],\n",
       "        [11.818245]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3792486],\n",
       "        [ 6.987498 ],\n",
       "        [ 8.595747 ],\n",
       "        [10.203997 ],\n",
       "        [11.812245 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.377249],\n",
       "        [ 6.984498],\n",
       "        [ 8.591747],\n",
       "        [10.198997],\n",
       "        [11.806246]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.375249],\n",
       "        [ 6.981498],\n",
       "        [ 8.587748],\n",
       "        [10.193996],\n",
       "        [11.800245]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.373249],\n",
       "        [ 6.978498],\n",
       "        [ 8.583747],\n",
       "        [10.188996],\n",
       "        [11.794246]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3712487],\n",
       "        [ 6.975498 ],\n",
       "        [ 8.579747 ],\n",
       "        [10.183996 ],\n",
       "        [11.788246 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.369249 ],\n",
       "        [ 6.972498 ],\n",
       "        [ 8.5757475],\n",
       "        [10.178997 ],\n",
       "        [11.782246 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.367249],\n",
       "        [ 6.969498],\n",
       "        [ 8.571747],\n",
       "        [10.173997],\n",
       "        [11.776246]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3652487],\n",
       "        [ 6.9664984],\n",
       "        [ 8.567747 ],\n",
       "        [10.168997 ],\n",
       "        [11.770246 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.363249],\n",
       "        [ 6.963498],\n",
       "        [ 8.563747],\n",
       "        [10.163997],\n",
       "        [11.764246]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.361249 ],\n",
       "        [ 6.9604983],\n",
       "        [ 8.559748 ],\n",
       "        [10.158997 ],\n",
       "        [11.758246 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.359249],\n",
       "        [ 6.957498],\n",
       "        [ 8.555748],\n",
       "        [10.153997],\n",
       "        [11.752247]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.357249 ],\n",
       "        [ 6.9544983],\n",
       "        [ 8.551747 ],\n",
       "        [10.148996 ],\n",
       "        [11.746246 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.355249],\n",
       "        [ 6.951498],\n",
       "        [ 8.547748],\n",
       "        [10.143997],\n",
       "        [11.740246]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.353249 ],\n",
       "        [ 6.9484982],\n",
       "        [ 8.543748 ],\n",
       "        [10.138997 ],\n",
       "        [11.734246 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3512487],\n",
       "        [ 6.9454985],\n",
       "        [ 8.539747 ],\n",
       "        [10.133997 ],\n",
       "        [11.728247 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.349249],\n",
       "        [ 6.942498],\n",
       "        [ 8.535748],\n",
       "        [10.128997],\n",
       "        [11.722246]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.347249 ],\n",
       "        [ 6.9394984],\n",
       "        [ 8.531748 ],\n",
       "        [10.123997 ],\n",
       "        [11.716247 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.345249],\n",
       "        [ 6.936498],\n",
       "        [ 8.527748],\n",
       "        [10.118998],\n",
       "        [11.710247]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.343249 ],\n",
       "        [ 6.9334984],\n",
       "        [ 8.523747 ],\n",
       "        [10.113997 ],\n",
       "        [11.7042465]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.341249 ],\n",
       "        [ 6.9304986],\n",
       "        [ 8.519748 ],\n",
       "        [10.108997 ],\n",
       "        [11.698247 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.339249 ],\n",
       "        [ 6.9274983],\n",
       "        [ 8.515748 ],\n",
       "        [10.103997 ],\n",
       "        [11.692246 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.337249 ],\n",
       "        [ 6.9244986],\n",
       "        [ 8.511747 ],\n",
       "        [10.098997 ],\n",
       "        [11.686247 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.335249 ],\n",
       "        [ 6.9214983],\n",
       "        [ 8.507748 ],\n",
       "        [10.093998 ],\n",
       "        [11.680246 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.333249 ],\n",
       "        [ 6.9184985],\n",
       "        [ 8.503748 ],\n",
       "        [10.088998 ],\n",
       "        [11.674247 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.331249 ],\n",
       "        [ 6.9154987],\n",
       "        [ 8.499748 ],\n",
       "        [10.083998 ],\n",
       "        [11.668247 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.329249 ],\n",
       "        [ 6.9124985],\n",
       "        [ 8.4957485],\n",
       "        [10.078998 ],\n",
       "        [11.662247 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.327249 ],\n",
       "        [ 6.9094987],\n",
       "        [ 8.491748 ],\n",
       "        [10.0739975],\n",
       "        [11.656247 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.325249],\n",
       "        [ 6.906499],\n",
       "        [ 8.487748],\n",
       "        [10.068998],\n",
       "        [11.650248]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3232493],\n",
       "        [ 6.9034986],\n",
       "        [ 8.483748 ],\n",
       "        [10.063997 ],\n",
       "        [11.644247 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.321249 ],\n",
       "        [ 6.900499 ],\n",
       "        [ 8.479748 ],\n",
       "        [10.058998 ],\n",
       "        [11.6382475]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.319249],\n",
       "        [ 6.897499],\n",
       "        [ 8.475748],\n",
       "        [10.053998],\n",
       "        [11.632248]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3172493],\n",
       "        [ 6.894499 ],\n",
       "        [ 8.471748 ],\n",
       "        [10.048998 ],\n",
       "        [11.626247 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3152494],\n",
       "        [ 6.891499 ],\n",
       "        [ 8.467749 ],\n",
       "        [10.043998 ],\n",
       "        [11.620248 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.313249],\n",
       "        [ 6.888499],\n",
       "        [ 8.463749],\n",
       "        [10.038998],\n",
       "        [11.614248]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3112493],\n",
       "        [ 6.885499 ],\n",
       "        [ 8.459748 ],\n",
       "        [10.0339985],\n",
       "        [11.608248 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3092494],\n",
       "        [ 6.8824987],\n",
       "        [ 8.455749 ],\n",
       "        [10.028998 ],\n",
       "        [11.602248 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3072495],\n",
       "        [ 6.879499 ],\n",
       "        [ 8.451749 ],\n",
       "        [10.023998 ],\n",
       "        [11.596248 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.305249],\n",
       "        [ 6.876499],\n",
       "        [ 8.447748],\n",
       "        [10.018998],\n",
       "        [11.590248]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3032494],\n",
       "        [ 6.873499 ],\n",
       "        [ 8.443748 ],\n",
       "        [10.013998 ],\n",
       "        [11.584248 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.3012495],\n",
       "        [ 6.870499 ],\n",
       "        [ 8.439749 ],\n",
       "        [10.008999 ],\n",
       "        [11.578248 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2992496],\n",
       "        [ 6.8674994],\n",
       "        [ 8.435749 ],\n",
       "        [10.003999 ],\n",
       "        [11.572248 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2972493],\n",
       "        [ 6.864499 ],\n",
       "        [ 8.431749 ],\n",
       "        [ 9.998999 ],\n",
       "        [11.566248 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2952495],\n",
       "        [ 6.8614993],\n",
       "        [ 8.427749 ],\n",
       "        [ 9.993999 ],\n",
       "        [11.560248 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2932496],\n",
       "        [ 6.8584995],\n",
       "        [ 8.423749 ],\n",
       "        [ 9.988998 ],\n",
       "        [11.554249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2912498],\n",
       "        [ 6.8554993],\n",
       "        [ 8.419749 ],\n",
       "        [ 9.983999 ],\n",
       "        [11.548248 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2892494],\n",
       "        [ 6.8524995],\n",
       "        [ 8.415749 ],\n",
       "        [ 9.978999 ],\n",
       "        [11.542249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2872496],\n",
       "        [ 6.8494997],\n",
       "        [ 8.411749 ],\n",
       "        [ 9.973999 ],\n",
       "        [11.536249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2852497],\n",
       "        [ 6.8464994],\n",
       "        [ 8.407749 ],\n",
       "        [ 9.968999 ],\n",
       "        [11.530249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.28325  ],\n",
       "        [ 6.8434997],\n",
       "        [ 8.403749 ],\n",
       "        [ 9.963999 ],\n",
       "        [11.524249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2812495],\n",
       "        [ 6.8404994],\n",
       "        [ 8.39975  ],\n",
       "        [ 9.959    ],\n",
       "        [11.5182495]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2792497],\n",
       "        [ 6.8374996],\n",
       "        [ 8.395749 ],\n",
       "        [ 9.9539995],\n",
       "        [11.512249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.27725  ],\n",
       "        [ 6.8344994],\n",
       "        [ 8.391749 ],\n",
       "        [ 9.948999 ],\n",
       "        [11.506249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.27525  ],\n",
       "        [ 6.8314996],\n",
       "        [ 8.38775  ],\n",
       "        [ 9.943999 ],\n",
       "        [11.500249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2732496],\n",
       "        [ 6.8285   ],\n",
       "        [ 8.383749 ],\n",
       "        [ 9.938999 ],\n",
       "        [11.494249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.27125  ],\n",
       "        [ 6.8254995],\n",
       "        [ 8.379749 ],\n",
       "        [ 9.934    ],\n",
       "        [11.488249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.26925  ],\n",
       "        [ 6.8224998],\n",
       "        [ 8.37575  ],\n",
       "        [ 9.929    ],\n",
       "        [11.482249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.26725],\n",
       "        [ 6.8195 ],\n",
       "        [ 8.37175],\n",
       "        [ 9.924  ],\n",
       "        [11.47625]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2652497],\n",
       "        [ 6.8164997],\n",
       "        [ 8.36775  ],\n",
       "        [ 9.919    ],\n",
       "        [11.470249 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.26325  ],\n",
       "        [ 6.8135   ],\n",
       "        [ 8.3637495],\n",
       "        [ 9.914    ],\n",
       "        [11.46425  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.26125 ],\n",
       "        [ 6.8105  ],\n",
       "        [ 8.35975 ],\n",
       "        [ 9.908999],\n",
       "        [11.45825 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.25925],\n",
       "        [ 6.8075 ],\n",
       "        [ 8.35575],\n",
       "        [ 9.904  ],\n",
       "        [11.45225]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.25725 ],\n",
       "        [ 6.8045  ],\n",
       "        [ 8.351749],\n",
       "        [ 9.899   ],\n",
       "        [11.44625 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.25525  ],\n",
       "        [ 6.8015003],\n",
       "        [ 8.34775  ],\n",
       "        [ 9.894    ],\n",
       "        [11.44025  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.25325],\n",
       "        [ 6.7985 ],\n",
       "        [ 8.34375],\n",
       "        [ 9.889  ],\n",
       "        [11.43425]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2512503],\n",
       "        [ 6.7955003],\n",
       "        [ 8.33975  ],\n",
       "        [ 9.884    ],\n",
       "        [11.42825  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.24925 ],\n",
       "        [ 6.7925  ],\n",
       "        [ 8.335751],\n",
       "        [ 9.879   ],\n",
       "        [11.422251]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.24725 ],\n",
       "        [ 6.7895  ],\n",
       "        [ 8.33175 ],\n",
       "        [ 9.874001],\n",
       "        [11.41625 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.24525 ],\n",
       "        [ 6.7865  ],\n",
       "        [ 8.32775 ],\n",
       "        [ 9.869   ],\n",
       "        [11.410251]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2432504],\n",
       "        [ 6.7835   ],\n",
       "        [ 8.3237505],\n",
       "        [ 9.864    ],\n",
       "        [11.40425  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.24125  ],\n",
       "        [ 6.7805004],\n",
       "        [ 8.31975  ],\n",
       "        [ 9.859    ],\n",
       "        [11.398251 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.23925],\n",
       "        [ 6.7775 ],\n",
       "        [ 8.31575],\n",
       "        [ 9.854  ],\n",
       "        [11.39225]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2372503],\n",
       "        [ 6.7745004],\n",
       "        [ 8.31175  ],\n",
       "        [ 9.849001 ],\n",
       "        [11.3862505]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2352505],\n",
       "        [ 6.7715006],\n",
       "        [ 8.307751 ],\n",
       "        [ 9.844001 ],\n",
       "        [11.380251 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.23325  ],\n",
       "        [ 6.7685003],\n",
       "        [ 8.303751 ],\n",
       "        [ 9.839001 ],\n",
       "        [11.37425  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2312503],\n",
       "        [ 6.7655005],\n",
       "        [ 8.29975  ],\n",
       "        [ 9.834001 ],\n",
       "        [11.368251 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2292504],\n",
       "        [ 6.762501 ],\n",
       "        [ 8.295751 ],\n",
       "        [ 9.829    ],\n",
       "        [11.362251 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2272506],\n",
       "        [ 6.7595005],\n",
       "        [ 8.291751 ],\n",
       "        [ 9.824001 ],\n",
       "        [11.356251 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2252502],\n",
       "        [ 6.7565007],\n",
       "        [ 8.28775  ],\n",
       "        [ 9.819001 ],\n",
       "        [11.350251 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2232504],\n",
       "        [ 6.753501 ],\n",
       "        [ 8.283751 ],\n",
       "        [ 9.814001 ],\n",
       "        [11.344252 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2212505],\n",
       "        [ 6.7505007],\n",
       "        [ 8.279751 ],\n",
       "        [ 9.809001 ],\n",
       "        [11.338251 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2192507],\n",
       "        [ 6.747501 ],\n",
       "        [ 8.275751 ],\n",
       "        [ 9.804001 ],\n",
       "        [11.332252 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2172503],\n",
       "        [ 6.7445006],\n",
       "        [ 8.271751 ],\n",
       "        [ 9.799002 ],\n",
       "        [11.326252 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2152505],\n",
       "        [ 6.741501 ],\n",
       "        [ 8.267751 ],\n",
       "        [ 9.794002 ],\n",
       "        [11.320251 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2132506],\n",
       "        [ 6.7385006],\n",
       "        [ 8.263751 ],\n",
       "        [ 9.789001 ],\n",
       "        [11.314252 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.211251],\n",
       "        [ 6.735501],\n",
       "        [ 8.259751],\n",
       "        [ 9.784001],\n",
       "        [11.308251]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2092505],\n",
       "        [ 6.732501 ],\n",
       "        [ 8.255751 ],\n",
       "        [ 9.779001 ],\n",
       "        [11.302252 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2072506],\n",
       "        [ 6.729501 ],\n",
       "        [ 8.251751 ],\n",
       "        [ 9.774002 ],\n",
       "        [11.296251 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2052507],\n",
       "        [ 6.726501 ],\n",
       "        [ 8.247751 ],\n",
       "        [ 9.769002 ],\n",
       "        [11.290252 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.203251],\n",
       "        [ 6.723501],\n",
       "        [ 8.243752],\n",
       "        [ 9.764002],\n",
       "        [11.284252]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.2012506],\n",
       "        [ 6.720501 ],\n",
       "        [ 8.239752 ],\n",
       "        [ 9.759002 ],\n",
       "        [11.278252 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1992507],\n",
       "        [ 6.717501 ],\n",
       "        [ 8.235751 ],\n",
       "        [ 9.754002 ],\n",
       "        [11.272252 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.197251 ],\n",
       "        [ 6.7145014],\n",
       "        [ 8.231751 ],\n",
       "        [ 9.7490015],\n",
       "        [11.2662525]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.195251],\n",
       "        [ 6.711501],\n",
       "        [ 8.227752],\n",
       "        [ 9.744002],\n",
       "        [11.260252]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1932507],\n",
       "        [ 6.7085013],\n",
       "        [ 8.223751 ],\n",
       "        [ 9.739002 ],\n",
       "        [11.254252 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.191251 ],\n",
       "        [ 6.7055016],\n",
       "        [ 8.219751 ],\n",
       "        [ 9.734002 ],\n",
       "        [11.248253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.189251 ],\n",
       "        [ 6.7025013],\n",
       "        [ 8.215752 ],\n",
       "        [ 9.729002 ],\n",
       "        [11.242252 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.187251 ],\n",
       "        [ 6.6995015],\n",
       "        [ 8.211752 ],\n",
       "        [ 9.724002 ],\n",
       "        [11.236253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1852508],\n",
       "        [ 6.6965013],\n",
       "        [ 8.207752 ],\n",
       "        [ 9.719002 ],\n",
       "        [11.230253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.183251 ],\n",
       "        [ 6.6935015],\n",
       "        [ 8.203752 ],\n",
       "        [ 9.714003 ],\n",
       "        [11.224253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.181251 ],\n",
       "        [ 6.690501 ],\n",
       "        [ 8.199752 ],\n",
       "        [ 9.7090025],\n",
       "        [11.218253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.179251 ],\n",
       "        [ 6.6875014],\n",
       "        [ 8.195752 ],\n",
       "        [ 9.704002 ],\n",
       "        [11.212253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.177251 ],\n",
       "        [ 6.6845016],\n",
       "        [ 8.1917515],\n",
       "        [ 9.699002 ],\n",
       "        [11.206253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.175251 ],\n",
       "        [ 6.6815014],\n",
       "        [ 8.187752 ],\n",
       "        [ 9.694002 ],\n",
       "        [11.200253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.173251 ],\n",
       "        [ 6.6785016],\n",
       "        [ 8.183752 ],\n",
       "        [ 9.689003 ],\n",
       "        [11.194253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1712513],\n",
       "        [ 6.675502 ],\n",
       "        [ 8.179752 ],\n",
       "        [ 9.684003 ],\n",
       "        [11.188253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.169251 ],\n",
       "        [ 6.6725016],\n",
       "        [ 8.175753 ],\n",
       "        [ 9.679003 ],\n",
       "        [11.182253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.167251],\n",
       "        [ 6.669502],\n",
       "        [ 8.171752],\n",
       "        [ 9.674003],\n",
       "        [11.176253]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1652513],\n",
       "        [ 6.666502 ],\n",
       "        [ 8.167752 ],\n",
       "        [ 9.669003 ],\n",
       "        [11.170254 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1632514],\n",
       "        [ 6.6635017],\n",
       "        [ 8.163753 ],\n",
       "        [ 9.664003 ],\n",
       "        [11.164253 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.161251],\n",
       "        [ 6.660502],\n",
       "        [ 8.159752],\n",
       "        [ 9.659003],\n",
       "        [11.158254]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.159251],\n",
       "        [ 6.657502],\n",
       "        [ 8.155752],\n",
       "        [ 9.654003],\n",
       "        [11.152254]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1572514],\n",
       "        [ 6.654502 ],\n",
       "        [ 8.151752 ],\n",
       "        [ 9.649003 ],\n",
       "        [11.146254 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1552515],\n",
       "        [ 6.651502 ],\n",
       "        [ 8.147753 ],\n",
       "        [ 9.644003 ],\n",
       "        [11.140254 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.153251],\n",
       "        [ 6.648502],\n",
       "        [ 8.143753],\n",
       "        [ 9.639004],\n",
       "        [11.134254]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1512513],\n",
       "        [ 6.645502 ],\n",
       "        [ 8.139752 ],\n",
       "        [ 9.634004 ],\n",
       "        [11.128254 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1492515],\n",
       "        [ 6.642502 ],\n",
       "        [ 8.135753 ],\n",
       "        [ 9.629004 ],\n",
       "        [11.122254 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1472516],\n",
       "        [ 6.639502 ],\n",
       "        [ 8.131753 ],\n",
       "        [ 9.624003 ],\n",
       "        [11.116254 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1452513],\n",
       "        [ 6.6365023],\n",
       "        [ 8.127752 ],\n",
       "        [ 9.619003 ],\n",
       "        [11.110254 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1432514],\n",
       "        [ 6.633502 ],\n",
       "        [ 8.123753 ],\n",
       "        [ 9.614004 ],\n",
       "        [11.104254 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1412516],\n",
       "        [ 6.630502 ],\n",
       "        [ 8.119753 ],\n",
       "        [ 9.609004 ],\n",
       "        [11.098254 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1392517],\n",
       "        [ 6.6275024],\n",
       "        [ 8.115753 ],\n",
       "        [ 9.604004 ],\n",
       "        [11.092255 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1372514],\n",
       "        [ 6.624502 ],\n",
       "        [ 8.111753 ],\n",
       "        [ 9.599004 ],\n",
       "        [11.086254 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1352515],\n",
       "        [ 6.6215024],\n",
       "        [ 8.107753 ],\n",
       "        [ 9.594004 ],\n",
       "        [11.080255 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1332517],\n",
       "        [ 6.6185026],\n",
       "        [ 8.103753 ],\n",
       "        [ 9.589004 ],\n",
       "        [11.074255 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.131252 ],\n",
       "        [ 6.6155024],\n",
       "        [ 8.099753 ],\n",
       "        [ 9.584004 ],\n",
       "        [11.068254 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1292515],\n",
       "        [ 6.6125026],\n",
       "        [ 8.095753 ],\n",
       "        [ 9.579004 ],\n",
       "        [11.062255 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1272516],\n",
       "        [ 6.609503 ],\n",
       "        [ 8.091753 ],\n",
       "        [ 9.574004 ],\n",
       "        [11.056255 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.125252 ],\n",
       "        [ 6.6065025],\n",
       "        [ 8.087753 ],\n",
       "        [ 9.569004 ],\n",
       "        [11.050255 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.123252 ],\n",
       "        [ 6.6035028],\n",
       "        [ 8.083754 ],\n",
       "        [ 9.564004 ],\n",
       "        [11.044255 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1212516],\n",
       "        [ 6.6005025],\n",
       "        [ 8.079754 ],\n",
       "        [ 9.559004 ],\n",
       "        [11.038256 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1192517],\n",
       "        [ 6.5975027],\n",
       "        [ 8.075753 ],\n",
       "        [ 9.554005 ],\n",
       "        [11.032255 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.117252 ],\n",
       "        [ 6.5945024],\n",
       "        [ 8.0717535],\n",
       "        [ 9.549005 ],\n",
       "        [11.026256 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.115252 ],\n",
       "        [ 6.5915027],\n",
       "        [ 8.067754 ],\n",
       "        [ 9.544004 ],\n",
       "        [11.020255 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.1132517],\n",
       "        [ 6.588503 ],\n",
       "        [ 8.063753 ],\n",
       "        [ 9.539004 ],\n",
       "        [11.014256 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.111252 ],\n",
       "        [ 6.5855026],\n",
       "        [ 8.059753 ],\n",
       "        [ 9.534004 ],\n",
       "        [11.008255 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.109252],\n",
       "        [ 6.582503],\n",
       "        [ 8.055754],\n",
       "        [ 9.529005],\n",
       "        [11.002255]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.107252],\n",
       "        [ 6.579503],\n",
       "        [ 8.051754],\n",
       "        [ 9.524005],\n",
       "        [10.996256]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.105252],\n",
       "        [ 6.576503],\n",
       "        [ 8.047754],\n",
       "        [ 9.519005],\n",
       "        [10.990255]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.103252],\n",
       "        [ 6.573503],\n",
       "        [ 8.043754],\n",
       "        [ 9.514005],\n",
       "        [10.984256]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.101252],\n",
       "        [ 6.570503],\n",
       "        [ 8.039754],\n",
       "        [ 9.509005],\n",
       "        [10.978256]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.099252],\n",
       "        [ 6.567503],\n",
       "        [ 8.035754],\n",
       "        [ 9.504005],\n",
       "        [10.972256]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.097252],\n",
       "        [ 6.564503],\n",
       "        [ 8.031754],\n",
       "        [ 9.499005],\n",
       "        [10.966256]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.095252 ],\n",
       "        [ 6.5615034],\n",
       "        [ 8.027754 ],\n",
       "        [ 9.494005 ],\n",
       "        [10.960257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.093252],\n",
       "        [ 6.558503],\n",
       "        [ 8.023754],\n",
       "        [ 9.489005],\n",
       "        [10.954256]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0912523],\n",
       "        [ 6.5555034],\n",
       "        [ 8.019754 ],\n",
       "        [ 9.484005 ],\n",
       "        [10.9482565]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.089252],\n",
       "        [ 6.552503],\n",
       "        [ 8.015755],\n",
       "        [ 9.479006],\n",
       "        [10.942257]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.087252 ],\n",
       "        [ 6.5495033],\n",
       "        [ 8.011754 ],\n",
       "        [ 9.474006 ],\n",
       "        [10.936256 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0852523],\n",
       "        [ 6.546503 ],\n",
       "        [ 8.007754 ],\n",
       "        [ 9.469006 ],\n",
       "        [10.930257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0832524],\n",
       "        [ 6.5435033],\n",
       "        [ 8.003755 ],\n",
       "        [ 9.464005 ],\n",
       "        [10.924256 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.081252 ],\n",
       "        [ 6.5405035],\n",
       "        [ 7.9997544],\n",
       "        [ 9.459005 ],\n",
       "        [10.918257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0792522],\n",
       "        [ 6.5375032],\n",
       "        [ 7.9957547],\n",
       "        [ 9.454006 ],\n",
       "        [10.912256 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0772524],\n",
       "        [ 6.5345035],\n",
       "        [ 7.9917545],\n",
       "        [ 9.449006 ],\n",
       "        [10.906257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0752525],\n",
       "        [ 6.5315037],\n",
       "        [ 7.987755 ],\n",
       "        [ 9.444006 ],\n",
       "        [10.900257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.073252 ],\n",
       "        [ 6.5285034],\n",
       "        [ 7.9837546],\n",
       "        [ 9.439006 ],\n",
       "        [10.894257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0712523],\n",
       "        [ 6.5255036],\n",
       "        [ 7.979755 ],\n",
       "        [ 9.434006 ],\n",
       "        [10.888257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0692525],\n",
       "        [ 6.522504 ],\n",
       "        [ 7.9757547],\n",
       "        [ 9.429006 ],\n",
       "        [10.882257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0672526],\n",
       "        [ 6.5195036],\n",
       "        [ 7.971755 ],\n",
       "        [ 9.424006 ],\n",
       "        [10.876257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0652523],\n",
       "        [ 6.516504 ],\n",
       "        [ 7.967755 ],\n",
       "        [ 9.419006 ],\n",
       "        [10.870257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0632524],\n",
       "        [ 6.513504 ],\n",
       "        [ 7.963755 ],\n",
       "        [ 9.414006 ],\n",
       "        [10.864258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0612526],\n",
       "        [ 6.510504 ],\n",
       "        [ 7.959755 ],\n",
       "        [ 9.409006 ],\n",
       "        [10.858257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0592527],\n",
       "        [ 6.507504 ],\n",
       "        [ 7.955755 ],\n",
       "        [ 9.404006 ],\n",
       "        [10.852258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0572524],\n",
       "        [ 6.5045037],\n",
       "        [ 7.951755 ],\n",
       "        [ 9.399006 ],\n",
       "        [10.846258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0552526],\n",
       "        [ 6.501504 ],\n",
       "        [ 7.9477553],\n",
       "        [ 9.394007 ],\n",
       "        [10.840258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0532527],\n",
       "        [ 6.498504 ],\n",
       "        [ 7.943755 ],\n",
       "        [ 9.389007 ],\n",
       "        [10.834258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.051253 ],\n",
       "        [ 6.495504 ],\n",
       "        [ 7.9397554],\n",
       "        [ 9.3840065],\n",
       "        [10.828258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0492525],\n",
       "        [ 6.492504 ],\n",
       "        [ 7.9357553],\n",
       "        [ 9.379006 ],\n",
       "        [10.822258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0472527],\n",
       "        [ 6.489504 ],\n",
       "        [ 7.9317555],\n",
       "        [ 9.374006 ],\n",
       "        [10.816257 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.045253 ],\n",
       "        [ 6.486504 ],\n",
       "        [ 7.9277554],\n",
       "        [ 9.369007 ],\n",
       "        [10.810258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.043253 ],\n",
       "        [ 6.4835043],\n",
       "        [ 7.9237556],\n",
       "        [ 9.364007 ],\n",
       "        [10.804258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0412526],\n",
       "        [ 6.480504 ],\n",
       "        [ 7.9197555],\n",
       "        [ 9.359007 ],\n",
       "        [10.798258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0392528],\n",
       "        [ 6.4775043],\n",
       "        [ 7.9157557],\n",
       "        [ 9.354007 ],\n",
       "        [10.792258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.037253 ],\n",
       "        [ 6.4745045],\n",
       "        [ 7.9117556],\n",
       "        [ 9.349007 ],\n",
       "        [10.786259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.035253 ],\n",
       "        [ 6.471504 ],\n",
       "        [ 7.907756 ],\n",
       "        [ 9.3440075],\n",
       "        [10.780258 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0332527],\n",
       "        [ 6.4685044],\n",
       "        [ 7.9037557],\n",
       "        [ 9.339007 ],\n",
       "        [10.774259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.031253],\n",
       "        [ 6.465504],\n",
       "        [ 7.899756],\n",
       "        [ 9.334007],\n",
       "        [10.768259]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.029253 ],\n",
       "        [ 6.4625044],\n",
       "        [ 7.895756 ],\n",
       "        [ 9.329007 ],\n",
       "        [10.762259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.027253 ],\n",
       "        [ 6.4595046],\n",
       "        [ 7.891756 ],\n",
       "        [ 9.324007 ],\n",
       "        [10.756259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.025253 ],\n",
       "        [ 6.4565043],\n",
       "        [ 7.887756 ],\n",
       "        [ 9.319008 ],\n",
       "        [10.750259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.023253 ],\n",
       "        [ 6.4535046],\n",
       "        [ 7.883756 ],\n",
       "        [ 9.314008 ],\n",
       "        [10.744259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.021253],\n",
       "        [ 6.450505],\n",
       "        [ 7.879756],\n",
       "        [ 9.309008],\n",
       "        [10.738259]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0192533],\n",
       "        [ 6.4475045],\n",
       "        [ 7.8757563],\n",
       "        [ 9.304008 ],\n",
       "        [10.732259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.017253 ],\n",
       "        [ 6.4445047],\n",
       "        [ 7.871756 ],\n",
       "        [ 9.299007 ],\n",
       "        [10.726259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.015253 ],\n",
       "        [ 6.4415045],\n",
       "        [ 7.8677564],\n",
       "        [ 9.294008 ],\n",
       "        [10.720259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.013253 ],\n",
       "        [ 6.4385047],\n",
       "        [ 7.863756 ],\n",
       "        [ 9.289008 ],\n",
       "        [10.714259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0112534],\n",
       "        [ 6.435505 ],\n",
       "        [ 7.8597565],\n",
       "        [ 9.284008 ],\n",
       "        [10.70826  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.009253 ],\n",
       "        [ 6.4325047],\n",
       "        [ 7.8557563],\n",
       "        [ 9.279008 ],\n",
       "        [10.702259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.007253 ],\n",
       "        [ 6.429505 ],\n",
       "        [ 7.8517566],\n",
       "        [ 9.274008 ],\n",
       "        [10.6962595]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0052533],\n",
       "        [ 6.426505 ],\n",
       "        [ 7.8477564],\n",
       "        [ 9.269008 ],\n",
       "        [10.69026  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.0032535],\n",
       "        [ 6.423505 ],\n",
       "        [ 7.8437567],\n",
       "        [ 9.2640085],\n",
       "        [10.684259 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.001253 ],\n",
       "        [ 6.420505 ],\n",
       "        [ 7.8397565],\n",
       "        [ 9.259008 ],\n",
       "        [10.67826  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9992533],\n",
       "        [ 6.417505 ],\n",
       "        [ 7.835757 ],\n",
       "        [ 9.254008 ],\n",
       "        [10.67226  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9972534],\n",
       "        [ 6.414505 ],\n",
       "        [ 7.8317566],\n",
       "        [ 9.249008 ],\n",
       "        [10.66626  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9952536],\n",
       "        [ 6.411505 ],\n",
       "        [ 7.827757 ],\n",
       "        [ 9.244008 ],\n",
       "        [10.66026  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.993253 ],\n",
       "        [ 6.408505 ],\n",
       "        [ 7.8237567],\n",
       "        [ 9.239008 ],\n",
       "        [10.654261 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9912534],\n",
       "        [ 6.405505 ],\n",
       "        [ 7.819757 ],\n",
       "        [ 9.234009 ],\n",
       "        [10.64826  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9892535],\n",
       "        [ 6.4025054],\n",
       "        [ 7.815757 ],\n",
       "        [ 9.229009 ],\n",
       "        [10.642261 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9872537],\n",
       "        [ 6.399505 ],\n",
       "        [ 7.811757 ],\n",
       "        [ 9.224009 ],\n",
       "        [10.63626  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9852533],\n",
       "        [ 6.3965054],\n",
       "        [ 7.807757 ],\n",
       "        [ 9.219008 ],\n",
       "        [10.63026  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9832535],\n",
       "        [ 6.393505 ],\n",
       "        [ 7.803757 ],\n",
       "        [ 9.214008 ],\n",
       "        [10.62426  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9812536],\n",
       "        [ 6.3905053],\n",
       "        [ 7.799757 ],\n",
       "        [ 9.209009 ],\n",
       "        [10.61826  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.979254 ],\n",
       "        [ 6.3875055],\n",
       "        [ 7.7957573],\n",
       "        [ 9.204009 ],\n",
       "        [10.612261 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9772534],\n",
       "        [ 6.3845053],\n",
       "        [ 7.791757 ],\n",
       "        [ 9.199009 ],\n",
       "        [10.60626  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9752536],\n",
       "        [ 6.3815055],\n",
       "        [ 7.7877574],\n",
       "        [ 9.194009 ],\n",
       "        [10.600261 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9732537],\n",
       "        [ 6.3785057],\n",
       "        [ 7.783757 ],\n",
       "        [ 9.189009 ],\n",
       "        [10.594261 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.971254 ],\n",
       "        [ 6.3755054],\n",
       "        [ 7.7797575],\n",
       "        [ 9.18401  ],\n",
       "        [10.588261 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9692535],\n",
       "        [ 6.3725057],\n",
       "        [ 7.7757573],\n",
       "        [ 9.179009 ],\n",
       "        [10.582261 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9672537],\n",
       "        [ 6.3695054],\n",
       "        [ 7.7717576],\n",
       "        [ 9.174009 ],\n",
       "        [10.5762615]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.965254 ],\n",
       "        [ 6.3665056],\n",
       "        [ 7.7677574],\n",
       "        [ 9.169009 ],\n",
       "        [10.570261 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.963254 ],\n",
       "        [ 6.363506 ],\n",
       "        [ 7.7637577],\n",
       "        [ 9.164009 ],\n",
       "        [10.564261 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9612536],\n",
       "        [ 6.3605056],\n",
       "        [ 7.7597575],\n",
       "        [ 9.15901  ],\n",
       "        [10.558262 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.959254],\n",
       "        [ 6.357506],\n",
       "        [ 7.755758],\n",
       "        [ 9.15401 ],\n",
       "        [10.552261]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.957254 ],\n",
       "        [ 6.354506 ],\n",
       "        [ 7.7517576],\n",
       "        [ 9.14901  ],\n",
       "        [10.546262 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.955254 ],\n",
       "        [ 6.3515058],\n",
       "        [ 7.747758 ],\n",
       "        [ 9.14401  ],\n",
       "        [10.540261 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9532537],\n",
       "        [ 6.348506 ],\n",
       "        [ 7.7437577],\n",
       "        [ 9.139009 ],\n",
       "        [10.534262 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.951254 ],\n",
       "        [ 6.3455057],\n",
       "        [ 7.739758 ],\n",
       "        [ 9.13401  ],\n",
       "        [10.528262 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.949254],\n",
       "        [ 6.342506],\n",
       "        [ 7.735758],\n",
       "        [ 9.12901 ],\n",
       "        [10.522262]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.947254],\n",
       "        [ 6.339506],\n",
       "        [ 7.731758],\n",
       "        [ 9.12401 ],\n",
       "        [10.516262]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.945254],\n",
       "        [ 6.336506],\n",
       "        [ 7.727758],\n",
       "        [ 9.11901 ],\n",
       "        [10.510262]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.943254],\n",
       "        [ 6.333506],\n",
       "        [ 7.723758],\n",
       "        [ 9.11401 ],\n",
       "        [10.504262]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.941254 ],\n",
       "        [ 6.3305063],\n",
       "        [ 7.719758 ],\n",
       "        [ 9.10901  ],\n",
       "        [10.498262 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9392543],\n",
       "        [ 6.327506 ],\n",
       "        [ 7.7157583],\n",
       "        [ 9.104011 ],\n",
       "        [10.492262 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.937254 ],\n",
       "        [ 6.3245063],\n",
       "        [ 7.711758 ],\n",
       "        [ 9.09901  ],\n",
       "        [10.486262 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.935254 ],\n",
       "        [ 6.321506 ],\n",
       "        [ 7.7077584],\n",
       "        [ 9.09401  ],\n",
       "        [10.480263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9332542],\n",
       "        [ 6.3185062],\n",
       "        [ 7.7037582],\n",
       "        [ 9.08901  ],\n",
       "        [10.474262 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9312544],\n",
       "        [ 6.3155065],\n",
       "        [ 7.6997585],\n",
       "        [ 9.08401  ],\n",
       "        [10.468263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.929254 ],\n",
       "        [ 6.312506 ],\n",
       "        [ 7.6957583],\n",
       "        [ 9.07901  ],\n",
       "        [10.462263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.927254 ],\n",
       "        [ 6.3095064],\n",
       "        [ 7.6917586],\n",
       "        [ 9.074011 ],\n",
       "        [10.456263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9252543],\n",
       "        [ 6.3065066],\n",
       "        [ 7.6877584],\n",
       "        [ 9.069011 ],\n",
       "        [10.450263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9232545],\n",
       "        [ 6.3035064],\n",
       "        [ 7.6837587],\n",
       "        [ 9.064011 ],\n",
       "        [10.4442625]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.921254 ],\n",
       "        [ 6.3005066],\n",
       "        [ 7.6797585],\n",
       "        [ 9.0590105],\n",
       "        [10.438263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9192543],\n",
       "        [ 6.2975063],\n",
       "        [ 7.675759 ],\n",
       "        [ 9.05401  ],\n",
       "        [10.432263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9172544],\n",
       "        [ 6.2945065],\n",
       "        [ 7.6717587],\n",
       "        [ 9.049011 ],\n",
       "        [10.426263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9152546],\n",
       "        [ 6.291507 ],\n",
       "        [ 7.667759 ],\n",
       "        [ 9.044011 ],\n",
       "        [10.420263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9132543],\n",
       "        [ 6.2885065],\n",
       "        [ 7.6637588],\n",
       "        [ 9.039011 ],\n",
       "        [10.414263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9112544],\n",
       "        [ 6.2855067],\n",
       "        [ 7.659759 ],\n",
       "        [ 9.034011 ],\n",
       "        [10.408263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9092546],\n",
       "        [ 6.282507 ],\n",
       "        [ 7.655759 ],\n",
       "        [ 9.029011 ],\n",
       "        [10.402264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9072547],\n",
       "        [ 6.2795067],\n",
       "        [ 7.651759 ],\n",
       "        [ 9.024012 ],\n",
       "        [10.396263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9052544],\n",
       "        [ 6.276507 ],\n",
       "        [ 7.647759 ],\n",
       "        [ 9.0190115],\n",
       "        [10.390264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9032545],\n",
       "        [ 6.2735066],\n",
       "        [ 7.6437593],\n",
       "        [ 9.014011 ],\n",
       "        [10.384264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.9012547],\n",
       "        [ 6.270507 ],\n",
       "        [ 7.639759 ],\n",
       "        [ 9.009011 ],\n",
       "        [10.378263 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.899255 ],\n",
       "        [ 6.267507 ],\n",
       "        [ 7.6357594],\n",
       "        [ 9.004011 ],\n",
       "        [10.372264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8972545],\n",
       "        [ 6.264507 ],\n",
       "        [ 7.631759 ],\n",
       "        [ 8.999012 ],\n",
       "        [10.366264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8952546],\n",
       "        [ 6.261507 ],\n",
       "        [ 7.6277595],\n",
       "        [ 8.994012 ],\n",
       "        [10.360264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8932548],\n",
       "        [ 6.2585073],\n",
       "        [ 7.6237593],\n",
       "        [ 8.989012 ],\n",
       "        [10.354264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.891255 ],\n",
       "        [ 6.255507 ],\n",
       "        [ 7.6197596],\n",
       "        [ 8.984012 ],\n",
       "        [10.348264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8892546],\n",
       "        [ 6.252507 ],\n",
       "        [ 7.6157594],\n",
       "        [ 8.979012 ],\n",
       "        [10.342264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8872547],\n",
       "        [ 6.249507 ],\n",
       "        [ 7.6117597],\n",
       "        [ 8.974012 ],\n",
       "        [10.336265 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.885255 ],\n",
       "        [ 6.246507 ],\n",
       "        [ 7.6077595],\n",
       "        [ 8.969012 ],\n",
       "        [10.330264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.883255 ],\n",
       "        [ 6.2435074],\n",
       "        [ 7.60376  ],\n",
       "        [ 8.964012 ],\n",
       "        [10.324265 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8812547],\n",
       "        [ 6.240507 ],\n",
       "        [ 7.5997596],\n",
       "        [ 8.959012 ],\n",
       "        [10.318264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.879255 ],\n",
       "        [ 6.2375073],\n",
       "        [ 7.59576  ],\n",
       "        [ 8.954012 ],\n",
       "        [10.312264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.877255 ],\n",
       "        [ 6.2345076],\n",
       "        [ 7.5917597],\n",
       "        [ 8.949012 ],\n",
       "        [10.306265 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.875255 ],\n",
       "        [ 6.2315073],\n",
       "        [ 7.58776  ],\n",
       "        [ 8.944013 ],\n",
       "        [10.300264 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.873255 ],\n",
       "        [ 6.2285075],\n",
       "        [ 7.58376  ],\n",
       "        [ 8.939013 ],\n",
       "        [10.294265 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.871255 ],\n",
       "        [ 6.2255073],\n",
       "        [ 7.57976  ],\n",
       "        [ 8.934012 ],\n",
       "        [10.288265 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.869255 ],\n",
       "        [ 6.2225075],\n",
       "        [ 7.57576  ],\n",
       "        [ 8.929012 ],\n",
       "        [10.282265 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.867255 ],\n",
       "        [ 6.2195077],\n",
       "        [ 7.57176  ],\n",
       "        [ 8.924012 ],\n",
       "        [10.276265 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.865255 ],\n",
       "        [ 6.2165074],\n",
       "        [ 7.56776  ],\n",
       "        [ 8.919012 ],\n",
       "        [10.270266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.863255 ],\n",
       "        [ 6.2135077],\n",
       "        [ 7.5637603],\n",
       "        [ 8.914013 ],\n",
       "        [10.264265 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.861255 ],\n",
       "        [ 6.210508 ],\n",
       "        [ 7.55976  ],\n",
       "        [ 8.909013 ],\n",
       "        [10.2582655]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8592553],\n",
       "        [ 6.2075076],\n",
       "        [ 7.5557604],\n",
       "        [ 8.904013 ],\n",
       "        [10.252265 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.857255],\n",
       "        [ 6.204508],\n",
       "        [ 7.55176 ],\n",
       "        [ 8.899013],\n",
       "        [10.246265]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.855255 ],\n",
       "        [ 6.2015076],\n",
       "        [ 7.5477605],\n",
       "        [ 8.894012 ],\n",
       "        [10.240266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8532553],\n",
       "        [ 6.198508 ],\n",
       "        [ 7.5437603],\n",
       "        [ 8.889013 ],\n",
       "        [10.234265 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8512554],\n",
       "        [ 6.195508 ],\n",
       "        [ 7.5397606],\n",
       "        [ 8.884013 ],\n",
       "        [10.228266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.849255 ],\n",
       "        [ 6.1925077],\n",
       "        [ 7.5357604],\n",
       "        [ 8.879013 ],\n",
       "        [10.222265 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.847255 ],\n",
       "        [ 6.189508 ],\n",
       "        [ 7.5317607],\n",
       "        [ 8.874013 ],\n",
       "        [10.216266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8452554],\n",
       "        [ 6.186508 ],\n",
       "        [ 7.5277605],\n",
       "        [ 8.869013 ],\n",
       "        [10.210266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8432555],\n",
       "        [ 6.183508 ],\n",
       "        [ 7.523761 ],\n",
       "        [ 8.864014 ],\n",
       "        [10.204266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.841255 ],\n",
       "        [ 6.180508 ],\n",
       "        [ 7.5197606],\n",
       "        [ 8.859014 ],\n",
       "        [10.198266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8392553],\n",
       "        [ 6.177508 ],\n",
       "        [ 7.515761 ],\n",
       "        [ 8.854013 ],\n",
       "        [10.192266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8372555],\n",
       "        [ 6.174508 ],\n",
       "        [ 7.5117607],\n",
       "        [ 8.849013 ],\n",
       "        [10.186266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8352556],\n",
       "        [ 6.1715083],\n",
       "        [ 7.507761 ],\n",
       "        [ 8.844013 ],\n",
       "        [10.180266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8332553],\n",
       "        [ 6.168508 ],\n",
       "        [ 7.503761 ],\n",
       "        [ 8.839014 ],\n",
       "        [10.174267 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8312554],\n",
       "        [ 6.1655083],\n",
       "        [ 7.499761 ],\n",
       "        [ 8.834014 ],\n",
       "        [10.168266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8292556],\n",
       "        [ 6.162508 ],\n",
       "        [ 7.495761 ],\n",
       "        [ 8.829014 ],\n",
       "        [10.162267 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8272557],\n",
       "        [ 6.159508 ],\n",
       "        [ 7.491761 ],\n",
       "        [ 8.824014 ],\n",
       "        [10.156266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8252554],\n",
       "        [ 6.1565084],\n",
       "        [ 7.487761 ],\n",
       "        [ 8.819014 ],\n",
       "        [10.150267 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8232555],\n",
       "        [ 6.153508 ],\n",
       "        [ 7.4837613],\n",
       "        [ 8.8140135],\n",
       "        [10.144266 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8212557],\n",
       "        [ 6.1505084],\n",
       "        [ 7.479761 ],\n",
       "        [ 8.809014 ],\n",
       "        [10.138267 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.819256 ],\n",
       "        [ 6.1475086],\n",
       "        [ 7.4757614],\n",
       "        [ 8.804014 ],\n",
       "        [10.132267 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8172555],\n",
       "        [ 6.1445084],\n",
       "        [ 7.471761 ],\n",
       "        [ 8.799014 ],\n",
       "        [10.1262665]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8152556],\n",
       "        [ 6.1415086],\n",
       "        [ 7.4677615],\n",
       "        [ 8.794014 ],\n",
       "        [10.120267 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.813256 ],\n",
       "        [ 6.138509 ],\n",
       "        [ 7.4637613],\n",
       "        [ 8.789014 ],\n",
       "        [10.114267 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.811256 ],\n",
       "        [ 6.1355085],\n",
       "        [ 7.4597616],\n",
       "        [ 8.784015 ],\n",
       "        [10.108267 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8092556],\n",
       "        [ 6.1325088],\n",
       "        [ 7.4557614],\n",
       "        [ 8.779015 ],\n",
       "        [10.102267 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8072557],\n",
       "        [ 6.1295085],\n",
       "        [ 7.4517617],\n",
       "        [ 8.774014 ],\n",
       "        [10.096268 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.805256 ],\n",
       "        [ 6.1265087],\n",
       "        [ 7.4477615],\n",
       "        [ 8.769014 ],\n",
       "        [10.090267 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.803256],\n",
       "        [ 6.123509],\n",
       "        [ 7.443762],\n",
       "        [ 8.764014],\n",
       "        [10.084268]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.8012557],\n",
       "        [ 6.1205087],\n",
       "        [ 7.4397616],\n",
       "        [ 8.759014 ],\n",
       "        [10.078268 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.799256],\n",
       "        [ 6.117509],\n",
       "        [ 7.435762],\n",
       "        [ 8.754015],\n",
       "        [10.072268]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.797256 ],\n",
       "        [ 6.1145086],\n",
       "        [ 7.4317617],\n",
       "        [ 8.749015 ],\n",
       "        [10.066268 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.795256],\n",
       "        [ 6.111509],\n",
       "        [ 7.427762],\n",
       "        [ 8.744015],\n",
       "        [10.060267]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.793256],\n",
       "        [ 6.108509],\n",
       "        [ 7.423762],\n",
       "        [ 8.739015],\n",
       "        [10.054268]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.791256 ],\n",
       "        [ 6.105509 ],\n",
       "        [ 7.419762 ],\n",
       "        [ 8.7340145],\n",
       "        [10.048267 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.789256],\n",
       "        [ 6.102509],\n",
       "        [ 7.415762],\n",
       "        [ 8.729015],\n",
       "        [10.042268]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.7872562],\n",
       "        [ 6.0995092],\n",
       "        [ 7.411762 ],\n",
       "        [ 8.724015 ],\n",
       "        [10.036268 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.785256],\n",
       "        [ 6.096509],\n",
       "        [ 7.407762],\n",
       "        [ 8.719015],\n",
       "        [10.030268]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.783256 ],\n",
       "        [ 6.093509 ],\n",
       "        [ 7.4037623],\n",
       "        [ 8.714015 ],\n",
       "        [10.024268 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.781256 ],\n",
       "        [ 6.0905094],\n",
       "        [ 7.399762 ],\n",
       "        [ 8.709015 ],\n",
       "        [10.018269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.7792563],\n",
       "        [ 6.087509 ],\n",
       "        [ 7.3957624],\n",
       "        [ 8.704016 ],\n",
       "        [10.012268 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.777256 ],\n",
       "        [ 6.0845094],\n",
       "        [ 7.3917623],\n",
       "        [ 8.699016 ],\n",
       "        [10.0062685]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 4.775256 ],\n",
       "        [ 6.081509 ],\n",
       "        [ 7.3877625],\n",
       "        [ 8.6940155],\n",
       "        [10.000269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7732563],\n",
       "        [6.0785093],\n",
       "        [7.3837624],\n",
       "        [8.689015 ],\n",
       "        [9.994268 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7712564],\n",
       "        [6.0755095],\n",
       "        [7.3797626],\n",
       "        [8.684015 ],\n",
       "        [9.988269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.769256 ],\n",
       "        [6.0725093],\n",
       "        [7.3757625],\n",
       "        [8.679016 ],\n",
       "        [9.982269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7672563],\n",
       "        [6.0695095],\n",
       "        [7.3717628],\n",
       "        [8.674016 ],\n",
       "        [9.976269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7652564],\n",
       "        [6.0665092],\n",
       "        [7.3677626],\n",
       "        [8.669016 ],\n",
       "        [9.970269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7632565],\n",
       "        [6.0635095],\n",
       "        [7.363763 ],\n",
       "        [8.664016 ],\n",
       "        [9.964269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.761256 ],\n",
       "        [6.0605097],\n",
       "        [7.3597627],\n",
       "        [8.659016 ],\n",
       "        [9.958269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7592564],\n",
       "        [6.0575094],\n",
       "        [7.355763 ],\n",
       "        [8.654016 ],\n",
       "        [9.95227  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7572565],\n",
       "        [6.0545096],\n",
       "        [7.351763 ],\n",
       "        [8.649016 ],\n",
       "        [9.946269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7552567],\n",
       "        [6.05151  ],\n",
       "        [7.347763 ],\n",
       "        [8.644016 ],\n",
       "        [9.940269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7532563],\n",
       "        [6.0485096],\n",
       "        [7.343763 ],\n",
       "        [8.639016 ],\n",
       "        [9.934269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7512565],\n",
       "        [6.04551  ],\n",
       "        [7.339763 ],\n",
       "        [8.634016 ],\n",
       "        [9.928269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7492566],\n",
       "        [6.04251  ],\n",
       "        [7.335763 ],\n",
       "        [8.629016 ],\n",
       "        [9.92227  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7472568],\n",
       "        [6.03951  ],\n",
       "        [7.3317633],\n",
       "        [8.624017 ],\n",
       "        [9.916269 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7452564],\n",
       "        [6.03651  ],\n",
       "        [7.327763 ],\n",
       "        [8.619017 ],\n",
       "        [9.91027  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7432566],\n",
       "        [6.0335097],\n",
       "        [7.3237634],\n",
       "        [8.614017 ],\n",
       "        [9.90427  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7412567],\n",
       "        [6.03051  ],\n",
       "        [7.319763 ],\n",
       "        [8.609016 ],\n",
       "        [9.89827  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.739257 ],\n",
       "        [6.02751  ],\n",
       "        [7.3157635],\n",
       "        [8.604016 ],\n",
       "        [9.89227  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7372565],\n",
       "        [6.02451  ],\n",
       "        [7.3117633],\n",
       "        [8.599016 ],\n",
       "        [9.8862705]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7352567],\n",
       "        [6.02151  ],\n",
       "        [7.3077636],\n",
       "        [8.594017 ],\n",
       "        [9.88027  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.733257 ],\n",
       "        [6.01851  ],\n",
       "        [7.3037634],\n",
       "        [8.589017 ],\n",
       "        [9.87427  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.731257 ],\n",
       "        [6.01551  ],\n",
       "        [7.2997637],\n",
       "        [8.584017 ],\n",
       "        [9.86827  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7292566],\n",
       "        [6.0125103],\n",
       "        [7.2957635],\n",
       "        [8.579017 ],\n",
       "        [9.86227  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.727257],\n",
       "        [6.00951 ],\n",
       "        [7.291764],\n",
       "        [8.574017],\n",
       "        [9.856271]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.725257 ],\n",
       "        [6.0065103],\n",
       "        [7.2877636],\n",
       "        [8.569017 ],\n",
       "        [9.85027  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.723257 ],\n",
       "        [6.0035105],\n",
       "        [7.283764 ],\n",
       "        [8.564017 ],\n",
       "        [9.844271 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7212567],\n",
       "        [6.00051  ],\n",
       "        [7.2797637],\n",
       "        [8.559017 ],\n",
       "        [9.83827  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.719257 ],\n",
       "        [5.9975104],\n",
       "        [7.275764 ],\n",
       "        [8.554017 ],\n",
       "        [9.832271 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.717257 ],\n",
       "        [5.9945107],\n",
       "        [7.271764 ],\n",
       "        [8.549017 ],\n",
       "        [9.826271 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.715257 ],\n",
       "        [5.9915104],\n",
       "        [7.267764 ],\n",
       "        [8.544018 ],\n",
       "        [9.820271 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.713257 ],\n",
       "        [5.9885106],\n",
       "        [7.263764 ],\n",
       "        [8.539018 ],\n",
       "        [9.814271 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.711257 ],\n",
       "        [5.9855103],\n",
       "        [7.259764 ],\n",
       "        [8.534018 ],\n",
       "        [9.808271 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.709257 ],\n",
       "        [5.9825106],\n",
       "        [7.255764 ],\n",
       "        [8.529017 ],\n",
       "        [9.802271 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.7072573],\n",
       "        [5.979511 ],\n",
       "        [7.2517643],\n",
       "        [8.524017 ],\n",
       "        [9.796271 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.705257 ],\n",
       "        [5.9765105],\n",
       "        [7.247764 ],\n",
       "        [8.519018 ],\n",
       "        [9.790272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.703257 ],\n",
       "        [5.9735107],\n",
       "        [7.2437644],\n",
       "        [8.514018 ],\n",
       "        [9.784271 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.701257 ],\n",
       "        [5.9705105],\n",
       "        [7.239764 ],\n",
       "        [8.509018 ],\n",
       "        [9.778272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6992574],\n",
       "        [5.9675107],\n",
       "        [7.2357645],\n",
       "        [8.504018 ],\n",
       "        [9.772271 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.697257 ],\n",
       "        [5.964511 ],\n",
       "        [7.2317643],\n",
       "        [8.499018 ],\n",
       "        [9.766272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.695257 ],\n",
       "        [5.9615107],\n",
       "        [7.2277646],\n",
       "        [8.494018 ],\n",
       "        [9.760272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6932573],\n",
       "        [5.958511 ],\n",
       "        [7.2237644],\n",
       "        [8.489018 ],\n",
       "        [9.7542715]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6912575],\n",
       "        [5.955511 ],\n",
       "        [7.2197647],\n",
       "        [8.484018 ],\n",
       "        [9.748272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.689257 ],\n",
       "        [5.952511 ],\n",
       "        [7.2157645],\n",
       "        [8.479018 ],\n",
       "        [9.742271 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6872573],\n",
       "        [5.949511 ],\n",
       "        [7.211765 ],\n",
       "        [8.474018 ],\n",
       "        [9.736272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6852574],\n",
       "        [5.9465113],\n",
       "        [7.2077646],\n",
       "        [8.469018 ],\n",
       "        [9.730272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6832576],\n",
       "        [5.943511 ],\n",
       "        [7.203765 ],\n",
       "        [8.464019 ],\n",
       "        [9.724272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6812572],\n",
       "        [5.940511 ],\n",
       "        [7.1997647],\n",
       "        [8.459019 ],\n",
       "        [9.718272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6792574],\n",
       "        [5.937511 ],\n",
       "        [7.195765 ],\n",
       "        [8.454019 ],\n",
       "        [9.712273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6772575],\n",
       "        [5.934511 ],\n",
       "        [7.191765 ],\n",
       "        [8.4490185],\n",
       "        [9.706272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6752577],\n",
       "        [5.9315114],\n",
       "        [7.187765 ],\n",
       "        [8.444018 ],\n",
       "        [9.700273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6732574],\n",
       "        [5.928511 ],\n",
       "        [7.183765 ],\n",
       "        [8.439018 ],\n",
       "        [9.694273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6712575],\n",
       "        [5.9255114],\n",
       "        [7.179765 ],\n",
       "        [8.434019 ],\n",
       "        [9.688272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6692576],\n",
       "        [5.922511 ],\n",
       "        [7.175765 ],\n",
       "        [8.429019 ],\n",
       "        [9.682273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.667258 ],\n",
       "        [5.9195113],\n",
       "        [7.1717653],\n",
       "        [8.424019 ],\n",
       "        [9.676272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6652575],\n",
       "        [5.9165115],\n",
       "        [7.167765 ],\n",
       "        [8.419019 ],\n",
       "        [9.670273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6632576],\n",
       "        [5.9135113],\n",
       "        [7.1637654],\n",
       "        [8.414019 ],\n",
       "        [9.664273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6612577],\n",
       "        [5.9105115],\n",
       "        [7.1597652],\n",
       "        [8.409019 ],\n",
       "        [9.658273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.659258 ],\n",
       "        [5.9075117],\n",
       "        [7.155765 ],\n",
       "        [8.404019 ],\n",
       "        [9.652273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6572576],\n",
       "        [5.9045115],\n",
       "        [7.1517653],\n",
       "        [8.399019 ],\n",
       "        [9.646273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6552577],\n",
       "        [5.9015117],\n",
       "        [7.147765 ],\n",
       "        [8.394019 ],\n",
       "        [9.640273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.653258 ],\n",
       "        [5.8985114],\n",
       "        [7.1437654],\n",
       "        [8.389019 ],\n",
       "        [9.634273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6512575],\n",
       "        [5.8955116],\n",
       "        [7.1397653],\n",
       "        [8.384019 ],\n",
       "        [9.628273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6492577],\n",
       "        [5.8925114],\n",
       "        [7.135765 ],\n",
       "        [8.379019 ],\n",
       "        [9.6222725]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.647258 ],\n",
       "        [5.8895116],\n",
       "        [7.1317654],\n",
       "        [8.374019 ],\n",
       "        [9.616273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.645258 ],\n",
       "        [5.8865113],\n",
       "        [7.127765 ],\n",
       "        [8.3690195],\n",
       "        [9.610273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6432576],\n",
       "        [5.8835115],\n",
       "        [7.1237655],\n",
       "        [8.364019 ],\n",
       "        [9.604273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.641258 ],\n",
       "        [5.8805118],\n",
       "        [7.1197653],\n",
       "        [8.359019 ],\n",
       "        [9.598273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.639258 ],\n",
       "        [5.8775115],\n",
       "        [7.1157656],\n",
       "        [8.354019 ],\n",
       "        [9.592273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6372576],\n",
       "        [5.8745117],\n",
       "        [7.1117654],\n",
       "        [8.349019 ],\n",
       "        [9.586273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6352577],\n",
       "        [5.8715115],\n",
       "        [7.107765 ],\n",
       "        [8.344019 ],\n",
       "        [9.580273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.633258 ],\n",
       "        [5.8685117],\n",
       "        [7.1037655],\n",
       "        [8.339019 ],\n",
       "        [9.574273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.631258 ],\n",
       "        [5.8655114],\n",
       "        [7.0997653],\n",
       "        [8.334019 ],\n",
       "        [9.568273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6292577],\n",
       "        [5.8625116],\n",
       "        [7.095765 ],\n",
       "        [8.32902  ],\n",
       "        [9.562273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.627258 ],\n",
       "        [5.8595114],\n",
       "        [7.0917654],\n",
       "        [8.324019 ],\n",
       "        [9.5562725]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.625258 ],\n",
       "        [5.8565116],\n",
       "        [7.087765 ],\n",
       "        [8.319019 ],\n",
       "        [9.550273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6232576],\n",
       "        [5.853512 ],\n",
       "        [7.0837655],\n",
       "        [8.314019 ],\n",
       "        [9.544272 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.621258 ],\n",
       "        [5.8505116],\n",
       "        [7.0797653],\n",
       "        [8.309019 ],\n",
       "        [9.538273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.619258 ],\n",
       "        [5.847512 ],\n",
       "        [7.0757656],\n",
       "        [8.304019 ],\n",
       "        [9.532273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.617258 ],\n",
       "        [5.8445115],\n",
       "        [7.0717654],\n",
       "        [8.299019 ],\n",
       "        [9.526273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6152577],\n",
       "        [5.8415117],\n",
       "        [7.067765 ],\n",
       "        [8.294019 ],\n",
       "        [9.520273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.613258 ],\n",
       "        [5.8385115],\n",
       "        [7.0637655],\n",
       "        [8.28902  ],\n",
       "        [9.514273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.611258 ],\n",
       "        [5.8355117],\n",
       "        [7.0597653],\n",
       "        [8.284019 ],\n",
       "        [9.508273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.6092577],\n",
       "        [5.832512 ],\n",
       "        [7.0557656],\n",
       "        [8.279019 ],\n",
       "        [9.502273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.607258 ],\n",
       "        [5.8295116],\n",
       "        [7.0517654],\n",
       "        [8.274019 ],\n",
       "        [9.496273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.605258 ],\n",
       "        [5.826512 ],\n",
       "        [7.0477653],\n",
       "        [8.269019 ],\n",
       "        [9.4902725]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.603258 ],\n",
       "        [5.8235116],\n",
       "        [7.0437655],\n",
       "        [8.264019 ],\n",
       "        [9.484273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.601258 ],\n",
       "        [5.820512 ],\n",
       "        [7.0397654],\n",
       "        [8.259019 ],\n",
       "        [9.478273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.599258 ],\n",
       "        [5.8175116],\n",
       "        [7.0357656],\n",
       "        [8.254019 ],\n",
       "        [9.472273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.597258 ],\n",
       "        [5.814512 ],\n",
       "        [7.0317655],\n",
       "        [8.24902  ],\n",
       "        [9.466273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.595258 ],\n",
       "        [5.811512 ],\n",
       "        [7.0277653],\n",
       "        [8.2440195],\n",
       "        [9.460273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.593258 ],\n",
       "        [5.8085117],\n",
       "        [7.0237656],\n",
       "        [8.239019 ],\n",
       "        [9.454273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.591258 ],\n",
       "        [5.805512 ],\n",
       "        [7.0197654],\n",
       "        [8.234019 ],\n",
       "        [9.448273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.589258 ],\n",
       "        [5.8025117],\n",
       "        [7.0157657],\n",
       "        [8.229019 ],\n",
       "        [9.442273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.587258 ],\n",
       "        [5.799512 ],\n",
       "        [7.0117655],\n",
       "        [8.224019 ],\n",
       "        [9.436273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.585258 ],\n",
       "        [5.7965117],\n",
       "        [7.007766 ],\n",
       "        [8.219019 ],\n",
       "        [9.430273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.583258 ],\n",
       "        [5.793512 ],\n",
       "        [7.0037656],\n",
       "        [8.214019 ],\n",
       "        [9.424273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5812583],\n",
       "        [5.7905116],\n",
       "        [6.9997654],\n",
       "        [8.20902  ],\n",
       "        [9.418273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.579258 ],\n",
       "        [5.787512 ],\n",
       "        [6.9957657],\n",
       "        [8.20402  ],\n",
       "        [9.412273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.577258 ],\n",
       "        [5.784512 ],\n",
       "        [6.9917655],\n",
       "        [8.199019 ],\n",
       "        [9.406273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5752583],\n",
       "        [5.781512 ],\n",
       "        [6.9877653],\n",
       "        [8.194019 ],\n",
       "        [9.400273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5732584],\n",
       "        [5.778512 ],\n",
       "        [6.9837656],\n",
       "        [8.189019 ],\n",
       "        [9.394273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.571258 ],\n",
       "        [5.7755117],\n",
       "        [6.9797654],\n",
       "        [8.184019 ],\n",
       "        [9.388273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.569258 ],\n",
       "        [5.772512 ],\n",
       "        [6.9757657],\n",
       "        [8.179019 ],\n",
       "        [9.382273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5672584],\n",
       "        [5.7695117],\n",
       "        [6.9717655],\n",
       "        [8.174019 ],\n",
       "        [9.376273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.565258],\n",
       "        [5.766512],\n",
       "        [6.967766],\n",
       "        [8.16902 ],\n",
       "        [9.370273]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.563258 ],\n",
       "        [5.7635117],\n",
       "        [6.9637656],\n",
       "        [8.16402  ],\n",
       "        [9.364273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5612583],\n",
       "        [5.760512 ],\n",
       "        [6.9597654],\n",
       "        [8.159019 ],\n",
       "        [9.358273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5592585],\n",
       "        [5.757512 ],\n",
       "        [6.9557657],\n",
       "        [8.154019 ],\n",
       "        [9.352273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.557258 ],\n",
       "        [5.754512 ],\n",
       "        [6.9517655],\n",
       "        [8.149019 ],\n",
       "        [9.346273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5552583],\n",
       "        [5.751512 ],\n",
       "        [6.947766 ],\n",
       "        [8.144019 ],\n",
       "        [9.340273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5532584],\n",
       "        [5.748512 ],\n",
       "        [6.9437656],\n",
       "        [8.139019 ],\n",
       "        [9.334273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.551258 ],\n",
       "        [5.745512 ],\n",
       "        [6.9397655],\n",
       "        [8.134019 ],\n",
       "        [9.328273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.549258 ],\n",
       "        [5.7425117],\n",
       "        [6.9357657],\n",
       "        [8.12902  ],\n",
       "        [9.322273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5472584],\n",
       "        [5.739512 ],\n",
       "        [6.9317656],\n",
       "        [8.12402  ],\n",
       "        [9.316273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5452585],\n",
       "        [5.736512 ],\n",
       "        [6.927766 ],\n",
       "        [8.1190195],\n",
       "        [9.310273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.543258 ],\n",
       "        [5.733512 ],\n",
       "        [6.9237657],\n",
       "        [8.114019 ],\n",
       "        [9.304273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5412583],\n",
       "        [5.730512 ],\n",
       "        [6.9197655],\n",
       "        [8.109019 ],\n",
       "        [9.298273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5392585],\n",
       "        [5.727512 ],\n",
       "        [6.915766 ],\n",
       "        [8.104019 ],\n",
       "        [9.292273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.537258 ],\n",
       "        [5.724512 ],\n",
       "        [6.9117656],\n",
       "        [8.099019 ],\n",
       "        [9.286273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5352583],\n",
       "        [5.721512 ],\n",
       "        [6.907766 ],\n",
       "        [8.094019 ],\n",
       "        [9.280273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5332584],\n",
       "        [5.718512 ],\n",
       "        [6.9037657],\n",
       "        [8.08902  ],\n",
       "        [9.274273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5312586],\n",
       "        [5.7155123],\n",
       "        [6.8997655],\n",
       "        [8.08402  ],\n",
       "        [9.268273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5292583],\n",
       "        [5.712512 ],\n",
       "        [6.895766 ],\n",
       "        [8.07902  ],\n",
       "        [9.262273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5272584],\n",
       "        [5.709512 ],\n",
       "        [6.8917656],\n",
       "        [8.074019 ],\n",
       "        [9.256273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5252585],\n",
       "        [5.706512 ],\n",
       "        [6.887766 ],\n",
       "        [8.069019 ],\n",
       "        [9.250273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.523258 ],\n",
       "        [5.703512 ],\n",
       "        [6.8837657],\n",
       "        [8.064019 ],\n",
       "        [9.244273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5212584],\n",
       "        [5.700512 ],\n",
       "        [6.8797655],\n",
       "        [8.059019 ],\n",
       "        [9.238273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5192585],\n",
       "        [5.697512 ],\n",
       "        [6.875766 ],\n",
       "        [8.054019 ],\n",
       "        [9.232273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5172586],\n",
       "        [5.694512 ],\n",
       "        [6.8717656],\n",
       "        [8.04902  ],\n",
       "        [9.226273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5152583],\n",
       "        [5.691512 ],\n",
       "        [6.867766 ],\n",
       "        [8.04402  ],\n",
       "        [9.220273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5132585],\n",
       "        [5.6885123],\n",
       "        [6.8637657],\n",
       "        [8.03902  ],\n",
       "        [9.214273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5112586],\n",
       "        [5.685512 ],\n",
       "        [6.859766 ],\n",
       "        [8.034019 ],\n",
       "        [9.208273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5092583],\n",
       "        [5.6825123],\n",
       "        [6.855766 ],\n",
       "        [8.029019 ],\n",
       "        [9.202273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5072584],\n",
       "        [5.679512 ],\n",
       "        [6.8517656],\n",
       "        [8.024019 ],\n",
       "        [9.196273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5052586],\n",
       "        [5.6765122],\n",
       "        [6.847766 ],\n",
       "        [8.019019 ],\n",
       "        [9.190273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5032587],\n",
       "        [5.673512 ],\n",
       "        [6.8437657],\n",
       "        [8.014019 ],\n",
       "        [9.184273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.5012584],\n",
       "        [5.670512 ],\n",
       "        [6.8397655],\n",
       "        [8.00902  ],\n",
       "        [9.178273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4992585],\n",
       "        [5.667512 ],\n",
       "        [6.835766 ],\n",
       "        [8.00402  ],\n",
       "        [9.172273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4972587],\n",
       "        [5.664512 ],\n",
       "        [6.8317657],\n",
       "        [7.9990196],\n",
       "        [9.166273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4952583],\n",
       "        [5.6615124],\n",
       "        [6.827766 ],\n",
       "        [7.9940195],\n",
       "        [9.160273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4932585],\n",
       "        [5.658512 ],\n",
       "        [6.8237658],\n",
       "        [7.9890194],\n",
       "        [9.154273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4912586],\n",
       "        [5.6555123],\n",
       "        [6.819766 ],\n",
       "        [7.9840193],\n",
       "        [9.148273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.489259 ],\n",
       "        [5.652512 ],\n",
       "        [6.815766 ],\n",
       "        [7.9790196],\n",
       "        [9.142273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4872584],\n",
       "        [5.6495123],\n",
       "        [6.8117657],\n",
       "        [7.9740195],\n",
       "        [9.136273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4852586],\n",
       "        [5.646512 ],\n",
       "        [6.807766 ],\n",
       "        [7.9690194],\n",
       "        [9.130273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4832587],\n",
       "        [5.6435122],\n",
       "        [6.803766 ],\n",
       "        [7.9640193],\n",
       "        [9.124273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4812584],\n",
       "        [5.6405125],\n",
       "        [6.799766 ],\n",
       "        [7.9590197],\n",
       "        [9.118273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4792585],\n",
       "        [5.637512 ],\n",
       "        [6.795766 ],\n",
       "        [7.9540195],\n",
       "        [9.112273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4772587],\n",
       "        [5.6345124],\n",
       "        [6.7917657],\n",
       "        [7.9490194],\n",
       "        [9.106273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.475259 ],\n",
       "        [5.631512 ],\n",
       "        [6.787766 ],\n",
       "        [7.9440193],\n",
       "        [9.100273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4732585],\n",
       "        [5.6285124],\n",
       "        [6.783766 ],\n",
       "        [7.9390197],\n",
       "        [9.094273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4712586],\n",
       "        [5.625512 ],\n",
       "        [6.779766 ],\n",
       "        [7.9340196],\n",
       "        [9.088273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.469259 ],\n",
       "        [5.6225123],\n",
       "        [6.775766 ],\n",
       "        [7.9290195],\n",
       "        [9.0822735]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4672585],\n",
       "        [5.6195126],\n",
       "        [6.7717657],\n",
       "        [7.9240193],\n",
       "        [9.076273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4652586],\n",
       "        [5.6165123],\n",
       "        [6.767766 ],\n",
       "        [7.9190197],\n",
       "        [9.070273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4632587],\n",
       "        [5.6135125],\n",
       "        [6.763766 ],\n",
       "        [7.9140196],\n",
       "        [9.064273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.461259 ],\n",
       "        [5.6105123],\n",
       "        [6.759766 ],\n",
       "        [7.9090195],\n",
       "        [9.058273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.459259 ],\n",
       "        [5.6075125],\n",
       "        [6.755766 ],\n",
       "        [7.9040194],\n",
       "        [9.052273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4572587],\n",
       "        [5.604512 ],\n",
       "        [6.751766 ],\n",
       "        [7.8990197],\n",
       "        [9.046273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.455259 ],\n",
       "        [5.6015124],\n",
       "        [6.747766 ],\n",
       "        [7.8940196],\n",
       "        [9.040273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.453259 ],\n",
       "        [5.598512 ],\n",
       "        [6.743766 ],\n",
       "        [7.8890195],\n",
       "        [9.034273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4512587],\n",
       "        [5.5955124],\n",
       "        [6.739766 ],\n",
       "        [7.8840194],\n",
       "        [9.028274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.449259 ],\n",
       "        [5.5925126],\n",
       "        [6.735766 ],\n",
       "        [7.8790197],\n",
       "        [9.022273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.447259 ],\n",
       "        [5.5895123],\n",
       "        [6.7317657],\n",
       "        [7.8740196],\n",
       "        [9.0162735]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.445259 ],\n",
       "        [5.5865126],\n",
       "        [6.727766 ],\n",
       "        [7.8690195],\n",
       "        [9.010273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.443259 ],\n",
       "        [5.5835123],\n",
       "        [6.723766 ],\n",
       "        [7.8640194],\n",
       "        [9.004273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.441259 ],\n",
       "        [5.5805125],\n",
       "        [6.719766 ],\n",
       "        [7.8590198],\n",
       "        [8.998273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.439259 ],\n",
       "        [5.5775123],\n",
       "        [6.715766 ],\n",
       "        [7.8540196],\n",
       "        [8.992273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4372587],\n",
       "        [5.5745125],\n",
       "        [6.7117662],\n",
       "        [7.8490195],\n",
       "        [8.986273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.435259 ],\n",
       "        [5.571512 ],\n",
       "        [6.707766 ],\n",
       "        [7.8440194],\n",
       "        [8.980273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.433259 ],\n",
       "        [5.5685124],\n",
       "        [6.703766 ],\n",
       "        [7.83902  ],\n",
       "        [8.974273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.431259 ],\n",
       "        [5.5655127],\n",
       "        [6.699766 ],\n",
       "        [7.8340197],\n",
       "        [8.968273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.429259 ],\n",
       "        [5.5625124],\n",
       "        [6.695766 ],\n",
       "        [7.8290195],\n",
       "        [8.962274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.427259 ],\n",
       "        [5.5595126],\n",
       "        [6.6917663],\n",
       "        [7.8240194],\n",
       "        [8.956273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.425259 ],\n",
       "        [5.5565124],\n",
       "        [6.687766 ],\n",
       "        [7.81902  ],\n",
       "        [8.9502735]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.423259 ],\n",
       "        [5.5535126],\n",
       "        [6.683766 ],\n",
       "        [7.8140197],\n",
       "        [8.944273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.421259 ],\n",
       "        [5.5505123],\n",
       "        [6.679766 ],\n",
       "        [7.8090196],\n",
       "        [8.938273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.419259 ],\n",
       "        [5.5475125],\n",
       "        [6.675766 ],\n",
       "        [7.8040195],\n",
       "        [8.932273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.417259 ],\n",
       "        [5.5445127],\n",
       "        [6.6717663],\n",
       "        [7.79902  ],\n",
       "        [8.926273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.415259 ],\n",
       "        [5.5415125],\n",
       "        [6.667766 ],\n",
       "        [7.7940197],\n",
       "        [8.920273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.413259 ],\n",
       "        [5.5385127],\n",
       "        [6.663766 ],\n",
       "        [7.7890196],\n",
       "        [8.914273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.411259 ],\n",
       "        [5.5355124],\n",
       "        [6.659766 ],\n",
       "        [7.7840195],\n",
       "        [8.908273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.409259 ],\n",
       "        [5.5325127],\n",
       "        [6.655766 ],\n",
       "        [7.77902  ],\n",
       "        [8.902273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.407259 ],\n",
       "        [5.5295124],\n",
       "        [6.6517663],\n",
       "        [7.7740197],\n",
       "        [8.896274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.405259 ],\n",
       "        [5.5265126],\n",
       "        [6.647766 ],\n",
       "        [7.7690196],\n",
       "        [8.890273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.4032593],\n",
       "        [5.523513 ],\n",
       "        [6.643766 ],\n",
       "        [7.7640195],\n",
       "        [8.884274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.401259 ],\n",
       "        [5.5205126],\n",
       "        [6.639766 ],\n",
       "        [7.75902  ],\n",
       "        [8.878273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.399259 ],\n",
       "        [5.517513 ],\n",
       "        [6.635766 ],\n",
       "        [7.7540197],\n",
       "        [8.872273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.397259 ],\n",
       "        [5.5145125],\n",
       "        [6.6317663],\n",
       "        [7.7490196],\n",
       "        [8.866273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.395259 ],\n",
       "        [5.5115128],\n",
       "        [6.627766 ],\n",
       "        [7.7440195],\n",
       "        [8.860273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.393259 ],\n",
       "        [5.5085125],\n",
       "        [6.623766 ],\n",
       "        [7.73902  ],\n",
       "        [8.854273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.391259 ],\n",
       "        [5.5055127],\n",
       "        [6.619766 ],\n",
       "        [7.7340198],\n",
       "        [8.848273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3892593],\n",
       "        [5.5025125],\n",
       "        [6.615766 ],\n",
       "        [7.7290196],\n",
       "        [8.842274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.387259 ],\n",
       "        [5.4995127],\n",
       "        [6.6117663],\n",
       "        [7.7240195],\n",
       "        [8.836273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.385259 ],\n",
       "        [5.4965124],\n",
       "        [6.607766 ],\n",
       "        [7.71902  ],\n",
       "        [8.830274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3832593],\n",
       "        [5.4935126],\n",
       "        [6.6037664],\n",
       "        [7.71402  ],\n",
       "        [8.824273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.381259 ],\n",
       "        [5.490513 ],\n",
       "        [6.5997663],\n",
       "        [7.7090197],\n",
       "        [8.818274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.379259 ],\n",
       "        [5.4875126],\n",
       "        [6.595766 ],\n",
       "        [7.7040195],\n",
       "        [8.812273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3772593],\n",
       "        [5.484513 ],\n",
       "        [6.5917664],\n",
       "        [7.69902  ],\n",
       "        [8.806273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3752594],\n",
       "        [5.4815125],\n",
       "        [6.587766 ],\n",
       "        [7.69402  ],\n",
       "        [8.800273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.373259 ],\n",
       "        [5.478513 ],\n",
       "        [6.583766 ],\n",
       "        [7.6890197],\n",
       "        [8.794273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.371259 ],\n",
       "        [5.4755125],\n",
       "        [6.5797663],\n",
       "        [7.6840196],\n",
       "        [8.788273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3692594],\n",
       "        [5.4725127],\n",
       "        [6.575766 ],\n",
       "        [7.67902  ],\n",
       "        [8.782273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.367259 ],\n",
       "        [5.469513 ],\n",
       "        [6.5717664],\n",
       "        [7.67402  ],\n",
       "        [8.776274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.365259 ],\n",
       "        [5.4665127],\n",
       "        [6.567766 ],\n",
       "        [7.6690197],\n",
       "        [8.770273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3632593],\n",
       "        [5.463513 ],\n",
       "        [6.5637665],\n",
       "        [7.6640196],\n",
       "        [8.764274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3612595],\n",
       "        [5.4605126],\n",
       "        [6.5597663],\n",
       "        [7.65902  ],\n",
       "        [8.758273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.359259],\n",
       "        [5.457513],\n",
       "        [6.555766],\n",
       "        [7.65402 ],\n",
       "        [8.752274]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3572593],\n",
       "        [5.4545126],\n",
       "        [6.5517664],\n",
       "        [7.6490197],\n",
       "        [8.746273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3552594],\n",
       "        [5.451513 ],\n",
       "        [6.547766 ],\n",
       "        [7.6440196],\n",
       "        [8.740273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.353259 ],\n",
       "        [5.448513 ],\n",
       "        [6.5437665],\n",
       "        [7.63902  ],\n",
       "        [8.734273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.351259 ],\n",
       "        [5.445513 ],\n",
       "        [6.5397663],\n",
       "        [7.63402  ],\n",
       "        [8.728273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3492594],\n",
       "        [5.442513 ],\n",
       "        [6.535766 ],\n",
       "        [7.6290197],\n",
       "        [8.722273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3472595],\n",
       "        [5.4395127],\n",
       "        [6.5317664],\n",
       "        [7.6240196],\n",
       "        [8.716273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.345259],\n",
       "        [5.436513],\n",
       "        [6.527766],\n",
       "        [7.61902 ],\n",
       "        [8.710274]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3432593],\n",
       "        [5.4335127],\n",
       "        [6.5237665],\n",
       "        [7.61402  ],\n",
       "        [8.704273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3412595],\n",
       "        [5.430513 ],\n",
       "        [6.5197663],\n",
       "        [7.6090198],\n",
       "        [8.698274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.339259 ],\n",
       "        [5.427513 ],\n",
       "        [6.515766 ],\n",
       "        [7.6040196],\n",
       "        [8.692273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3372593],\n",
       "        [5.424513 ],\n",
       "        [6.5117664],\n",
       "        [7.59902  ],\n",
       "        [8.686274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3352594],\n",
       "        [5.421513 ],\n",
       "        [6.5077662],\n",
       "        [7.59402  ],\n",
       "        [8.680273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3332596],\n",
       "        [5.418513 ],\n",
       "        [6.5037665],\n",
       "        [7.58902  ],\n",
       "        [8.6742735]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3312597],\n",
       "        [5.415513 ],\n",
       "        [6.4997663],\n",
       "        [7.5840197],\n",
       "        [8.668273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3292594],\n",
       "        [5.412513 ],\n",
       "        [6.4957666],\n",
       "        [7.57902  ],\n",
       "        [8.662273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3272595],\n",
       "        [5.409513 ],\n",
       "        [6.4917665],\n",
       "        [7.57402  ],\n",
       "        [8.656273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3252597],\n",
       "        [5.406513 ],\n",
       "        [6.4877663],\n",
       "        [7.56902  ],\n",
       "        [8.650273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3232594],\n",
       "        [5.403513 ],\n",
       "        [6.4837666],\n",
       "        [7.5640197],\n",
       "        [8.644274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3212595],\n",
       "        [5.4005127],\n",
       "        [6.4797664],\n",
       "        [7.55902  ],\n",
       "        [8.638273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3192596],\n",
       "        [5.397513 ],\n",
       "        [6.475766 ],\n",
       "        [7.55402  ],\n",
       "        [8.632274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.31726  ],\n",
       "        [5.394513 ],\n",
       "        [6.4717665],\n",
       "        [7.54902  ],\n",
       "        [8.626273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3152595],\n",
       "        [5.391513 ],\n",
       "        [6.4677663],\n",
       "        [7.5440197],\n",
       "        [8.620274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3132596],\n",
       "        [5.388513 ],\n",
       "        [6.4637666],\n",
       "        [7.53902  ],\n",
       "        [8.614273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3112597],\n",
       "        [5.385513 ],\n",
       "        [6.4597664],\n",
       "        [7.53402  ],\n",
       "        [8.6082735]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3092594],\n",
       "        [5.382513 ],\n",
       "        [6.4557667],\n",
       "        [7.52902  ],\n",
       "        [8.602273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3072596],\n",
       "        [5.379513 ],\n",
       "        [6.4517665],\n",
       "        [7.5240197],\n",
       "        [8.596273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3052597],\n",
       "        [5.376513 ],\n",
       "        [6.4477663],\n",
       "        [7.51902  ],\n",
       "        [8.590273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.30326  ],\n",
       "        [5.373513 ],\n",
       "        [6.4437666],\n",
       "        [7.51402  ],\n",
       "        [8.584273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.3012595],\n",
       "        [5.370513 ],\n",
       "        [6.4397664],\n",
       "        [7.50902  ],\n",
       "        [8.578274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2992597],\n",
       "        [5.367513 ],\n",
       "        [6.435766 ],\n",
       "        [7.5040197],\n",
       "        [8.572273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.29726  ],\n",
       "        [5.364513 ],\n",
       "        [6.4317665],\n",
       "        [7.49902  ],\n",
       "        [8.566274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2952595],\n",
       "        [5.361513 ],\n",
       "        [6.4277663],\n",
       "        [7.49402  ],\n",
       "        [8.560273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2932596],\n",
       "        [5.358513 ],\n",
       "        [6.4237666],\n",
       "        [7.48902  ],\n",
       "        [8.554274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.29126  ],\n",
       "        [5.355513 ],\n",
       "        [6.4197664],\n",
       "        [7.4840198],\n",
       "        [8.548273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.28926  ],\n",
       "        [5.3525133],\n",
       "        [6.4157667],\n",
       "        [7.47902  ],\n",
       "        [8.5422735]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2872596],\n",
       "        [5.349513 ],\n",
       "        [6.4117665],\n",
       "        [7.47402  ],\n",
       "        [8.536273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2852597],\n",
       "        [5.3465133],\n",
       "        [6.4077663],\n",
       "        [7.46902  ],\n",
       "        [8.530273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.28326  ],\n",
       "        [5.343513 ],\n",
       "        [6.4037666],\n",
       "        [7.46402  ],\n",
       "        [8.524274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2812595],\n",
       "        [5.340513 ],\n",
       "        [6.3997664],\n",
       "        [7.45902  ],\n",
       "        [8.518273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2792597],\n",
       "        [5.337513 ],\n",
       "        [6.3957667],\n",
       "        [7.45402  ],\n",
       "        [8.512274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.27726  ],\n",
       "        [5.334513 ],\n",
       "        [6.3917665],\n",
       "        [7.44902  ],\n",
       "        [8.506273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.27526  ],\n",
       "        [5.3315134],\n",
       "        [6.387767 ],\n",
       "        [7.4440203],\n",
       "        [8.500274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2732596],\n",
       "        [5.328513 ],\n",
       "        [6.3837667],\n",
       "        [7.43902  ],\n",
       "        [8.494273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.27126  ],\n",
       "        [5.3255134],\n",
       "        [6.3797665],\n",
       "        [7.43402  ],\n",
       "        [8.488274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.26926  ],\n",
       "        [5.322513 ],\n",
       "        [6.3757668],\n",
       "        [7.42902  ],\n",
       "        [8.482273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2672596],\n",
       "        [5.3195133],\n",
       "        [6.3717666],\n",
       "        [7.4240203],\n",
       "        [8.476274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2652597],\n",
       "        [5.316513 ],\n",
       "        [6.367767 ],\n",
       "        [7.41902  ],\n",
       "        [8.470274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.26326  ],\n",
       "        [5.3135133],\n",
       "        [6.3637667],\n",
       "        [7.41402  ],\n",
       "        [8.464273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.26126  ],\n",
       "        [5.3105135],\n",
       "        [6.359767 ],\n",
       "        [7.4090204],\n",
       "        [8.458274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2592597],\n",
       "        [5.307513 ],\n",
       "        [6.355767 ],\n",
       "        [7.4040203],\n",
       "        [8.452273 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.25726  ],\n",
       "        [5.3045135],\n",
       "        [6.3517666],\n",
       "        [7.39902  ],\n",
       "        [8.446274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.25526 ],\n",
       "        [5.301513],\n",
       "        [6.347767],\n",
       "        [7.39402 ],\n",
       "        [8.440273]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.25326  ],\n",
       "        [5.2985134],\n",
       "        [6.3437667],\n",
       "        [7.3890204],\n",
       "        [8.434274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.25126  ],\n",
       "        [5.295513 ],\n",
       "        [6.339767 ],\n",
       "        [7.3840203],\n",
       "        [8.428274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.24926  ],\n",
       "        [5.2925134],\n",
       "        [6.335767 ],\n",
       "        [7.37902  ],\n",
       "        [8.422274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.24726  ],\n",
       "        [5.2895136],\n",
       "        [6.331767 ],\n",
       "        [7.3740206],\n",
       "        [8.416274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2452602],\n",
       "        [5.2865133],\n",
       "        [6.327767 ],\n",
       "        [7.3690205],\n",
       "        [8.410274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.24326  ],\n",
       "        [5.2835135],\n",
       "        [6.3237667],\n",
       "        [7.3640203],\n",
       "        [8.404274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.24126 ],\n",
       "        [5.280514],\n",
       "        [6.319767],\n",
       "        [7.35902 ],\n",
       "        [8.398273]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.23926  ],\n",
       "        [5.2775135],\n",
       "        [6.315767 ],\n",
       "        [7.3540206],\n",
       "        [8.392274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.23726  ],\n",
       "        [5.2745137],\n",
       "        [6.311767 ],\n",
       "        [7.3490205],\n",
       "        [8.386274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.23526  ],\n",
       "        [5.2715135],\n",
       "        [6.307767 ],\n",
       "        [7.3440204],\n",
       "        [8.380274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.23326  ],\n",
       "        [5.2685137],\n",
       "        [6.303767 ],\n",
       "        [7.3390207],\n",
       "        [8.374274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2312603],\n",
       "        [5.2655134],\n",
       "        [6.299767 ],\n",
       "        [7.3340206],\n",
       "        [8.368274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.22926  ],\n",
       "        [5.2625136],\n",
       "        [6.295767 ],\n",
       "        [7.3290205],\n",
       "        [8.362274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.22726  ],\n",
       "        [5.259514 ],\n",
       "        [6.291767 ],\n",
       "        [7.3240204],\n",
       "        [8.356274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2252603],\n",
       "        [5.2565136],\n",
       "        [6.287767 ],\n",
       "        [7.3190207],\n",
       "        [8.350274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.22326  ],\n",
       "        [5.253514 ],\n",
       "        [6.283767 ],\n",
       "        [7.3140206],\n",
       "        [8.3442745]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.22126  ],\n",
       "        [5.2505136],\n",
       "        [6.279767 ],\n",
       "        [7.3090205],\n",
       "        [8.338274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.21926  ],\n",
       "        [5.247514 ],\n",
       "        [6.2757673],\n",
       "        [7.3040204],\n",
       "        [8.332274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2172604],\n",
       "        [5.2445135],\n",
       "        [6.271767 ],\n",
       "        [7.299021 ],\n",
       "        [8.326274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.21526  ],\n",
       "        [5.2415137],\n",
       "        [6.267767 ],\n",
       "        [7.2940207],\n",
       "        [8.320274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.21326  ],\n",
       "        [5.238514 ],\n",
       "        [6.2637672],\n",
       "        [7.2890205],\n",
       "        [8.314274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2112603],\n",
       "        [5.2355137],\n",
       "        [6.259767 ],\n",
       "        [7.2840204],\n",
       "        [8.308274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.20926  ],\n",
       "        [5.232514 ],\n",
       "        [6.2557673],\n",
       "        [7.279021 ],\n",
       "        [8.302274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.20726  ],\n",
       "        [5.2295136],\n",
       "        [6.251767 ],\n",
       "        [7.2740207],\n",
       "        [8.296274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2052603],\n",
       "        [5.226514 ],\n",
       "        [6.2477674],\n",
       "        [7.2690206],\n",
       "        [8.290274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2032604],\n",
       "        [5.2235136],\n",
       "        [6.2437673],\n",
       "        [7.264021 ],\n",
       "        [8.284274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.2012606],\n",
       "        [5.220514 ],\n",
       "        [6.239767 ],\n",
       "        [7.259021 ],\n",
       "        [8.278275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.19926  ],\n",
       "        [5.217514 ],\n",
       "        [6.2357674],\n",
       "        [7.2540207],\n",
       "        [8.272274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1972604],\n",
       "        [5.214514 ],\n",
       "        [6.231767 ],\n",
       "        [7.2490206],\n",
       "        [8.266274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1952605],\n",
       "        [5.211514 ],\n",
       "        [6.2277675],\n",
       "        [7.244021 ],\n",
       "        [8.260274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.19326  ],\n",
       "        [5.2085137],\n",
       "        [6.2237673],\n",
       "        [7.239021 ],\n",
       "        [8.254274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1912603],\n",
       "        [5.205514 ],\n",
       "        [6.2197676],\n",
       "        [7.2340207],\n",
       "        [8.248274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1892605],\n",
       "        [5.2025137],\n",
       "        [6.2157674],\n",
       "        [7.2290206],\n",
       "        [8.242274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1872606],\n",
       "        [5.199514 ],\n",
       "        [6.211767 ],\n",
       "        [7.224021 ],\n",
       "        [8.236275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1852603],\n",
       "        [5.196514 ],\n",
       "        [6.2077675],\n",
       "        [7.219021 ],\n",
       "        [8.230274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1832604],\n",
       "        [5.193514 ],\n",
       "        [6.2037673],\n",
       "        [7.2140207],\n",
       "        [8.224275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1812606],\n",
       "        [5.190514 ],\n",
       "        [6.199767 ],\n",
       "        [7.2090206],\n",
       "        [8.218274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1792603],\n",
       "        [5.187514 ],\n",
       "        [6.1957674],\n",
       "        [7.204021 ],\n",
       "        [8.212275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1772604],\n",
       "        [5.184514 ],\n",
       "        [6.191767 ],\n",
       "        [7.199021 ],\n",
       "        [8.206274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1752605],\n",
       "        [5.181514 ],\n",
       "        [6.1877675],\n",
       "        [7.1940207],\n",
       "        [8.200274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1732607],\n",
       "        [5.178514 ],\n",
       "        [6.1837673],\n",
       "        [7.189021 ],\n",
       "        [8.194274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1712604],\n",
       "        [5.175514 ],\n",
       "        [6.1797676],\n",
       "        [7.184021 ],\n",
       "        [8.188274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1692605],\n",
       "        [5.172514 ],\n",
       "        [6.1757674],\n",
       "        [7.179021 ],\n",
       "        [8.182275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1672606],\n",
       "        [5.169514 ],\n",
       "        [6.171767 ],\n",
       "        [7.174021 ],\n",
       "        [8.176274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1652603],\n",
       "        [5.166514 ],\n",
       "        [6.1677675],\n",
       "        [7.169021 ],\n",
       "        [8.170275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1632605],\n",
       "        [5.163514 ],\n",
       "        [6.1637673],\n",
       "        [7.164021 ],\n",
       "        [8.164274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1612606],\n",
       "        [5.160514 ],\n",
       "        [6.1597676],\n",
       "        [7.159021 ],\n",
       "        [8.158275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1592607],\n",
       "        [5.157514 ],\n",
       "        [6.1557674],\n",
       "        [7.154021 ],\n",
       "        [8.152274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1572604],\n",
       "        [5.1545143],\n",
       "        [6.1517677],\n",
       "        [7.149021 ],\n",
       "        [8.146275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1552606],\n",
       "        [5.151514 ],\n",
       "        [6.1477675],\n",
       "        [7.144021 ],\n",
       "        [8.140274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1532607],\n",
       "        [5.1485143],\n",
       "        [6.1437674],\n",
       "        [7.139021 ],\n",
       "        [8.1342745]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1512604],\n",
       "        [5.145514 ],\n",
       "        [6.1397676],\n",
       "        [7.134021 ],\n",
       "        [8.128275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1492605],\n",
       "        [5.142514 ],\n",
       "        [6.1357675],\n",
       "        [7.129021 ],\n",
       "        [8.122274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1472607],\n",
       "        [5.139514 ],\n",
       "        [6.1317677],\n",
       "        [7.124021 ],\n",
       "        [8.116275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.145261 ],\n",
       "        [5.136514 ],\n",
       "        [6.1277676],\n",
       "        [7.119021 ],\n",
       "        [8.110274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1432605],\n",
       "        [5.1335144],\n",
       "        [6.123768 ],\n",
       "        [7.1140213],\n",
       "        [8.104275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1412606],\n",
       "        [5.130514 ],\n",
       "        [6.1197677],\n",
       "        [7.109021 ],\n",
       "        [8.098274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.139261 ],\n",
       "        [5.1275144],\n",
       "        [6.1157675],\n",
       "        [7.104021 ],\n",
       "        [8.092275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.137261],\n",
       "        [5.124514],\n",
       "        [6.111768],\n",
       "        [7.099021],\n",
       "        [8.086275]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1352606],\n",
       "        [5.1215143],\n",
       "        [6.1077676],\n",
       "        [7.0940213],\n",
       "        [8.080275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1332607],\n",
       "        [5.118514 ],\n",
       "        [6.103768 ],\n",
       "        [7.089021 ],\n",
       "        [8.074275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.131261 ],\n",
       "        [5.1155143],\n",
       "        [6.0997677],\n",
       "        [7.084021 ],\n",
       "        [8.0682745]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.129261 ],\n",
       "        [5.1125145],\n",
       "        [6.095768 ],\n",
       "        [7.0790215],\n",
       "        [8.062275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1272607],\n",
       "        [5.109514 ],\n",
       "        [6.091768 ],\n",
       "        [7.0740213],\n",
       "        [8.056274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.125261 ],\n",
       "        [5.1065145],\n",
       "        [6.0877676],\n",
       "        [7.069021 ],\n",
       "        [8.050275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.123261],\n",
       "        [5.103514],\n",
       "        [6.083768],\n",
       "        [7.064021],\n",
       "        [8.044274]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1212606],\n",
       "        [5.1005144],\n",
       "        [6.0797677],\n",
       "        [7.059021 ],\n",
       "        [8.038275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.119261 ],\n",
       "        [5.097514 ],\n",
       "        [6.0757675],\n",
       "        [7.0540214],\n",
       "        [8.032274 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.117261 ],\n",
       "        [5.0945144],\n",
       "        [6.071768 ],\n",
       "        [7.0490212],\n",
       "        [8.026275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.115261 ],\n",
       "        [5.0915146],\n",
       "        [6.0677676],\n",
       "        [7.044021 ],\n",
       "        [8.020275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1132607],\n",
       "        [5.0885143],\n",
       "        [6.063768 ],\n",
       "        [7.0390215],\n",
       "        [8.014275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.111261 ],\n",
       "        [5.0855145],\n",
       "        [6.0597677],\n",
       "        [7.0340214],\n",
       "        [8.008275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.109261 ],\n",
       "        [5.0825143],\n",
       "        [6.055768 ],\n",
       "        [7.0290213],\n",
       "        [8.0022745]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.1072607],\n",
       "        [5.0795145],\n",
       "        [6.051768 ],\n",
       "        [7.024021 ],\n",
       "        [7.996275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.105261 ],\n",
       "        [5.0765142],\n",
       "        [6.0477676],\n",
       "        [7.0190215],\n",
       "        [7.990275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.103261 ],\n",
       "        [5.0735145],\n",
       "        [6.043768 ],\n",
       "        [7.0140214],\n",
       "        [7.984275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.101261 ],\n",
       "        [5.0705147],\n",
       "        [6.0397677],\n",
       "        [7.0090213],\n",
       "        [7.978275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.099261 ],\n",
       "        [5.0675144],\n",
       "        [6.035768 ],\n",
       "        [7.0040216],\n",
       "        [7.972275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.097261 ],\n",
       "        [5.0645146],\n",
       "        [6.031768 ],\n",
       "        [6.9990215],\n",
       "        [7.9662747]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.095261 ],\n",
       "        [5.0615144],\n",
       "        [6.027768 ],\n",
       "        [6.9940214],\n",
       "        [7.9602747]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.093261 ],\n",
       "        [5.0585146],\n",
       "        [6.023768 ],\n",
       "        [6.9890213],\n",
       "        [7.954275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.091261 ],\n",
       "        [5.0555143],\n",
       "        [6.0197678],\n",
       "        [6.984021 ],\n",
       "        [7.948275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.089261 ],\n",
       "        [5.0525146],\n",
       "        [6.015768 ],\n",
       "        [6.9790215],\n",
       "        [7.942275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.087261 ],\n",
       "        [5.049515 ],\n",
       "        [6.011768 ],\n",
       "        [6.9740214],\n",
       "        [7.936275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.085261 ],\n",
       "        [5.0465145],\n",
       "        [6.007768 ],\n",
       "        [6.969022 ],\n",
       "        [7.930275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.083261 ],\n",
       "        [5.0435147],\n",
       "        [6.003768 ],\n",
       "        [6.9640217],\n",
       "        [7.924275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.081261 ],\n",
       "        [5.0405145],\n",
       "        [5.9997683],\n",
       "        [6.9590216],\n",
       "        [7.918275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0792613],\n",
       "        [5.0375147],\n",
       "        [5.995768 ],\n",
       "        [6.9540215],\n",
       "        [7.9122753]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.077261 ],\n",
       "        [5.0345144],\n",
       "        [5.991768 ],\n",
       "        [6.9490213],\n",
       "        [7.906275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.075261 ],\n",
       "        [5.0315146],\n",
       "        [5.987768 ],\n",
       "        [6.9440217],\n",
       "        [7.900275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0732613],\n",
       "        [5.028515 ],\n",
       "        [5.983768 ],\n",
       "        [6.9390216],\n",
       "        [7.894275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0712614],\n",
       "        [5.0255146],\n",
       "        [5.9797683],\n",
       "        [6.934022 ],\n",
       "        [7.888275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.069261],\n",
       "        [5.022515],\n",
       "        [5.975768],\n",
       "        [6.929022],\n",
       "        [7.882275]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.067261 ],\n",
       "        [5.0195146],\n",
       "        [5.9717684],\n",
       "        [6.9240217],\n",
       "        [7.876275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0652614],\n",
       "        [5.016515 ],\n",
       "        [5.967768 ],\n",
       "        [6.9190216],\n",
       "        [7.8702755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.063261 ],\n",
       "        [5.0135145],\n",
       "        [5.963768 ],\n",
       "        [6.9140215],\n",
       "        [7.864275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.061261 ],\n",
       "        [5.0105147],\n",
       "        [5.9597683],\n",
       "        [6.909022 ],\n",
       "        [7.8582754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0592613],\n",
       "        [5.007515 ],\n",
       "        [5.955768 ],\n",
       "        [6.9040217],\n",
       "        [7.8522754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0572615],\n",
       "        [5.0045147],\n",
       "        [5.9517684],\n",
       "        [6.8990216],\n",
       "        [7.8462753]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.055261 ],\n",
       "        [5.001515 ],\n",
       "        [5.947768 ],\n",
       "        [6.894022 ],\n",
       "        [7.8402753]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0532613],\n",
       "        [4.9985147],\n",
       "        [5.9437685],\n",
       "        [6.889022 ],\n",
       "        [7.8342752]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0512614],\n",
       "        [4.995515 ],\n",
       "        [5.9397683],\n",
       "        [6.8840218],\n",
       "        [7.828275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.049261 ],\n",
       "        [4.9925146],\n",
       "        [5.935768 ],\n",
       "        [6.8790216],\n",
       "        [7.822275 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.047261 ],\n",
       "        [4.989515 ],\n",
       "        [5.9317684],\n",
       "        [6.874022 ],\n",
       "        [7.8162756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0452614],\n",
       "        [4.986515 ],\n",
       "        [5.927768 ],\n",
       "        [6.869022 ],\n",
       "        [7.8102756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0432615],\n",
       "        [4.983515 ],\n",
       "        [5.9237685],\n",
       "        [6.864022 ],\n",
       "        [7.8042755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.041261 ],\n",
       "        [4.980515 ],\n",
       "        [5.9197683],\n",
       "        [6.859022 ],\n",
       "        [7.7982755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0392613],\n",
       "        [4.977515 ],\n",
       "        [5.9157686],\n",
       "        [6.854022 ],\n",
       "        [7.7922754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0372615],\n",
       "        [4.974515 ],\n",
       "        [5.9117684],\n",
       "        [6.849022 ],\n",
       "        [7.7862754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.035261 ],\n",
       "        [4.9715147],\n",
       "        [5.9077682],\n",
       "        [6.844022 ],\n",
       "        [7.7802753]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0332613],\n",
       "        [4.968515 ],\n",
       "        [5.9037685],\n",
       "        [6.8390217],\n",
       "        [7.774276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0312614],\n",
       "        [4.965515 ],\n",
       "        [5.8997684],\n",
       "        [6.834022 ],\n",
       "        [7.7682753]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0292616],\n",
       "        [4.962515 ],\n",
       "        [5.8957686],\n",
       "        [6.829022 ],\n",
       "        [7.7622757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0272617],\n",
       "        [4.959515 ],\n",
       "        [5.8917685],\n",
       "        [6.8240223],\n",
       "        [7.7562757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0252614],\n",
       "        [4.956515 ],\n",
       "        [5.8877687],\n",
       "        [6.819022 ],\n",
       "        [7.7502756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0232615],\n",
       "        [4.953515 ],\n",
       "        [5.8837686],\n",
       "        [6.814022 ],\n",
       "        [7.7442756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0212617],\n",
       "        [4.950515 ],\n",
       "        [5.8797684],\n",
       "        [6.809022 ],\n",
       "        [7.7382755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0192614],\n",
       "        [4.947515 ],\n",
       "        [5.8757687],\n",
       "        [6.804022 ],\n",
       "        [7.7322755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0172615],\n",
       "        [4.944515 ],\n",
       "        [5.8717685],\n",
       "        [6.7990217],\n",
       "        [7.7262754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0152617],\n",
       "        [4.941515 ],\n",
       "        [5.8677683],\n",
       "        [6.794022 ],\n",
       "        [7.7202754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.013262 ],\n",
       "        [4.938515 ],\n",
       "        [5.8637686],\n",
       "        [6.789022 ],\n",
       "        [7.7142754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0112615],\n",
       "        [4.935515 ],\n",
       "        [5.8597684],\n",
       "        [6.7840223],\n",
       "        [7.708276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0092616],\n",
       "        [4.932515 ],\n",
       "        [5.8557687],\n",
       "        [6.779022 ],\n",
       "        [7.7022753]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0072618],\n",
       "        [4.929515 ],\n",
       "        [5.8517685],\n",
       "        [6.774022 ],\n",
       "        [7.6962757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0052614],\n",
       "        [4.926515 ],\n",
       "        [5.847769 ],\n",
       "        [6.769022 ],\n",
       "        [7.6902757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0032616],\n",
       "        [4.9235153],\n",
       "        [5.8437686],\n",
       "        [6.764022 ],\n",
       "        [7.6842756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.0012617],\n",
       "        [4.920515 ],\n",
       "        [5.8397684],\n",
       "        [6.7590218],\n",
       "        [7.6782756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9992616],\n",
       "        [4.917515 ],\n",
       "        [5.8357687],\n",
       "        [6.754022 ],\n",
       "        [7.6722755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9972615],\n",
       "        [4.914515 ],\n",
       "        [5.8317685],\n",
       "        [6.749022 ],\n",
       "        [7.6662755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9952617],\n",
       "        [4.911515 ],\n",
       "        [5.8277683],\n",
       "        [6.7440224],\n",
       "        [7.6602755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9932618],\n",
       "        [4.908515 ],\n",
       "        [5.8237686],\n",
       "        [6.7390223],\n",
       "        [7.6542754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9912617],\n",
       "        [4.905515 ],\n",
       "        [5.819769 ],\n",
       "        [6.734022 ],\n",
       "        [7.6482754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9892616],\n",
       "        [4.9025154],\n",
       "        [5.8157687],\n",
       "        [6.729022 ],\n",
       "        [7.642276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9872618],\n",
       "        [4.899515 ],\n",
       "        [5.8117685],\n",
       "        [6.724022 ],\n",
       "        [7.6362753]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9852617],\n",
       "        [4.896515 ],\n",
       "        [5.807769 ],\n",
       "        [6.719022 ],\n",
       "        [7.6302757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9832616],\n",
       "        [4.893515 ],\n",
       "        [5.8037686],\n",
       "        [6.714022 ],\n",
       "        [7.6242757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9812617],\n",
       "        [4.8905153],\n",
       "        [5.7997684],\n",
       "        [6.709022 ],\n",
       "        [7.6182756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9792619],\n",
       "        [4.887515 ],\n",
       "        [5.7957687],\n",
       "        [6.7040224],\n",
       "        [7.6122756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9772618],\n",
       "        [4.8845153],\n",
       "        [5.7917686],\n",
       "        [6.6990223],\n",
       "        [7.6062756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9752617],\n",
       "        [4.8815155],\n",
       "        [5.7877684],\n",
       "        [6.694022 ],\n",
       "        [7.6002755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9732618],\n",
       "        [4.8785152],\n",
       "        [5.7837687],\n",
       "        [6.689022 ],\n",
       "        [7.5942755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9712617],\n",
       "        [4.875515 ],\n",
       "        [5.779769 ],\n",
       "        [6.684022 ],\n",
       "        [7.5882754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9692616],\n",
       "        [4.872515 ],\n",
       "        [5.7757688],\n",
       "        [6.679022 ],\n",
       "        [7.5822754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9672618],\n",
       "        [4.8695154],\n",
       "        [5.7717686],\n",
       "        [6.674022 ],\n",
       "        [7.576276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.965262 ],\n",
       "        [4.866515 ],\n",
       "        [5.767769 ],\n",
       "        [6.669022 ],\n",
       "        [7.5702753]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9632618],\n",
       "        [4.8635154],\n",
       "        [5.7637687],\n",
       "        [6.6640224],\n",
       "        [7.5642757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9612617],\n",
       "        [4.860515 ],\n",
       "        [5.7597685],\n",
       "        [6.6590223],\n",
       "        [7.5582757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.959262 ],\n",
       "        [4.8575153],\n",
       "        [5.755769 ],\n",
       "        [6.654022 ],\n",
       "        [7.5522757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9572618],\n",
       "        [4.854515 ],\n",
       "        [5.7517686],\n",
       "        [6.649022 ],\n",
       "        [7.5462756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9552617],\n",
       "        [4.8515153],\n",
       "        [5.747769 ],\n",
       "        [6.644022 ],\n",
       "        [7.5402756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9532619],\n",
       "        [4.8485155],\n",
       "        [5.7437687],\n",
       "        [6.639022 ],\n",
       "        [7.5342755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.951262 ],\n",
       "        [4.8455153],\n",
       "        [5.739769 ],\n",
       "        [6.634022 ],\n",
       "        [7.5282755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.949262 ],\n",
       "        [4.8425155],\n",
       "        [5.735769 ],\n",
       "        [6.629022 ],\n",
       "        [7.522276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9472618],\n",
       "        [4.839515 ],\n",
       "        [5.7317686],\n",
       "        [6.6240225],\n",
       "        [7.5162754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.945262 ],\n",
       "        [4.8365154],\n",
       "        [5.727769 ],\n",
       "        [6.6190224],\n",
       "        [7.510276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9432619],\n",
       "        [4.833515 ],\n",
       "        [5.7237687],\n",
       "        [6.6140223],\n",
       "        [7.5042753]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9412618],\n",
       "        [4.8305154],\n",
       "        [5.7197685],\n",
       "        [6.609022 ],\n",
       "        [7.4982758]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.939262 ],\n",
       "        [4.8275156],\n",
       "        [5.715769 ],\n",
       "        [6.604022 ],\n",
       "        [7.4922757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.937262 ],\n",
       "        [4.8245153],\n",
       "        [5.711769 ],\n",
       "        [6.599022 ],\n",
       "        [7.4862757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.935262 ],\n",
       "        [4.821515 ],\n",
       "        [5.707769 ],\n",
       "        [6.5940223],\n",
       "        [7.4802756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9332619],\n",
       "        [4.8185153],\n",
       "        [5.7037687],\n",
       "        [6.589022 ],\n",
       "        [7.4742756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.931262 ],\n",
       "        [4.8155155],\n",
       "        [5.699769 ],\n",
       "        [6.5840225],\n",
       "        [7.4682755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9292622],\n",
       "        [4.8125153],\n",
       "        [5.695769 ],\n",
       "        [6.5790224],\n",
       "        [7.4622755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.927262 ],\n",
       "        [4.8095155],\n",
       "        [5.6917686],\n",
       "        [6.5740223],\n",
       "        [7.456276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.925262 ],\n",
       "        [4.8065157],\n",
       "        [5.687769 ],\n",
       "        [6.569022 ],\n",
       "        [7.4502754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9232621],\n",
       "        [4.8035154],\n",
       "        [5.6837687],\n",
       "        [6.564022 ],\n",
       "        [7.444276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.921262 ],\n",
       "        [4.800515 ],\n",
       "        [5.6797686],\n",
       "        [6.559022 ],\n",
       "        [7.4382753]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.919262 ],\n",
       "        [4.7975154],\n",
       "        [5.675769 ],\n",
       "        [6.5540223],\n",
       "        [7.432276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.917262 ],\n",
       "        [4.7945156],\n",
       "        [5.671769 ],\n",
       "        [6.549022 ],\n",
       "        [7.4262757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9152622],\n",
       "        [4.7915154],\n",
       "        [5.667769 ],\n",
       "        [6.5440226],\n",
       "        [7.4202757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9132621],\n",
       "        [4.7885156],\n",
       "        [5.663769 ],\n",
       "        [6.5390224],\n",
       "        [7.4142756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.911262 ],\n",
       "        [4.785516 ],\n",
       "        [5.659769 ],\n",
       "        [6.5340223],\n",
       "        [7.4082756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9092622],\n",
       "        [4.7825155],\n",
       "        [5.655769 ],\n",
       "        [6.529022 ],\n",
       "        [7.4022756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.907262 ],\n",
       "        [4.7795153],\n",
       "        [5.6517687],\n",
       "        [6.524022 ],\n",
       "        [7.3962755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.905262 ],\n",
       "        [4.7765155],\n",
       "        [5.647769 ],\n",
       "        [6.519022 ],\n",
       "        [7.390276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9032621],\n",
       "        [4.7735157],\n",
       "        [5.643769 ],\n",
       "        [6.5140224],\n",
       "        [7.3842754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.9012623],\n",
       "        [4.7705154],\n",
       "        [5.6397686],\n",
       "        [6.509022 ],\n",
       "        [7.378276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8992622],\n",
       "        [4.7675157],\n",
       "        [5.635769 ],\n",
       "        [6.5040226],\n",
       "        [7.3722754]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.897262 ],\n",
       "        [4.7645154],\n",
       "        [5.631769 ],\n",
       "        [6.4990225],\n",
       "        [7.366276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8952622],\n",
       "        [4.7615156],\n",
       "        [5.627769 ],\n",
       "        [6.4940224],\n",
       "        [7.3602757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8932621],\n",
       "        [4.7585154],\n",
       "        [5.623769 ],\n",
       "        [6.4890223],\n",
       "        [7.3542757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.891262 ],\n",
       "        [4.7555156],\n",
       "        [5.619769 ],\n",
       "        [6.484022 ],\n",
       "        [7.3482757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8892622],\n",
       "        [4.752516 ],\n",
       "        [5.615769 ],\n",
       "        [6.479022 ],\n",
       "        [7.3422756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8872623],\n",
       "        [4.7495155],\n",
       "        [5.6117687],\n",
       "        [6.4740224],\n",
       "        [7.3362756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8852623],\n",
       "        [4.7465158],\n",
       "        [5.607769 ],\n",
       "        [6.4690223],\n",
       "        [7.3302755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8832622],\n",
       "        [4.7435155],\n",
       "        [5.603769 ],\n",
       "        [6.4640226],\n",
       "        [7.324276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8812623],\n",
       "        [4.7405157],\n",
       "        [5.599769 ],\n",
       "        [6.4590225],\n",
       "        [7.3182755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8792622],\n",
       "        [4.7375154],\n",
       "        [5.595769 ],\n",
       "        [6.4540224],\n",
       "        [7.312276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.877262 ],\n",
       "        [4.7345157],\n",
       "        [5.591769 ],\n",
       "        [6.4490223],\n",
       "        [7.306276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8752623],\n",
       "        [4.731516 ],\n",
       "        [5.587769 ],\n",
       "        [6.444022 ],\n",
       "        [7.300276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8732624],\n",
       "        [4.7285156],\n",
       "        [5.583769 ],\n",
       "        [6.439022 ],\n",
       "        [7.2942758]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8712623],\n",
       "        [4.7255154],\n",
       "        [5.579769 ],\n",
       "        [6.4340224],\n",
       "        [7.2882757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8692622],\n",
       "        [4.7225156],\n",
       "        [5.575769 ],\n",
       "        [6.4290223],\n",
       "        [7.2822757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8672624],\n",
       "        [4.719516 ],\n",
       "        [5.5717688],\n",
       "        [6.4240227],\n",
       "        [7.2762756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8652625],\n",
       "        [4.7165155],\n",
       "        [5.567769 ],\n",
       "        [6.4190226],\n",
       "        [7.2702756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8632624],\n",
       "        [4.7135158],\n",
       "        [5.5637693],\n",
       "        [6.4140224],\n",
       "        [7.2642756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8612623],\n",
       "        [4.710516 ],\n",
       "        [5.559769 ],\n",
       "        [6.4090223],\n",
       "        [7.258276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8592625],\n",
       "        [4.7075157],\n",
       "        [5.555769 ],\n",
       "        [6.404022 ],\n",
       "        [7.2522755]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8572624],\n",
       "        [4.7045155],\n",
       "        [5.5517693],\n",
       "        [6.399022 ],\n",
       "        [7.246276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8552623],\n",
       "        [4.7015157],\n",
       "        [5.547769 ],\n",
       "        [6.3940225],\n",
       "        [7.240276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8532624],\n",
       "        [4.698516 ],\n",
       "        [5.543769 ],\n",
       "        [6.3890224],\n",
       "        [7.234276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8512626],\n",
       "        [4.6955156],\n",
       "        [5.539769 ],\n",
       "        [6.3840227],\n",
       "        [7.228276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8492625],\n",
       "        [4.692516 ],\n",
       "        [5.535769 ],\n",
       "        [6.3790226],\n",
       "        [7.2222757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8472624],\n",
       "        [4.689516 ],\n",
       "        [5.531769 ],\n",
       "        [6.3740225],\n",
       "        [7.2162757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8452625],\n",
       "        [4.686516 ],\n",
       "        [5.527769 ],\n",
       "        [6.3690224],\n",
       "        [7.2102757]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8432624],\n",
       "        [4.6835155],\n",
       "        [5.5237694],\n",
       "        [6.3640223],\n",
       "        [7.204276 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8412623],\n",
       "        [4.680516 ],\n",
       "        [5.519769 ],\n",
       "        [6.359022 ],\n",
       "        [7.1982756]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[3.8392625],\n",
       "        [4.677516 ],\n",
       "        [5.515769 ],\n",
       "        [6.3540225],\n",
       "        [7.192276 ]], dtype=float32)>,\n",
       " ...]"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "71de0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = tf.Variable([1,2,3,4,5], dtype = tf.float32)\n",
    "input2 = tf.Variable([1,2,3,4,5], dtype = tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def test_grad(input1, input2):\n",
    "    return tf.reduce_mean((input1 + input2) * tf.cast(tf.random.categorical(tf.math.log([[0.5, 0.5]]), 5), dtype = tf.float32) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "407b7e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = 'logs/func/%s' % stamp\n",
    "writer = tf.summary.create_file_writer(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6985c467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Trace already enabled\n"
     ]
    }
   ],
   "source": [
    "#with tf.GradientTape() as tape:\n",
    "tf.summary.trace_on(graph=True, profiler=False)\n",
    "loss = test_grad(input1, input2) * 5 + 1 * 6\n",
    "\n",
    "with writer.as_default():\n",
    "              tf.summary.trace_export(\n",
    "              name=\"new_func3\",\n",
    "              step=0,\n",
    "              profiler_outdir=logdir)            \n",
    "#gradients = tape.gradient([loss], [input1, input2])\n",
    "\n",
    "#optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "3b080634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=16.0>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "5f035cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3516), started 2:02:32 ago. (Use '!kill 3516' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-60af12512be35ce2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-60af12512be35ce2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs/func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6816da32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = tf.Variable([[1,2,3,4,5]], dtype = tf.float32)\n",
    "input2 = tf.Variable([1,2,3,4,5], dtype = tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def test_grad(input1):\n",
    "    val = tf.shape(input1) > 1\n",
    "    tf.print(val)\n",
    "    return val\n",
    "\n",
    "\n",
    "test_grad(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "3eb7208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = tf.transpose(tf.Variable([[1,2,3,4,5]], dtype = tf.int64))\n",
    "def sampler(A, sample_size = 255):\n",
    "    tf.print(tf.shape(A))\n",
    "    if tf.shape(A)[0]<sample_size:\n",
    "        sample_size = tf.shape(A)[0]\n",
    "    sample = tf.random.uniform_candidate_sampler(\n",
    "    A, 1, sample_size, unique = True, range_max = tf.shape(A)[0].numpy(), seed=None, name=None)[0]\n",
    "    tf.print(sample)\n",
    "    return(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "2c2625ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1]\n",
      "[2 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 0], dtype=int64)>]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.py_function(func = sampler , inp = [input1, 2], Tout = [tf.int64], name=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "795390bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.025, 0.025, 0.02 , 0.005],\n",
       "       [0.025, 0.025, 0.1  , 0.025],\n",
       "       [0.025, 0.025, 0.2  , 0.05 ],\n",
       "       ...,\n",
       "       [0.975, 0.975, 0.005, 0.02 ],\n",
       "       [0.975, 0.975, 0.025, 0.1  ],\n",
       "       [0.975, 0.975, 0.05 , 0.2  ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import generate_bbox_coords\n",
    "generate_bbox_coords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c9eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "771b9738d1ff369ab2f9edf10e5747f47a3eca8dd2a1e6732fe45fb0c7d2613f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
